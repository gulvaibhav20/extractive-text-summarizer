Title,Sub_Title,Text,Word_Count,Category,Author,URL,Human Generated Summary
Beware of the AI tag,Recognize the hype in disguise.,"Beware of the AI tag
I would go ahead and say that I’m no expert in artificial intelligence but I do engage in conversations related with AI quite often. While having breakfast with my friend and founder of Pendulibrium, Ilija Lalkovski we ended up talking about the apparent omnipresence of the two most common letters put together in the tech world in the past period, AI. We concluded that this abbreviation is slowly taking sweep and it’s automatically incorporated into every web site or digital marketing effort, without any justification. Since Ilija is fully dedicated to AI, aware of most of the challenges, yet not all, we continued discussing about this shameless approach and whether this AI spread is possible at this point.
All of a sudden we’re seeing this everywhere around us. Out of nowhere, every service company has its own AI enhanced solution that is probably used for amping up the price of their service or to provoke enough curiosity just to get you knocking on the door.

There are many writings about the hype in AI but I’d like to give fresh perspective and understanding to people, such as myself, who are not technically knowledgeable of what this AI tag entails.
If you’re mildly trying to educate yourself about AI and how you can get somewhere with it, you should probably continue reading so to understand, if nothing else, that the road before you is long and full of unknowns.
I’d go on to highlight several points in a broad manner which I believe everyone should take into consideration when looking for a company providing AI services or providing any service with sprinkles of AI on it.
1. Data analysis is not AI. But it’s an essential part of it and you must have it. 
Yes, data analysis is a necessity when talking about AI and it’s one of the most excruciating parts of the job. This step helps you to understand what your data can offer, whether it’s any good i.e. relevant and useful. Many companies that are already doing this are definitely one step ahead of the rest. If you see a company offering AI services ask how they are analyzing the data.
2. Domain Knowledge.
In order to create a solution, you must understand the field that’s surrounding the problem. You cannot go and build an AI solution that can forecast cashflow without at least understanding basic finance. Bradford Cross, the CEO of Merlon Intelligence explains how both tech expertise and domain knowledge are interdependent and necessary for one such product to succeed:
Building full-stack products requires deep subject matter expertise. Selling these products requires trust, respect, and relationships within the industry. Teams that manage to combine the subject matter and technical expertise are able to model the domain richly and drive innovation that comes from thinking outside the box by understanding what the box is. Teams that come with a domain-first approach tend to get stuck inside the box, and teams that come with a tech-first tend to get stuck out in left field.
This shouldn’t scare you since most of the companies, I believe, that are offering their services have the domain knowledge from their respective fields. However, the question here is whether they have the tech expertise that’s extremely hard to get these days due to the scarcity, which brings me to the next point.
3. Scarcity.
What has to be understood is the fact that the market is in deficit when it comes to bright minds in AI. And it’ll probably stay like that for some time. Companies like Google and Facebook are relentlessly trying to snatch every single person they can find that has background in AI. We’re constantly reading of collaborations between prestigious universities with companies (mostly tech behemoths) in a way that enables both parties to grow and prosper. Courses are emerging behind every corner. This just indicates that the force is not yet here. The force is being built. Any company that’s offering AI services has to have AI people on board. Be sure to check that box as well.
4. Actual data that’s real and relevant.
Data is bigger than model. Since we all have some sense as to what data is, let’s focus on the model. Without going in too much technicalities, the model can be understood as something that trains continuously on the data through a process of iteration which provides a better insight. This is why the data has to be real and has to relate with the things you do. You cannot play with any data just because you have it in abundance. I mean, okay, you can, if you have spare time but this is not how things should work. Quite often you’ll find that not all data is useful, as we do in real life, and very quickly you’ll become aware that what matters is the quality of the data. 
Why real and relevant? Well, you have to compare it with something, you have to validate your results against results from the real world in order to figure out whether you’re on the right track. Knowing this, don’t forget to ask what kind of data is being used and how.
5. Limited Possibilities (Desires vs. Reality).
Not everything can be done… yet. People are still in the dark, but there’s light. I think that in his interview, Facebook’s AI Director, Yann LeCun perfectly explained the actual difficulties AI people are facing each day:
AI researchers, down in the trenches, have to strike a delicate balance: be optimistic about what you can achieve, but don’t oversell what you can do. Point out how difficult your job is, but don’t make it sound hopeless. You need to be honest with your funders, sponsors, and employers, with your peers and colleagues, with the public, and with yourself. It is difficult when there is a lot of uncertainty about future progress, and when less honest or more self-deluded people make wild claims of future success. That’s why we don’t like hype: it is made by people who are either dishonest or self-deluded, and makes the life of serious and honest scientists considerably more difficult.This honest approach can help us understand that we’re far from having the AI implemented in almost everything, as we’re seeing now.
This honest approach can help us understand that we’re far from having the AI implemented in almost everything, although this hype and the information it feeds us tells otherwise. There are breakthroughs but all the points mentioned in this post are required in order to have the AI implemented with the actual benefits spurring from it. You shouldn’t expect anything tangible if you don’t have these things set in place.
In another post, I will go in depth about the effort we’re putting into a pilot project and how long it took us to create a prototype with data that’s already been processed and served on a platter.
To get back to the main advice, make sure the company you’re working with is honest with you about their AI enhanced services. The desire is different from the reality but this doesn’t stop companies in using this as their unique advantage.
6. Computational power and cost.
Someone needs to train the models using the data. And all this costs computational power and time, thus money. Understand that running a model and training it requires tons of tweaks in the data and the model itself. It’s also highly important to note that the type of data that’s being processed affects the cost big time. Working with data sets consisted of numbers is completely different from working with data sets of images, let alone videos. Just as an example, at last year’s Nvidia conference in Munich, Mercedes provided their behind the scenes numbers when trying to improve the driver’s experience about their next possible destination.
Mercedes-Benz’s presentation at Nvidia’s GTC conference in October, 2017
They exploited their GPUs for more than 18,000 hours just to find the right model for their data sets. And this is coming from a company that can safely afford this luxury. Although I must say that what Mercedes is pursuing is worthy for the whole AI community, I’d suggest trying to keep these numbers in mind next time you spot AI on some website offering design services, financial assistance, or what have you.
7. Primarily, AI should be beneficial for the business itself, not for its clients.
There’s no need to pump up prices by stating you have AI backing your solution, especially if you’re not in the AI business. It’s good, but this shouldn’t be the business’ main point. AI is just the buzz word and any business with lots of data and main focus on something completely different than AI shouldn’t be taken for granted. AI is more than some advanced algorithm. It should help us predict better with time, it should learn from itself, it should grow independently. The AI should solve day to day problems and make the businesses more efficient. It should transform the business in its core. So remember, businesses should cut their costs by having AI on board. You should still get the full service for what you paid for and not pay a penny more.
8. No silver bullets.
This is probably the most important thing. One solution cannot be applied to a general problem. At least not yet. There are just so many small things affecting the big picture in a way that cannot go unnoticed. General and small breakthroughs don’t work. If they do show promise, they are quickly accepted as a general and common truths, trivialities even, and they get quickly dispersed into the community which is, without question, something marvelous to see as a joint effort of contribution.
In his post about choosing the right vertical, Brendon also notes the following:
The reality is that these low level tasks are commoditized very quickly. Today’s novelty is tomorrow’s open source, and that’s happening faster and faster each year. Look at low level tasks as building blocks that you compose into higher level solutions rather than as the critical IP of your business.
I refrain from using the word tailored but it fits perfectly in this occasion. Understand that you cannot kill two birds with one stone. Every solution to its own problem.
I hope I shed some light to some of you and helped you recognize the hype in disguise. Remember that sticking these two letters, or their less attractive alternative (ML — machine learning) to a service company should always be questioned.
If you found this post useful I’d appreciate a clap and be sure to share your thoughts in the comments.
",1784,Data Science,Petar Papikj,https://medium.com/s/story/https-medium-com-pendulibrium-beware-of-the-ai-tag-80226bf4556e,"Out of nowhere, every service company has its own AI enhanced solution that is probably used for amping up the price of their service or to provoke enough curiosity just to get you knocking on the door.
There are many writings about the hype in AI but I’d like to give fresh perspective and understanding to people, such as myself, who are not technically knowledgeable of what this AI tag entails.
This step helps you to understand what your data can offer, whether it’s any good i.e. relevant and useful.
If you see a company offering AI services ask how they are analyzing the data.
In order to create a solution, you must understand the field that’s surrounding the problem.
Teams that manage to combine the subject matter and technical expertise are able to model the domain richly and drive innovation that comes from thinking outside the box by understanding what the box is.
This shouldn’t scare you since most of the companies, I believe, that are offering their services have the domain knowledge from their respective fields.
However, the question here is whether they have the tech expertise that’s extremely hard to get these days due to the scarcity, which brings me to the next point.
Any company that’s offering AI services has to have AI people on board.
4. Actual data that’s real and relevant.
Without going in too much technicalities, the model can be understood as something that trains continuously on the data through a process of iteration which provides a better insight.
This is why the data has to be real and has to relate with the things you do.
That’s why we don’t like hype: it is made by people who are either dishonest or self-deluded, and makes the life of serious and honest scientists considerably more difficult.This honest approach can help us understand that we’re far from having the AI implemented in almost everything, as we’re seeing now.
This honest approach can help us understand that we’re far from having the AI implemented in almost everything, although this hype and the information it feeds us tells otherwise.
In another post, I will go in depth about the effort we’re putting into a pilot project and how long it took us to create a prototype with data that’s already been processed and served on a platter.
To get back to the main advice, make sure the company you’re working with is honest with you about their AI enhanced services.
Someone needs to train the models using the data.
It’s also highly important to note that the type of data that’s being processed affects the cost big time.
Although I must say that what Mercedes is pursuing is worthy for the whole AI community, I’d suggest trying to keep these numbers in mind next time you spot AI on some website offering design services, financial assistance, or what have you.
AI is just the buzz word and any business with lots of data and main focus on something completely different than AI shouldn’t be taken for granted."
Female Disruptors: Joana Gutierrez is Shaking Up How We Show Gratitude,"“A piece of advice I have gotten and taken to heart is, “Be vigilant about your energy, thoughts, and actions.” I believe that the energy…","Female Disruptors: Joana Gutierrez is Shaking Up How We Show Gratitude

“A piece of advice I have gotten and taken to heart is, “Be vigilant about your energy, thoughts, and actions.” I believe that the energy you put out into the universe will come back to you.It’s important to make sure you are putting out the right type of energy. I always tend to attract people based on what aspect of my life I am working on. Where I’ve had negative energy in the past, I have attracted people with characteristics that reflected my own negative ones, and were bad for me. I chalk those people up to my list of lessons learned.”
I had the pleasure of interviewing Joana Gutierrez, the Founder and CEO of Meethappy. The Meethappy ecosystem consists of an AI assisted accommodation mobile application, partnered with TIPSc, a gratitude and recommendations based virtual currency, and The Real Meet, a digital series of interviews with celebrities and influencers conducted over a meal. These platforms work collectively to encourage authentic human interactions through technology, and provide the most accommodating experiences.
Thank you so much for doing this with us! What is your “backstory”?
I was a former partner at an Emerging Markets Multi-Strategy Hedge Fund. I left hedge fund management and ventured into the tech world in 2017 after being recruited by a VC firm to run an AI focused software company. I also served as an adviser for an open-source protocol that improves the efficiency of AI training, which was the first decentralized network for AI and machine learning.
Why did you found your company?
Leaving inspired by building an AI software startup from scratch, I sought to create my own solution to my everyday problem concerning meetings. Working in finance, information is crucial to opportunistic investment decisions. I would often have to schedule meetings with sell side analysts, portfolio managers, and potential investors. Meetings that would take place beyond the office would occur anywhere from an easily accessible coffee shop to dinner in a Michelin Star restaurant. If I wasn’t familiar with the neighborhood, it would unnecessarily take time out of my day to figure out a place to meet, and then find a table at the chosen location. Whether it was searching for a place via a food blog or reviewing the worst reviews on Yelp to see if positive reviews were manufactured, or trying to ascertain the type of service level; all of these processes were especially important when meeting a potential investor. Meanwhile, I couldn’t even anticipate the type of cuisine my invitee would prefer. All of this activity was time wasted. I wanted to create something that would eliminate the hassle of finding venues to meet, and provided a sense of certainty of seamless accommodation during my meeting experience. My early career experience working for a five-star hospitality institution, known for its impeccable service experience gave me insight into the highest level standard of service. I also wanted to replicate that experience regardless of where I meet, while providing revolutionary value to both the consumer and the business owner. I believe I can help billions of people afford VIP meeting experiences while creating revolutionary value for company owners. This is how I came up with the idea for Meethappy and the surrounding Meethappy ecosystem.

What is it about the work you’re doing that’s disruptive?
Meethappy is disrupting the 799 billion dollar hospitality industry with its AI integrated mobile application with which users will recommend, search, reserve, order, and pay participating venues. The AI component is in the form of a recommendation engine, allowing the app to recommend places for you to meet based on your previous reservation preferences, in addition to the preferences of the person that you’re meeting with.
The Meethappy app is also revolutionary because with a US patent application, I have created an incentive model with which a business owner can place a minimum spending option for reservation on Meethappy so that they will know guaranteed revenue on every Meethappy reservation. For example, a coffee shop can now place a table for reservation for a minimum purchase of a cup of coffee and pastry per person, per hour. The same model would apply to a high-end restaurant or VIP entry at a posh nightclub. Guaranteed revenue for business owners creates incentive for them to provide the most accommodating experience possible to Meethappy customers.
As previously mentioned, The Meethappy ecosystem consists of an AI assisted accommodation mobile application, partnered with TIPSc, a gratitude and recommendations based virtual currency, and The Real Meet, a digital series of interviews with influencers conducted over a meal.
TIPSc is a gratitude and recommendation based virtual currency. It allows users to acknowledge individuals who go above and beyond to assist, accommodate, or provide you with a spectacular product or service. People can earn TIPSc by excelling in what they do. In addition, the value of TIPSc increases every time a user performs a transaction — users will be rewarded every time they show gratitude. Eventually, TIPSc will provide verified public recommendations on which individuals or institutions provide the best product and/or service. We will be thanking our guests by exchanging TIPSc on The Real Meet series, as gratitude for coming on the show and sharing with us.
I am also the Executive Producer of The Real Meet series. The show, which is currently in the pre-production stage, is a digital series that brings together influencers, celebrities, rising stars, and thought leaders to foster authentic connections, and possibly mark the beginnings of a rewarding venture. The show makes introductions and interviews guests over a meal while filming at various venues throughout New York City.
I am thrilled at the opportunity to create a series that celebrates the ambition and accomplishments of today’s most influential individuals. Being able to not only bring such parties together to discuss their ventures, charitable causes, and passions, but to share them over a wholesome meal is the epitome of what I believe is the recipe for meaningful, long-lasting connection. We have positioned The Real Meet series as the first aspect of the ecosystem to launch first. Stay tuned for The Real Meet.

We all need a little help along the journey — who have been some of your mentors?
Lucille Douglas has been a significant mentor throughout my career. She is a Managing Director of the Global Client Group at Deutsche Bank Asset & Wealth Management where she is one of the first females and people of color to attain a Managing Director title.
Another mentor of mine is Tom Batac, my uncle, and godfather, who is the Founder and President of Liberty Marble, an industrial stone fabrication company based in New York City. Growing up, rather than attending summer camps, my summers were spent at his company and going to business meetings with him and my cousin. My mother’s family had very humble beginnings living in the Philippines. He grew up as the youngest brother of eight children, with at times, no shirt on his back. He worked diligently throughout his career and eventually started his own company that worked with world-renowned real estate developers to build some of Manhattan’s most memorable buildings like Winter Garden Memorial at the Freedom Tower, and the iconic Apple Store on Fifth Avenue, among several other projects over the years. He has since opened another Liberty Stone location and several Liberty Loan, microloan companies in the Philippines.
My parents, Carlos and Cristina Gutierrez, have also played strong mentor roles in my life. They taught me the importance of having good moral values, and have supported me in every way throughout my entire life. Their own “come back” stories of built success continue to inspire me as I forge my own path.
How are you going to shake things up next?
With the A-list guest list, the content, and the innovative model of the show, we are looking to hopefully stream The Real Meet series in a potential partnership with Netflix.
With the issuance of our virtual currency, TIPSc, we are looking to build a decentralized app (DApp) that generates a feed of images of the best recommendations for outstanding people and services, similar to Instagram, and conduct TIPSc transactions by capturing an image of the exemplary product/service and send payment via a username, similar to Venmo. We would like to move forward into the future, where we see the TIPSc DApp as a possible performance tracking program for businesses, where they can use it to justify bonus compensation structures, also paid in TIPSc.

Can you share 3 of the best words of advice you’ve gotten along your journey? Please give a story or example for each.
The first is, “Only take advice from someone that you would be willing to trade positions with.”
This allows me to take the person’s perspective into consideration, before acting on their advice.
For example, you may admire a person’s professional endeavors, but you might not fully see the quality of their personal life. . If a personal life or family is important to you then you may not want to take all of their advice.
A second piece of advice I’ve received is, “Fail early and fast.”
No one expects you to be perfect. We only hear of the glamourous success of entrepreneur life. This is not the case. It is a difficult path riddled with highs,lows, and detours. As long as you don’t give up, you will only get better and better. To quote Peter Marshall on the subject, “When we long for life without difficulties, remind us that oaks grow strong in contrary winds and diamonds are made under pressure.”
Finally, a third piece of advice I have gotten and taken to heart is, “Be vigilant about your energy, thoughts, and actions.”
I believe that the energy you put out into the universe will come back to you.It’s important to make sure you are putting out the right type of energy. I always tend to attract people based on what aspect of my life I am working on. Where I’ve had negative energy in the past, I have attracted people with characteristics that reflected my own negative ones, and were bad for me. I chalk those people up to my list of lessons learned.
What’s a book/podcast/talk that’s had a deep impact on your thinking? Share a story with us.
Good to Great by Jim Collins;
Give and Take by Adam Grant;
Oh, the Places You’ll Go! by Dr. Seuss;
How To Win Friends and Influence People by Dale Carnegie;
Freakonomics Podcast by Stephen Dubner
All of these books and podcasts have been significant references for me when it came to tackling all aspects of life, my career, and my company. Whether it was strategizing and implementing a business continuity plan for a client, learning to read people’s motivations and intentions in a small amount of time, or even becoming more self-aware, these resources have proven useful time and time again.
Some of the biggest names in business, VC funding, Sports, and Entertainment read this column. Is there a person in the world, or in the US whom you would love to have a private breakfast or lunch with, and why? He or she might see this. :-)
Arianna Huffington would be it. She has continued to serve as an inspiring model for powerful businesswomen and with the introduction of Thrive Global, an advocate for holistic wellness. She influences in a personal and a professional way. I want to thank Erika Ashley and her for providing me with a platform on which to share my endeavors.
How can our readers follow you on social media?
Linkedin: /in/joanagutierrez; /therealmeet; /meethappy; /tipsc
Instagram:@the_realmeet @meethappy @meethappylive @tipsc_io
Facebook: @therealmeet @tipscvirtualcurrency @meethappy
Twitter: @joanawaldhari @meethappy @threalmeet @tipsc_io
This was very inspiring. Thank you so much for joining us!
",1988,Startup,Yitzi Weiner,https://medium.com/s/story/female-disruptors-joana-gutierrez-is-shaking-up-how-we-show-gratitude-e053e430b223,"“A piece of advice I have gotten and taken to heart is, “Be vigilant about your energy, thoughts, and actions.” I believe that the energy you put out into the universe will come back to you.It’s important to make sure you are putting out the right type of energy.
The Meethappy ecosystem consists of an AI assisted accommodation mobile application, partnered with TIPSc, a gratitude and recommendations based virtual currency, and The Real Meet, a digital series of interviews with celebrities and influencers conducted over a meal.
If I wasn’t familiar with the neighborhood, it would unnecessarily take time out of my day to figure out a place to meet, and then find a table at the chosen location.
Whether it was searching for a place via a food blog or reviewing the worst reviews on Yelp to see if positive reviews were manufactured, or trying to ascertain the type of service level; all of these processes were especially important when meeting a potential investor.
I also wanted to replicate that experience regardless of where I meet, while providing revolutionary value to both the consumer and the business owner.
I believe I can help billions of people afford VIP meeting experiences while creating revolutionary value for company owners.
Meethappy is disrupting the 799 billion dollar hospitality industry with its AI integrated mobile application with which users will recommend, search, reserve, order, and pay participating venues.
The AI component is in the form of a recommendation engine, allowing the app to recommend places for you to meet based on your previous reservation preferences, in addition to the preferences of the person that you’re meeting with.
The Meethappy app is also revolutionary because with a US patent application, I have created an incentive model with which a business owner can place a minimum spending option for reservation on Meethappy so that they will know guaranteed revenue on every Meethappy reservation.
Guaranteed revenue for business owners creates incentive for them to provide the most accommodating experience possible to Meethappy customers.
As previously mentioned, The Meethappy ecosystem consists of an AI assisted accommodation mobile application, partnered with TIPSc, a gratitude and recommendations based virtual currency, and The Real Meet, a digital series of interviews with influencers conducted over a meal.
TIPSc is a gratitude and recommendation based virtual currency.
We will be thanking our guests by exchanging TIPSc on The Real Meet series, as gratitude for coming on the show and sharing with us.
The show, which is currently in the pre-production stage, is a digital series that brings together influencers, celebrities, rising stars, and thought leaders to foster authentic connections, and possibly mark the beginnings of a rewarding venture.
We have positioned The Real Meet series as the first aspect of the ecosystem to launch first.
Growing up, rather than attending summer camps, my summers were spent at his company and going to business meetings with him and my cousin.
With the A-list guest list, the content, and the innovative model of the show, we are looking to hopefully stream The Real Meet series in a potential partnership with Netflix.
. If a personal life or family is important to you then you may not want to take all of their advice.
I always tend to attract people based on what aspect of my life I am working on.
I always tend to attract people based on what aspect of my life I am working on.
All of these books and podcasts have been significant references for me when it came to tackling all aspects of life, my career, and my company."
Build your LSTM language model with Tensorflow,A language model is a machine learning model that we can use to estimate how grammatically accurate some pieces of words are. This kind of…,"Build your LSTM language model with Tensorflow
A language model is a machine learning model that we can use to estimate how grammatically accurate some pieces of words are. This kind of model is pretty useful when we are dealing with Natural Language Processing(NLP) problems. And in speech recognition tasks, the model is essential to be here to give us prior knowledge about the language your recognition model is based on.
As you may have known already, for most of the traditional statistical language models, they are enlightened by Markov property. For example, this is the way a bigram language model works:
The memory length of a traditional language model is not very long .You can see that in a bigram model, the current word only depends on one previous word. And in a trigram model, the current word depends on two preceding words. From my experience, the trigram model is the most popular choice, some big companies whose corpus data is quite abundant would use a 5-gram model. Nevertheless, you can see that even the memory of a 5-gram model is not that long.
Except for the short-term memory of statistical language models, another defect of traditional statistical language models is that they hardly decern similarities and differences among words. For instance, P(dog, eats, veg) might be very low if this phrase does not occur in our training corpus, even when our model has seen lots of other sentences contain “dog”. The model just can’t understand words.
So, this is when our LSTM language model begin to help us. Firstly, it can definitely memorize a long-term memory. More important, it can seize features of words, this is a valuable advantage we can get from an LSTM model.
In this tutorial, we will build an LSTM language model with Tensorflow together. At the end of this tutorial, we’ll test a 5-gram language model and an LSTM model on some gap filling exercise to see which one is better.

Prepare Data
I’m going to use PTB corpus for our model training; you can get more details on this page. PTB is good enough for our experiment, but if you want your model to perform better, you can feed it with more data.
The preprocessing of your raw corpus is quite necessary. Typically, every first step of an NLP problem is preprocessing your raw corpus. This step sometimes includes word tokenization, stemming and lemmatization. But, in here, we just simply split sentences since the PTB data has been already processed.
Here is the code.

First, we generate our basic vocabulary records. One important thing is that you need to add identifiers of the begin and the end of every sentence, and the padding identifier can make LSTM skip some input data to save time, you can see more details in the latter part.
Then, we turn our word sequences into index sequences. This processing is very similar to how we generate vocabularies. We set the OOV (out of vocabulary) words to _UNK_ to deal with certain vocabularies that we have never seen in the training process.
Build the LSTM Model
You can see the code on github. Here, I am going to just show some snippets.
The first step is to feed our model inputs and outputs. One thing important is that you need to tell the begin and the end of a sentence to utilize the information of every word in one sentence entirely. As you can see in Fig.1, for sequence “1 2605 5976 5976 3548 2000 3596 802 344 6068 2” (one number is one word), the input sequence is “1 2605 5976 5976 3548 2000 3596 802 344 6068,” and the output sequence is “2605 5976 5976 3548 2000 3596 802 344 6068 2”. “1” indicates the beginning and “2” indicates the end if you remember the way we symbolize our raw sentence.
Fig.1
You may have noticed the dots in fig.1, they mean that we are processing sequences with different lengths. There are many ways to deal with this situation. An intuitive solution is zero-padding, which is to append zeros to some sequences to get a bunch of sequences with the same lengths (We sometimes call it “max_time”). You can see it in Fig.2. However, we need to be careful to avoid padding every sequence in your data set. For example, if you have a very very long sequence with length like 1000, and the lengths of all you other sequences are just about 10, if you did zero-padding on this whole data set, every sequence length would be 1000, and apparently, you would waste your space and computation time. So, doing zero-padding for just a batch of data is more appropriate. This process sounds laborious, luckily, Tensorflow offers us great functions to manipulate our data.
Fig.2
We are going to use tf.data to read data from files directly and also feed zero-padded data to LSTM model (more convenient and concise than FIFOQueue I think).
In the code above, we use placeholders to indicate the training file, the validation file, and the test file. And then, we can do batch zero-padding by merely using padded_batch and Iterator.
But, we still have a problem. It is weird to put lonely word indices to our model directly, isn’t it? Yes! You may have seen a terminology like “embedding” in certain places. This is what we’ll talk about in our next step.
The reason we do embedding is to create a feature for every word. One advantage of embedding is that more affluent information can be here to represent a word, for example, the features of the word “dog” and the word “cat” will be similar after embedding, which is beneficial for our language model.
Fig.3
Embedding itself is quite simple, as you can see in Fig.3, it is just mapping our input word indices to dense feature vectors. The decision of dimension of feature vectors is up to you. For example, we have a 10*100 embedding feature matrix given 10 vocabularies and 100 feature dimension. Then, we get a sequence “1, 9, 4, 2”, all we have to do is just replace “1” with the 1st row of the feature matrix (don’t forget that the 0th row is prepared for “_PAD_”), then, turn “9” to the 9th row of the matrix, “4” to the 4th, “2” to the second, just like the way when you are looking up a word in the dictionary. In Tensorflow, we can do embedding with function tf.nn.embedding_lookup.
Then, we start to build our model, below is how we construct our cell in LSTM, it also consists of dropout. We can use that cell to build a model with multiple LSTM layers.
Next step, we build our LSTM model. As usual, Tensorflow gives us a potent and simple function to do this.
The dynamic_rnn can unfold nodes automatically according to the length of the input and be able to skip zero-padded nodes; these properties are valuable for us to cope with variable-length sequences. All it needs is just the lengths of your sequences. Here I write a function to get lengths of a batch of sequences.
The form of outputs from dynamic_rnn is [batch_size, max_time_nodes, output_vector_size] (default setting), just what we need! At this step, feature vectors corresponding to words have gone through a model and become new vectors that eventually contain information about words, context, etc. However, Since we have converted input word indices to dense vectors, we have to map vectors back to word indices after we get them through our model. A nonlinear transformation is enough to do this thing. First, we define our output embedding matrix (we call it embedding just for symmetry, cause it is not the same processing as the input embedding).
Fig.4
OK, we’ve got our embedded outputs from LSTM. What’next? Of course, we are gonna to calculate the popular cross-entropy losses. But before we move on, don’t forget that we are processing variable-length sequences, so, we need to dispense with the losses which are calculated from zero-padding inputs, as you can see in Fig.4. On the other hand, keep in mind that we have to care about every output derived from every input (except zero-padding input), this is not a sequence classification problem.
In the code above, we first calculate logits with tf.map_fn, this function can allow us to multiply each LSTM output by the output embedding matrix, and add the bias obviously.
Then, we reshape the logit matrix (3d, batch_num * sequence_length * vocabulary_num) to a 2d matrix. This reshaping is just to calculate cross-entropy loss easily.
The last thing we have missed is doing backpropagation. As always, Tensorflow is at your service.
Evaluate our model

In fact, when we want to evaluate a language model, the perplexity is more popular than cross entropy, why? You can see a good answer in this link. Here, I am gonna just quote:
Remember that, while entropy can be seen as information quantity, perplexity can be seen as the “number of choices” the random variable has.
So how to get perplexity? It is quite simple and straight; perplexity is equal to e^(cross-entropy). One more thing, you may have noticed that in some other places, they said that perplexity is equal to 2^(cross-entropy), this is also right because we just use different bases. In Tensorflow, we use natural logarithm when we calculate cross entropy whose base is e. So, if you calculate cross entropy function with base 2, the perplexity is equal to 2^(cross-entropy). In addation, I prove this equation if you have interest to look into.

Test our modelvim
Now, let’s test how good our model can be.
First, we compare our model with a 5-gram statistical model. Here, I chose to use SRILM, which is quite popular when we are dealing with speech recognition and NLP problems. Two commands have been executed to calculate the perplexity:
As you can see, we get the ppl and ppl1. According to SRILM documents, the ppl is normalized by the number of words and sentences while the ppl1 is just normalized by the number of words. Thus, the ppl1 is the score that we want to compare with the ppl comes from our RMMLM model.
Yes! Our model gets a much better score!
However, just one ppl score is not very fun, isn’t it? So, I’m going to use our model to do gap filling exercise for us!

You can find the questions in this link. The way we choose our answer is to pick the one with the lowest ppl score.
First, we utilize the 5-gram model to find answers.
We can add “-debug 1” to show the ppl of every sequence.
The answers of 5-gram model are:
1. everything that he said was wrong (T)
2. what she said made me angry (T)
3. everybody is arrived (F)
4. if you should happen to finish early give me a ring (T)
5. if she should be late we would have to leave without her (F)
6. the thing that happened next horrified me (T)
7. my jeans is too tight for me (F)
8. a lot of social problems is caused by poverty (F)
9. a lot of time is required to learn a language (T)
10. we have got only two liters of milk left that is not enough (T)
11. you are too little to be a soldier (F)
12. it was very hot that we stopped playing (F)
The accuracy rate is 50%. Remember, we have removed any punctuation and converted all uppercase words into lowercase.
The answers of rnnlm are:
1. everything that he said was wrong (T)
2. what she said made me angry (T)
3. everybody has arrived (T)
4. if you would happen to finish early give me a ring (F)
5. if she should be late we would have to leave without her (F)
6. the thing that happened next horrified me (T)
7. my jeans is too tight for me (F)
8. a lot of social problems are caused by poverty (T)
9. a lot of time is required to learn a language (T)
10. we have got only two liters of milk left that is not enough (T)
11. you are too small to be a soldier (T)
12. it was too hot that we stopped playing (F)
The accuracy rate is 66.7%
Our model gets a better score, obviously. :)
Conclusion
In this tutorial, we build an LSTM language model, which has a better performance than a traditional 5-gram model. The model in this tutorial is not very complicated; If you have more data, you can make your model deeper and larger.
",2392,Machine Learning,MilkKnight,https://medium.com/s/story/build-your-lstm-language-model-with-tensorflow-3416142c9919,"Build your LSTM language model with Tensorflow
A language model is a machine learning model that we can use to estimate how grammatically accurate some pieces of words are.
This kind of model is pretty useful when we are dealing with Natural Language Processing(NLP) problems.
The memory length of a traditional language model is not very long .You can see that in a bigram model, the current word only depends on one previous word.
From my experience, the trigram model is the most popular choice, some big companies whose corpus data is quite abundant would use a 5-gram model.
For instance, P(dog, eats, veg) might be very low if this phrase does not occur in our training corpus, even when our model has seen lots of other sentences contain “dog”.
So, this is when our LSTM language model begin to help us.
More important, it can seize features of words, this is a valuable advantage we can get from an LSTM model.
In this tutorial, we will build an LSTM language model with Tensorflow together.
At the end of this tutorial, we’ll test a 5-gram language model and an LSTM model on some gap filling exercise to see which one is better.
I’m going to use PTB corpus for our model training; you can get more details on this page.
PTB is good enough for our experiment, but if you want your model to perform better, you can feed it with more data.
One important thing is that you need to add identifiers of the begin and the end of every sentence, and the padding identifier can make LSTM skip some input data to save time, you can see more details in the latter part.
The first step is to feed our model inputs and outputs.
One thing important is that you need to tell the begin and the end of a sentence to utilize the information of every word in one sentence entirely.
For example, if you have a very very long sequence with length like 1000, and the lengths of all you other sequences are just about 10, if you did zero-padding on this whole data set, every sequence length would be 1000, and apparently, you would waste your space and computation time.
We are going to use tf.data to read data from files directly and also feed zero-padded data to LSTM model (more convenient and concise than FIFOQueue I think).
One advantage of embedding is that more affluent information can be here to represent a word, for example, the features of the word “dog” and the word “cat” will be similar after embedding, which is beneficial for our language model.
Embedding itself is quite simple, as you can see in Fig.3, it is just mapping our input word indices to dense feature vectors.
Then, we get a sequence “1, 9, 4, 2”, all we have to do is just replace “1” with the 1st row of the feature matrix (don’t forget that the 0th row is prepared for “_PAD_”), then, turn “9” to the 9th row of the matrix, “4” to the 4th, “2” to the second, just like the way when you are looking up a word in the dictionary.
We can use that cell to build a model with multiple LSTM layers.
Next step, we build our LSTM model.
But before we move on, don’t forget that we are processing variable-length sequences, so, we need to dispense with the losses which are calculated from zero-padding inputs, as you can see in Fig.4.
In the code above, we first calculate logits with tf.map_fn, this function can allow us to multiply each LSTM output by the output embedding matrix, and add the bias obviously.
In fact, when we want to evaluate a language model, the perplexity is more popular than cross entropy, why?
One more thing, you may have noticed that in some other places, they said that perplexity is equal to 2^(cross-entropy), this is also right because we just use different bases.
In Tensorflow, we use natural logarithm when we calculate cross entropy whose base is e.
The answers of 5-gram model are:
In this tutorial, we build an LSTM language model, which has a better performance than a traditional 5-gram model."
Deepiracy: Video piracy detection system by using Longest Common Subsequence and Deep Learning,"Por Claudia Saviaga, Carlos Toxtli","Deepiracy: Video piracy detection system by using Longest Common Subsequence and Deep Learning
Por Claudia Saviaga, Carlos Toxtli

Video clip localization is important for real life applications such as detecting copyright issues in videos, which has become crucial due to the increasing amount of videos uploaded to social media and video platforms. In this article we present a novel approach to do video clip localization. Our method is capable of detecting videos that have suffered from distortions (such as change in illumination, rotation) or even screened content, i.e. content that was recorded using a smart phone in a movie theater. We combine the longest common subsequence as way to measure similarity between videos and neural networks for object detection. Our results demonstrate the efficiency of combining these two methods. We also present results of the performance that our method achieves in real-time tests.
Introduction
Video clip localization consists of identifying video segments in a video stream. This is important for real life applications such as detecting copyright issues in video platforms. Managing the copyright of the huge number of videos uploaded everyday is a critical challenge for video platforms. There are several techniques to deal with video localization, for instance, some approaches consider bipartite graph matching to measure video clip similarity with a target video stream; however, these approaches do not cope with the important problem known as near-duplicate video clip/copy detection . Near-duplicate video copies are those video copies derived from the same original copy, by some global transformation such as video re-formatting and color shifting, or some local changes such as frame editing. Other methods deal with video copies by using principles of temporal consistency, which are difficult to thwart without significantly degrading the user’s viewing experience — contrary to the pirate’s goals. These approaches take advantage of a new temporal feature to index a reference library in a manner that is robust to popular spatial and temporal transformations in pirated videos. However, they have limitations, for example a pirate could temporally smooth a video’s gradient features to hide the sudden illumination changes used by these methods.
Deepiracy
In this article we introduce Deepiracy, an open-source anti-piracy tool that is able to detect distorted video clips in target streams in real-time by using deep learning and LCS (Longest Common Subsequence) a popular string algorithm. Figure 1 shows the system components:
Figure 1. Flowchart of the proposed approach.
The process works as follows:
We find the first frame (anchor frame) from the source video that matches in the target video.
Then we run a neural network based object classifier and get the objects on the images from the source video. For our tests we used TFSlim + SSDLite + MobileNet_V2 + COCO as is conveniently lightweight and used for real-time applications.
Once the objects are detected we convert them to symbols of an alphabet.
Then, we track the objects found in the anchor frame of the source video in the target video, advancing frame by frame until no object is present (as enough frames have passed and the object is no longer within the scene of the target video).
We skip the number of frames advanced on the source video to match the one in which the object was lost in the target video, this frame is used as the new anchor frame. By skipping frames and using anchor frames, we avoid comparing and processing frame by frame, which would be too costly in terms of performance.
We repeat the process from step 2 until the last frame in target video is processed.
The methods that we used for the process explained are the following:
Feature Extraction (SURF): Feature extraction allows us to represent the content of images in vectors. It also allows us to reduce the number of key-points of the source image to compare in the target image. We used SURF that is a popular feature detector that supports several images distortions.
Feature Matching (FLANN and KD Trees): Feature matching allows us to measure the difference in distance (i.e. Hamming, Euclidean) between the feature vectors extracted with SURF. FLANN creates KD-Trees to optimize the measuring.
Homography (RANSAC): RANSAC is a popular algorithm that we use to identify distortions. It chooses a subset of the points from one image, match these to the other image and compute the transformation which minimizes the re-projection error
Tracking (KCF): KCF is a correlation filter based tracker. It is a predictive type of tracker as it tries to predict the next position of the object.
Object Detection Algorithm: We use the TensorFlow Object Detection API , which is an open source framework built on top of TensorFlow to train and deploy object detection models based on neural networks. The model that we use is SSDLite + MobileNetV2 trained with the COCO dataset which contains over 80 categories of images.
Transforming images to symbols of an alphabet
In this article we formulate the problem of video sequence to sequence matching as a pattern matching problem. This is, we capture the information in a video sequence as a string representation. More specifically, we use the Longest Common Subsequence (LCS) as a measure of the similarity between the sequences.
The LCS representation provides an intuitive method to model objects observed in the video sequences. We try to approaches:
Modeling category of objects: The Figure 2 shows an example of category representation. In this case the algorithm assigns a symbol within an alphabet to the same category object, in this case P represents a person and B represents a bottle. In frame a) the algorithm assign a P to each person and a B to the bottle, and the same happens in frame b) even though the persons and the bottle are different.
Figure 2. Category Representation: algorithm assigns a symbol within alphabet to the same category object, in this case P for Person and B for bottle.
The Longest Common Subsequence (LCS) table can be found in Figure 3. In this case the S1={PPBB}, S2= {PPB} and the LCS is PPB. For this example the alphabet has the symbols {P,B}.
Figure 3. LCS table (Category Representation).
Modeling individual objects: Figure 4 shows an example of individual object representation. In a) the algorithm assigns a symbol within an alphabet to each object detected in the frame. In this case assigns symbol 1 and 2 to the persons and 3 to the bottle.
Figure 4. Individual Object Representation: algorithm assigns a symbol within alphabet to each object, 1 and 2 to the persons and 3 to the bottle.
Then finds an exact match in the other video frame b). In Figure 5, we can see that S1={1234}, s2={2,3,4} and LCS={234}, the alphabet in this case has the symbols {1,2,3,4}
Figure 5. Individual Representation.
A working example of the proposed approach can be seen in Figure 6. In there we compare two videos, we call them source and target. We use alphabet {a,b}. The first step is to detect the objects in the source video (frame 1) and assign a symbol from the alphabet to each one, then we track those objects in the target video (also frame 1). For simplicity we only track the two persons that appear in the video, therefore our alphabet has only two symbols. We can then skip frames in target video (frame 50 and 100) and keep tracking until a similarity measure is different from a predefined threshold, this is there are not enough objects from the source video in the target video. (in this case we use 0.8 as our threshold). In case of object detection at least one of the detected objects in the source video must be present in the target video.
Figure 6. Proposed approach
Tests and Results
To test our proposed method we followed two approaches:
Exact Matching: We compared the same image in the source and the target as seen in Figure 7.
Figure 7. Exact Matching Approach
Inexact Matching: We compared and image projected in an screened content as seen in Figure 8.
Figure 8. Inexact Matching Approach
Exact Matching Tests
For exact matching we used two videos with the followings characteristics:
Video 1: Contains people and objects with a duration of 2:09 seconds and 3 different scenes.
Video 2: Contains only people with a duration of 1:33 seconds and 2 different scenes.
For each video we test the feature descriptor, the category representation and the individual representation. 
The videos for the exact matching tests can be accessed here:

Only Feature Descriptor Video:
Video 1: https://youtu.be/heqs1AOBPoo
Video 2: https://youtu.be/mhrnrH20PkI
Object Detection Category Video:
Video 1: https://youtu.be/W6mIuL2-u0c
Video 2: https://youtu.be/jlk7a2K5r_o
Object Detection Individual Video:
Video 1: https://youtu.be/7pM-b7hQjPU
Video 2: https://youtu.be/ldTftRGMx2Y
Exact Matching Results
On Table 1 and Figure 9 we can see that representing the object with individual symbols of the alphabets is the best approach as we are able to reduce the time in 36.77% for video 1 and 46.21% for video 2.
Table 1. Exact matching time reduction
Figure 9. Time Reduction Exact Matching
The time reduction is bigger for video 2 because it only contains two different scenes, which means that the recalculation have to be done less times than video 1 which contains one more scene. Figure 10, Figure 11.
and Figure 9 show the detailed graphs related to time, skips and maximum number of skips.
Figure 10. Metrics Exact Matching Video 1
Figure 11. Metrics Exact Matching Video 2
Inexact Matching Tests
For inexact matching we used two videos with the followings characteristics:

Video 1: The source video contains scenes with two people in it, while the target video is the same scene projected in a cell phone.
Video 2: The source video is a scene with two people and one object while the target video is the source video projected in a cell phone.
For each video we test the feature descriptor, the category representation and the individual representation.
The videos for the inexact matching tests can be found here:
Only Feature Descriptor Video:
Video 1: https://youtu.be/ARhP4TwU314
Video 2: https://youtu.be/XtAHfkZFPKU
Object Detection Category Video:
Video 1: https://youtu.be/DSsqcN8dQn8
Video 2: https://youtu.be/rC2feFcIzx4
Object Detection Individual Video:
Video 1: https://youtu.be/_4dmleAOdq0
Video 2: https://youtu.be/-AiivR6ps7E
Inexact Matching Results
On Table 2 and Figure 12 we can see that representing the object with individual symbols of the alphabets was the also best approach as we were able to reduce the time in 39.52% for the first video and 62.52% for the second video.
Table 2. Inexact matching time reduction
Figure 12. Time Reduction Inexact Matching
In this case the time reduction also is influenced by the number of scenes in each video but also by the shaking of the smart phone of the person doing the test. Figure 13, Figure 14.
and Figure 12 show the detailed graphs related to time, skips and maximum number of skips. The final comparison for both methods can be seen in Figure 12.
Figure 13. Metrics Inexact Matching Video 1
Figure 14. Metrics Inexact Matching Video 2
Exact VS Inexact
Inexact matching tended to be faster than exact because the target device size was usually smaller than the source size, this is why the processing over the target was faster as is shown in Figure 15.
Figure 15. Time Reduction Exact vs Inexact Matching
Real-time tests
We tested the three approaches of inexact matching on real-time where a mobile device was playing a video from start to end while it was being recorded by a laptop web cam. In this setting, the important measurement is the number of frames skipped, since the processing time is the same than the video time.
 
 The videos for the real-time tests can be accessed here:
Only Feature Descriptor video:
Video 1: https://youtu.be/YO68ybWERDc 
Video 2: https://youtu.be/1Krhz-3Wiao
Object Detection Category Video: 
 Video 1: https://youtu.be/zKz9i0BjTe8
 Video 2: https://youtu.be/VdMIOV7Iz28
Object Detection Individual Video:
 Video 1: https://youtu.be/jh44X8gvKbQ 
 Video 2: https://youtu.be/7hgg85RC9Kk
 
Real-time Results
We can observe the results in Figure 16 and Figure 17 which show that both videos skipped more frames in the feature description condition, this means that more anchor frames were used and more processing was needed. In the object detection settings the individual approach performed much better in terms of frames skipped and number of frames skipped.
Figure 16. Real Time Matching Video 1
Figure 17. Real Time Matching Video 2
In terms of video fluency, the individual object approach felt more fluid that the rest of methods, this was because the average frames per second were 12 in comparison with 10 in the category object detection and only 5 in the features description approach.
The paper with the full evaluation can be found here.
Conclusions
In this article we propose a method for video clip localization. We showed that our method is capable of detecting piracy on screened videos. We used an approach that utilizes neural networks for object detection and Longest Common Subsequence (LCS) to translate objects in videos to patterns. Neural network algorithms are becoming faster and faster and this can be an ideal approach for alphabet based sequence matching. Our results showed that matching individual detected and tracked objects performed better than matching over category based objects and much better than feature description matching.
Code
The code of Deepiracy can be downloaded from here: https://github.com/toxtli/deepiracy
",2067,Machine Learning,Carlos Toxtli,https://medium.com/s/story/piracy-detection-using-longest-common-subsequence-and-neural-networks-a6f689a541a6,"Our method is capable of detecting videos that have suffered from distortions (such as change in illumination, rotation) or even screened content, i.e. content that was recorded using a smart phone in a movie theater.
We combine the longest common subsequence as way to measure similarity between videos and neural networks for object detection.
We also present results of the performance that our method achieves in real-time tests.
There are several techniques to deal with video localization, for instance, some approaches consider bipartite graph matching to measure video clip similarity with a target video stream; however, these approaches do not cope with the important problem known as near-duplicate video clip/copy detection .
In this article we introduce Deepiracy, an open-source anti-piracy tool that is able to detect distorted video clips in target streams in real-time by using deep learning and LCS (Longest Common Subsequence) a popular string algorithm.
We find the first frame (anchor frame) from the source video that matches in the target video.
We skip the number of frames advanced on the source video to match the one in which the object was lost in the target video, this frame is used as the new anchor frame.
The LCS representation provides an intuitive method to model objects observed in the video sequences.
Modeling category of objects: The Figure 2 shows an example of category representation.
In this case the algorithm assigns a symbol within an alphabet to the same category object, in this case P represents a person and B represents a bottle.
Category Representation: algorithm assigns a symbol within alphabet to the same category object, in this case P for Person and B for bottle.
In a) the algorithm assigns a symbol within an alphabet to each object detected in the frame.
Individual Object Representation: algorithm assigns a symbol within alphabet to each object, 1 and 2 to the persons and 3 to the bottle.
In Figure 5, we can see that S1={1234}, s2={2,3,4} and LCS={234}, the alphabet in this case has the symbols {1,2,3,4}
The first step is to detect the objects in the source video (frame 1) and assign a symbol from the alphabet to each one, then we track those objects in the target video (also frame 1).
We can then skip frames in target video (frame 50 and 100) and keep tracking until a similarity measure is different from a predefined threshold, this is there are not enough objects from the source video in the target video.
To test our proposed method we followed two approaches:
Exact Matching: We compared the same image in the source and the target as seen in Figure 7.
Inexact Matching: We compared and image projected in an screened content as seen in Figure 8.
The videos for the exact matching tests can be accessed here:
On Table 1 and Figure 9 we can see that representing the object with individual symbols of the alphabets is the best approach as we are able to reduce the time in 36.77% for video 1 and 46.21% for video 2.
The videos for the inexact matching tests can be found here:
On Table 2 and Figure 12 we can see that representing the object with individual symbols of the alphabets was the also best approach as we were able to reduce the time in 39.52% for the first video and 62.52% for the second video.
In this case the time reduction also is influenced by the number of scenes in each video but also by the shaking of the smart phone of the person doing the test.
Inexact matching tended to be faster than exact because the target device size was usually smaller than the source size, this is why the processing over the target was faster as is shown in Figure 15.
Time Reduction Exact vs Inexact Matching
We tested the three approaches of inexact matching on real-time where a mobile device was playing a video from start to end while it was being recorded by a laptop web cam.
The videos for the real-time tests can be accessed here:
In the object detection settings the individual approach performed much better in terms of frames skipped and number of frames skipped.
Real Time Matching Video 2
Real Time Matching Video 2
In terms of video fluency, the individual object approach felt more fluid that the rest of methods, this was because the average frames per second were 12 in comparison with 10 in the category object detection and only 5 in the features description approach.
We used an approach that utilizes neural networks for object detection and Longest Common Subsequence (LCS) to translate objects in videos to patterns.
Neural network algorithms are becoming faster and faster and this can be an ideal approach for alphabet based sequence matching.
Our results showed that matching individual detected and tracked objects performed better than matching over category based objects and much better than feature description matching."
Machine Learning: Trying to detect outliers or unusual behavior,This post is part of a series introducing Algorithm Explorer: a framework for exploring which data science methods relate to your business…,"Machine Learning: Trying to detect outliers or unusual behavior

This post is part of a series introducing Algorithm Explorer: a framework for exploring which data science methods relate to your business needs.
The introductory post “Machine Learning: Where to begin…” can be found here and Algorithm Explorer here
If you are looking to use machine learning to detect outliers or unusual behavior, you should look to Anomaly Detection Techniques.
Anomaly Detection Techniques
Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior. Those identified are often referred to as anomalies or outliers.
Use-Cases
Detect abnormal behavior of equipment in a manufacturing plant using sensor data such as temperature, pressure and humidity
Detect and prevent fraudulent spending by understanding normal customer spending amounts, locations and time between transactions
Most Common Anomaly Detection Algorithms
Below are introductions on the most common algorithms for anomaly detection: K-Means, One-class Support Vector Machines, and Autoencoders.
K-Means
Clustering techniques are a common approach for anomaly detection. Clusters of “normal” characteristics are identified and if the distance between a new point and all other clusters is too great, it is identified as an anomaly.
K-Means Clustering aims to partition n observations (data points) into k clusters in which each observation belongs to the cluster with the nearest center.
For more examples of Clustering techniques that can be used for anomaly detection, see Machine Learning: Trying to discover structure in your data
Pros
Simple and easy to implement
Easy to interpret results
Fast
Cons
Sensitive to outliers
You must define the number of clusters
Assumes the clusters are spherical
The clusters are found using a random starting point so may not be repeatable and can require multiple runs to find an optimal solutions
Vocabulary
Characteristics — Characteristics are common or similar values seen in the data based upon features, e.g. people that spend a lot of money on rent, those that infrequently make purchases, etc.
k — k is a user-defined value referring to the number of clusters the algorithm should find.
Observation — An observation is a single example, a data point or row in the data.
Cluster — A group of similar things that are close together.
One-Class Support Vector Machines
If you were to plot your data in an n-dimensional space (where n is the number of features), One-Class Support Vector Machines (SVM) attempt to identify the region where most cases lie, these are considered “normal”. It will then fit a hyperplane that best separates these “normal” examples from the rest. When you have a new data point, it is labeled as “normal” or an “anomaly” depending how close it is to the “normal” boundary.
Pros
No assumptions on the distribution of the data
Ability to find normal boundary that is non-linear
Can be used in high-dimensional space
Cons
Choosing the right hyper-parameters to find the appropriate non-linear shape of the boundary can be difficult
Can be slow to train large datasets
Memory intensive
Vocabulary
n-dimensional space — A 1-dimensional (1D) space is represented simply as a line and 2-dimensional (2D) is referred to as the Cartesian plane, where you can move up or down and right or left. To generalize, n-dimensional space is used.
hyperplane — A hyperplane in a 1-dimensional (1D) space is a point. In a 2-dimensional (2D) space, it is a line. A hyperplane in 3-dimensional (3D) space is a plane, a flat surface. To generalize for any dimension, the concept is referred to as a hyperplane.
boundary — This is the line, plane or hyperplane that divides the data between those that have been identified as ‘normal’ and those that are not.
Autoencoders
An Autoencoder is a technique for dimensionality reduction. It is a type of neural network where the first part of the network, called the encoder, reduces the input to a lower dimension. The second part of the network, called the decoded, aims to reconstruct the original input. The goal is to create a model where the input and output are the same. A new data point can be passed through the model and if the error between the input data and the computed output is too great, it can be flagged as an anomaly.
Pros
Can capture non-linear relationships and subtle connections between the features
Variations result in state-of-the-art results
If the data is temporal, LSTM (long-short-term-memory) autoencoders can be used
Cons
Requires a very large amount of data
Many hyper-parameters to tune
Long training time
Requires significant computing power for large datasets
Vocabulary
Dimensionality reduction — The initial number of dimension will be the number of features. The goal of dimensionality reduction is to reduce the number of dimensions without losing important information
Neural network — Neural networks can learn complex patterns using “hidden layers” between inputs and the output. These layers are made of neurons which mathematically transform the data.
Input — The features are passed as inputs.
Model — Machine learning algorithms create a model after training, this is a mathematical function that can then be used to take a new observation and calculates an appropriate prediction.
Output — This is the target variable, the thing we are trying to predict.
Non-linear relationships — A non-linear relationship means that the a change in the first variable doesn’t necessarily correspond with a constant change in the second. However, they may impact each other but it appears to be unpredictable.
Temporal — Temporal data is data relating to time
Hyper-parameters — A hyper-parameter is a value that is set prior to building a model; these values are important as they impact the success of the model
Further Reading
Other posts in this series:
Machine Learning: Where to begin…
Machine Learning: Trying to predict a numerical value
Machine Learning: Trying to classify your data
Machine Learning: Trying to discover structure in your data
Machine Learning: Trying to make recommendations
Many Thanks
I wish to thank Sam Rose for his great front end development work (and patience!), converting my raw idea into something much more consumable, streamlined and aesthetically pleasing.
Similarly, my drawing skills leave much to be desired so thank you to Mary Kim for adding an artistic flare to this work!
",1066,Machine Learning,Stacey Ronaghan,https://medium.com/s/story/machine-learning-trying-to-detect-outliers-or-unusual-behavior-2d9f364334f9,"Machine Learning: Trying to detect outliers or unusual behavior
This post is part of a series introducing Algorithm Explorer: a framework for exploring which data science methods relate to your business needs.
If you are looking to use machine learning to detect outliers or unusual behavior, you should look to Anomaly Detection Techniques.
Anomaly Detection is a technique used to identify unusual events or patterns that do not conform to expected behavior.
Most Common Anomaly Detection Algorithms
Below are introductions on the most common algorithms for anomaly detection: K-Means, One-class Support Vector Machines, and Autoencoders.
Clustering techniques are a common approach for anomaly detection.
For more examples of Clustering techniques that can be used for anomaly detection, see Machine Learning: Trying to discover structure in your data
If you were to plot your data in an n-dimensional space (where n is the number of features), One-Class Support Vector Machines (SVM) attempt to identify the region where most cases lie, these are considered “normal”.
When you have a new data point, it is labeled as “normal” or an “anomaly” depending how close it is to the “normal” boundary.
boundary — This is the line, plane or hyperplane that divides the data between those that have been identified as ‘normal’ and those that are not.
A new data point can be passed through the model and if the error between the input data and the computed output is too great, it can be flagged as an anomaly.
Model — Machine learning algorithms create a model after training, this is a mathematical function that can then be used to take a new observation and calculates an appropriate prediction.
Machine Learning: Trying to predict a numerical value
Machine Learning: Trying to classify your data
Machine Learning: Trying to discover structure in your data
Machine Learning: Trying to make recommendations"
On choosing the right PhD for you,I am one of those lucky individuals doing research in one of the most happening fields in the world. The field that is often seen as the…,"On choosing the right PhD for you

I am one of those lucky individuals doing research in one of the most happening fields in the world. The field that is often seen as the “electricity of the modern era”. Yes, I work in Machine Learning (ML). I work on its application to understand images and videos — Computer Vision. Though my perspective in this story is biased towards ML, it applies to most PhDs.
ML is a very active research field these days. There are billions of dollars floating around in terms of funding. Hundreds of academics are being approached by companies for consulting. For graduating Masters students this means ample opportunities for doing a PhD. From my experience of interacting with ~5 years of graduates from MSc programme, I notice a common trait: Top students get flooded with PhD offers and simply do not know how to choose the right PhD catered for them. As a result they regret having wasted their valuable years doing a PhD. Mediocre students just flow with the tide and go for the PhD that comes their way and regret having wasted their time. So, I wish to unleash some unwritten pointers for choosing the right PhD that suits ‘you’.
Topic
A PhD is a constant grind to explore the unknown aiming to push the boundary of a given tiny area of a field. As Andrej Karpathy puts it in his blog, “A PhD is simultaneously a fun and frustrating experience”. So to get a good PhD in the end (in n years 😃), it is better to choose a topic in an area of your field that is extremely interesting for you, excites you or has always been exciting. It is quite important not to be tempted by any other material temptations such as:
A prestigious scholarship. Most students accept these scholarships thinking that they make the CV look good. Not to forget that these scholarships demand a lot from the student: strict direction, meeting deadlines, making reports, a supervisor who is not fully academic, etc. As a result you lose the joy of doing research. In my opinion, the papers you publish during your PhD are much more valuable than just winning a prestigious scholarship to get you into a PhD. Your papers can bring recognition and the positions you get after your PhD can win you good salaries.
An industrial PhD. Salaries are good with industrial PhD. However, in my opinion there is no major difference between doing a project for the company and doing an industry funded PhD. The work tends to be too applied with the funding company often expecting results that can benefit the company or its products.
Hot topic. Just because deep learning is a hot topic, it may be extremely tempting to start a PhD in it. By the time you finish, the academic community might have moved on and be churning out papers on a different area of your field. So, sticking with something exciting for you is way better to survive the PhD grind.
So, simply choose a topic that interests you and will keep you going for years. It is quite normal to drift away from where you started. But even with the drift, I have always seen students stay in the expert zone of their supervisor and not drift too far.
Location
By location I mean the town / city where you will be working. Most of my friends declined PhD positions simply because the lab is located in a very small town in some corner of the world.
It is worth noting that some extremely skilled scientists and professors prefer to work in such quiet towns for personal reasons. I tend to think that taking a quiet stroll along the streets and parks of such small towns can trigger creative ideas.
For a student, one advantage of working in such locations is that the cost is less. Given the salaries that PhDs get, you get to save a significant sum even during your PhD by working in such locations.
There is no chance to party every Friday night. So you get to focus more on your thesis which by itself is challenging enough to finish in 3 years. With partying and extra curricular activities you are only extending the life of your PhD.
So why bother much about where you work when you get a chance to work with a renowned supervisor on a highly interesting topic. If you do best during your PhD years, you can bag a post-doc position in the city of your choice 😃.
Size matters
Some research groups tend to publish on diverse topics. For instance a group may be publishing in Computer Vision, Robotics and Medical Imaging. Such a diverse group indicates either one of the two:
Your supervisor is a world renowned Professor who can manage such a big, diverse group by hiring very skilled researchers.
The group is inter-disciplinary and so they need to work on multiple fields.
A bitter fact in such a big group is that the supervisor simply cannot keep up with what is happening with every student. For instance it becomes difficult for a Professor to read papers in every single conference in CV, ML, NLP. So they tend to manage the group as a hierarchy. They hire post-docs who are experts in different areas. The PhD students in turn get guidance from the post-docs. It is very easy to identify such groups looking at the publication record of your potential supervisor in the past 3–5 years.
If the group is hierarchical, talk to the post-doc who is going to be your supervisor. Because you will mostly discuss your problems with your post-doc. Nevertheless, be prepared to work all by yourself in this kind of environment as your post-doc will be busy in meeting, writing grants, etc.
If the group is small and focused on 2–3 areas of a field, then you can expect much more help from your supervisor. He/she will be keen to get good publications out of you which in turn bring grants, which in turn expand the group and the cycle continues…
“Small is beautiful” when it comes to group size. But if you can do it all alone, then choose a large group, work independently with lot of independent work happening around you.
Publishing style
In my opinion, there are two kinds of publishers in academia: quality and quantity publishers.
Quantity publishers wish to publish a lot of papers in a given year. So they send their papers to workshops or a tiny unknown conference in some corner of the world. To them what matters is getting another paper in their list of publications.
Quality publishers tend to focus on the quality of papers. So they wait and send the papers to top notch conferences or journals. For this, they patiently do lots of experiments to convincingly argue the statement they are making in the paper. Needless to say, they are the ones that get the most citations as they are constantly pushing the state-of-the-art. So their work tends to be recognised internationally.
So, if you find your potential supervisor has 30 publications in a year, be sceptical. A supervisor with 3–5 publications in a Tier 1 conference or journal could be a better bet to work with simply because you will also be expected to publish quality papers and not churn-out quantities.
Freedom
One of the joys of doing a PhD is that as a creative individual, most of the times you get to do whatever you want, at least within the scope of the area you are working on. But it may not be this way:
Your supervisor may be strong about his own ideas and could always expect you to try and experiment with his ideas.
Or he/she can be someone who lets you lead ‘your’ work and just guide you whenever you are totally lost or need resources like more compute power or some equipment to run your experiments.
First, think twice what sort of person you are. Are you abreast with what is happening in the field? Can you come up with your own ideas and show through your experiments that the ideas are worth it? if so, you probably need to work with the latter style of supervision. It is worth talking to your potential supervisor’s team members to find out how they come up with ideas, implement them, publish and iterate the process.
Group Mates
Though a PhD is a solo journey with mounting pressure to publish as days pass by, you tend to spend a lot of time with your group mates, go for a few drinks with them every now and then. They may not be too helpful to you even if they want to as their expertise will be in a different area. But they do define the energy and morale of the team. The team could bring positive vibes to you by simply encouraging you in your journey. Or they can simply throw words at you during that 5 mins lunch break chats which can tumble the bricks of your confidence. So the energy and morale of your potential group mates is never to be underestimated while choosing to commit yourselves 3+ years.
Try to chat with the potential group mates as much as possible before choosing to commit working with them for few years. Identify the thin line of work culture running in the group.
Other Commitments
Some other commitments for a PhD student could be:
Teaching courses and preparing course materials
Supervising MSc thesis
Supervising students in labs
All the above skills are quite valuable for someone wishing to pursue an academic career. But they also consume valuable research time from your 3 years. Would you like to graduate as that all-rounded person who has tasted every aspect of academia? Or would you like to focus on pure research during your PhD and let alone teaching and supervising others for later stages of your career?
Expectation
With all other factors fixed, some supervisors simply expect the students to work all by themselves, come up with ideas and get results. This is because a PhD is the only time to mould you into that independent scientist who can constantly push the field ahead.
Some supervisors will help you quite a lot. They discuss with you all the ideas you need to experiment with. They write the paper if you are running experiments. The do the figures for your paper if you are coding the solution.
You and “only you” know who you want to work with. You may asses yourself by looking at how you behaved while doing your MSc coursework. Did you work alone? Did you like group projects or independent projects?
Conclusion
Unlike other degrees, the value of a PhD is in what you contribute! You are more likely to succeed if you choose to work on the right topic and the right people. All else is next. I hope this article helps make that right decision before embarking on that journey rather than regretting with a hindsight. Happy PhD! 😄
Also note: Publishing a few papers as the first author is the starting point for a research career. PhD is one defined path to do it. But in a small world, don’t many paths lead to the same destination? 😏
",1913,Academia,Shrinivasan Sankar,https://hackernoon.com/on-choosing-the-right-phd-for-you-d7eaefec93cd,"From my experience of interacting with ~5 years of graduates from MSc programme, I notice a common trait: Top students get flooded with PhD offers and simply do not know how to choose the right PhD catered for them.
Mediocre students just flow with the tide and go for the PhD that comes their way and regret having wasted their time.
A PhD is a constant grind to explore the unknown aiming to push the boundary of a given tiny area of a field.
So to get a good PhD in the end (in n years 😃), it is better to choose a topic in an area of your field that is extremely interesting for you, excites you or has always been exciting.
In my opinion, the papers you publish during your PhD are much more valuable than just winning a prestigious scholarship to get you into a PhD.
Your papers can bring recognition and the positions you get after your PhD can win you good salaries.
By the time you finish, the academic community might have moved on and be churning out papers on a different area of your field.
So, simply choose a topic that interests you and will keep you going for years.
Most of my friends declined PhD positions simply because the lab is located in a very small town in some corner of the world.
If you do best during your PhD years, you can bag a post-doc position in the city of your choice 😃.
Some research groups tend to publish on diverse topics.
Your supervisor is a world renowned Professor who can manage such a big, diverse group by hiring very skilled researchers.
The group is inter-disciplinary and so they need to work on multiple fields.
A bitter fact in such a big group is that the supervisor simply cannot keep up with what is happening with every student.
The PhD students in turn get guidance from the post-docs.
It is very easy to identify such groups looking at the publication record of your potential supervisor in the past 3–5 years.
If the group is small and focused on 2–3 areas of a field, then you can expect much more help from your supervisor.
Quantity publishers wish to publish a lot of papers in a given year.
A supervisor with 3–5 publications in a Tier 1 conference or journal could be a better bet to work with simply because you will also be expected to publish quality papers and not churn-out quantities.
One of the joys of doing a PhD is that as a creative individual, most of the times you get to do whatever you want, at least within the scope of the area you are working on.
Though a PhD is a solo journey with mounting pressure to publish as days pass by, you tend to spend a lot of time with your group mates, go for a few drinks with them every now and then.
So the energy and morale of your potential group mates is never to be underestimated while choosing to commit yourselves 3+ years.
Try to chat with the potential group mates as much as possible before choosing to commit working with them for few years.
Some other commitments for a PhD student could be:
With all other factors fixed, some supervisors simply expect the students to work all by themselves, come up with ideas and get results."
In Conversation: Why the VR and AR industry can't ignore the computer in your pocket,"The smartphone is today's compute platform. But - as we look towards a world of AR & VR, we see this device's limitations.","In Conversation: Why the VR and AR industry can’t ignore the computer in your pocket
AUGUST 22, 2018
LENS HQ — Sydney, Australia
Travis Rice, CCO and Founder of LENS Group



DF: Your background is in contemporary art, correct?
TR: My background is in working with artists at that fuzzy intersection between ideas and technology. Contemporary artists are natural early adopters in of technology, as observers and philosophers that hold a mirror up to society. A lot of the stuff that we talk about in museums and galleries overlaps with tech, business and cultural structures, and that’s where the focus of my work is. That’s what I’m doing at LENS.
DF: We talked about the convergence of devices, and you mentioned that this is something many people were looking for — back before the iPhone was introduced.
TR: Back in 2004, A lot of people didn’t think think you could combine a camera, MP3 player, PDA & phone into one device. It’s hard to believe that looking back now. Many thought you couldn’t dilute the device expertise inherent in specialized tech, despite the similarities of components. 2004 was not that long ago!
DF: How do you see VR, AR & AI in 2018, in their current forms?
TR: Virtual Reality (VR), Augmented Reality (AR), and Artificial Intelligence (AI) are a natural outcome of digital technologies and culture. This digital revolution is growing from the intersection between disciplines such as the arts, humanities and sciences, which has not only enhanced our everyday lives but has also become a snapshot of the future of humans and technology — within our society and our relationships with technology.
DF: And the smartphone?
TR: The smartphone is today’s compute platform. It’s changed everything about computing. It’s changed the world! But — as we look towards a world of AR & VR, we see this device’s limitations. It’s a tiny window into a world — not a doorway. Not yet. For example, Google Maps. It’s an incredible digital visualization tool we use everyday, understanding your place in the physical world represented in a digital model — but screen real estate becomes an issue when that concept is scaled.
Trending AR VR Articles:
1. How XR Can Unleash Cognition
2. Oculus Go, the Nintendo Switch of VR
3. Expert View: 3 ways VR is transforming Learning & Development

DF: But there’s almost no limit to what a smartphone can provide.
TR: Yes, our smartphones today now have these ‘super apps’, such as Facebook and WeChat (the king of super apps) but they are still silo-ed experiences. Separate spaces and separate experiences. Devices still don’t talk to each other, limiting the chances of complete immersion. A smartphone doesn’t talk to an Xbox which doesn’t talk to a VR headset — not in terms of display, but in terms of digital awareness. There’s a lack of immersion, and lack of depth — or another way to look at it is a lack of ability to dive into these stories and worlds. Your digital space doesn’t allow for a way for your physical space to be represented.
The ultimate goal of AR & VR is for a true cohesive immersion and a collective experience. A world of true immersion.
DF: How does science fiction influence your work?
TR: I see science and speculative fiction as way for creating a collective vision. We once read Jules Verne’s fictions about traveling to the moon and we got there — working as a huge group, that was energized towards that one goal. When we work in film or in museums on large scale productions or projects — the very first step is to get people on the same page and that starts by visualizing the end goal. This is what science fiction does on a social scale. It gets everyone dreaming in the same direction. A social or collective knowledge that has informed our genuine perceptions of what is possible. For instance in Hitchhiker’s Guide to the Galaxy, the guide that is described in the book is essentially the iPhone.
DF: What do you see as the steps towards true immersion?
TR: It has four primary components before we can say we have truly arrived. We’re not there yet.
Virtual presence — you as a person need to have a presence in this digital world.
Monetary value for time and assets — Can you work in this space? Sell an idea? Can you study?
Real world cause and effect — Selling and buying, working and playing must be regulated by rules and laws. If your bank account is hacked, the authorities will be notified. What if your Facebook account is hacked? What if you lose your digital assets? What if it took you five years to accumulate those assets and they’re stolen? If you shoot someone in a game there are no consequences.
Work/play/life integration — Sure, you can get a job digitally or go to school and learn, but you also must be able to have a virtual life in addition to your physical one, and it must be integrated with your physical life. Meaning that the you that has interactions, on your own or with others — must exist on both sides.

DF: What are the barriers to true immersion?
TR: There are a few.
Bulky hardware — it’s just that. Bulky, heavy and not the most pleasant to look at. It also must address our inner ear. The visuals and audio are strong in VR content, but the sense of motion and balance is not there.
Software ‘walled gardens’ — separate apps, no cohesive integrated platform.
Digital technology limitations — display themselves do not even approach what our native eyes are capable of (doing, capturing, seeing, viewing).
Mobile compute power and bandwidth limitations — the necessity to pump all of this data into headsets or other devices requires an enormous amount of computing power and bandwidth that we cannot currently address.
Social limitations — There’s still a social stigma.
Absence of a luxury brand.
Cost is still too high.
Serious lack of engaging content & at the same time, a lack of a user base.
DF: Who’s working to solve this today?

TR: In general, the main players are: Microsoft with the Hololens, and their MR devices; Snap and their upcoming Snapchat Glasses; Facebook with Oculus Rift and Go, the most accessible VR devices; HTC who are pushing room scale VR & Magic Leap with their hotly anticipated and poorly received MR device.
DF: And LENS!
TR: And us! At LENS we have two primary projects, TYKO & TORII. They work together to power the push towards true immersion.
TYKO is our meta platform and user experience, built to leverage the one hundred years of media that’s already been created. To offer something consumable on all the different devices. It mixes market and user generated content. In this TYKO world where we work and play in a truly immersive environment, we need a new digital distribution infrastructure that can meet the data needs of high volume, three dimensional data. That’s TORII.
‘It’s a tiny window into a world — not a doorway. Not yet.’
TORII is the AI highway. It’s our proprietary digital infrastructure, and a new digital distribution model saving bandwidth costs, powered by what we call Progressive AI, which allows for streaming all content types, without downloads.
We see all of this satisfying that collective vision, of all the platforms converging into one common experience. Device convergence in 2018. It sounds like science fiction, but it’s happening and we’re building it.

At LENS, we believe streaming video should be clearer, faster and more economical.
Never download again.
lens-immersive.com

",1269,Virtual Reality,LENS Immersive,https://arvrjourney.com/in-conversation-why-the-vr-and-ar-industry-cant-ignore-the-computer-in-your-pocket-fd5ab73cc27f,"TR: My background is in working with artists at that fuzzy intersection between ideas and technology.
A lot of the stuff that we talk about in museums and galleries overlaps with tech, business and cultural structures, and that’s where the focus of my work is.
DF: We talked about the convergence of devices, and you mentioned that this is something many people were looking for — back before the iPhone was introduced.
But — as we look towards a world of AR & VR, we see this device’s limitations.
It’s a tiny window into a world — not a doorway.
It’s an incredible digital visualization tool we use everyday, understanding your place in the physical world represented in a digital model — but screen real estate becomes an issue when that concept is scaled.
Devices still don’t talk to each other, limiting the chances of complete immersion.
There’s a lack of immersion, and lack of depth — or another way to look at it is a lack of ability to dive into these stories and worlds.
The ultimate goal of AR & VR is for a true cohesive immersion and a collective experience.
A world of true immersion.
TR: I see science and speculative fiction as way for creating a collective vision.
When we work in film or in museums on large scale productions or projects — the very first step is to get people on the same page and that starts by visualizing the end goal.
At LENS we have two primary projects, TYKO & TORII.
They work together to power the push towards true immersion.
TYKO is our meta platform and user experience, built to leverage the one hundred years of media that’s already been created.
In this TYKO world where we work and play in a truly immersive environment, we need a new digital distribution infrastructure that can meet the data needs of high volume, three dimensional data.
It’s our proprietary digital infrastructure, and a new digital distribution model saving bandwidth costs, powered by what we call Progressive AI, which allows for streaming all content types, without downloads.
lens-immersive.com"
Smart speakers and A.I. will give your physician superpowers,"As a hybrid physician/engineer, I spend a lot of time pondering how new platforms can empower doctors.","Smart speakers and A.I. will give your physician superpowers
As a hybrid physician/engineer, I spend a lot of time pondering how new platforms can empower doctors.
I am particularly excited about the potential of smart speakers coupled with advances in A.I. and natural language processing (also looking at you, blockchain). I am bullish on conversational agents in general, previously building an iOS chatbot powered by Watson that simulates a human radiologist. Chatbots are cool and useful, but voice — that might be magic.

Sensing potential, I decided to hunker down with my trusty corgi, drink a bunch of coffee, and start building the cool voice tools I want to use in my own clinical practice. This experience made me a lot more excited.
In this article I will synthesize my findings, show a bunch of fun demo videos, and explain why smart speakers represent a transformative technology in healthcare.
My first Google Home tool. Lots of fun.
Why should people that care about healthcare innovation start thinking about smart speakers? Well…
Smart speakers massively empower surgeons
Imagine for a moment that you are a surgeon. You meticulously scrub your hands and undergo the long and complex process of surgical preparation, methodically putting on sterile gloves and a surgical gown and entering the operating theatre, scalpel in hand, exposed abdomen on the table.
You have entered the world of sterility. You are now incapable of checking your phone, which sucks. More importantly, you can no longer use a calculator, consult a medical reference, check the patient’s record, jot down a note — you can’t even Google things.
Smart speakers thus offer immense value to all surgeons and proceduralist physicians: they give them their modern technology back. Many important applications can be built around sterility needs, from software allowing physicians to dictate paperwork during surgical downtime to this simple (but useful) sizing tool:

The tool I am building uses voice to empower the sterile. The smart speaker allows surgeons to rapidly determine if a particular stent or other device fits within a particular catheter. There are hundreds of devices deployed using catheters, and remembering what fits in what becomes impossible. The current workflow involves constantly asking support staff to check reference materials…an awkward and painful game of telephone.
As a case study of one major sterility need, consider the workflow for determining if a particular device is available (for example, a stent of a particular size).
The surgeon says, “hey…do we have any 5 mm stents?” A human assistant then leaves the room, walks some distance to a storage area, and rummages through piles of boxes looking for stents. Precious minutes later they return and report their findings.
This is insanely inefficient, and a voice application allowing surgeons to rapidly query the inventory will be a game-changer. Please consider giving me a small finder’s fee when this makes you $1 billion.
Smart speakers facilitate eye contact and patient connection
Next, imagine that you are a physician in a busy emergency room. You have to see 20 patients before lunch, and after seeing them you have to complete mountains of paperwork documenting your findings.
This documentation generally occurs from 5-8 pm as you miss a dinner reservation with your wife and struggle to remember the details of your third patient with a cough. You were also supposed to play fetch with your corgi, and you missed it.
He is a good boy and he deserves fetch. Shame on you.
These time pressures create physicians that are buried in a computer screen when they should be focusing on their patient, making eye contact, and creating the connection that is fundamental to the physician-patient relationship. And patients are getting mad:

Smart speakers solve this problem. They allow physicians to chart data in realtime during a clinical encounter while continuing to make eye contact with the patient. This is huge. It both increases data accuracy — you don’t have to remember it later — and creates a better patient experience.
It also creates a better physician experience: we became doctors not to jump through documentation hoops but to take care of people and provide excellent, personable care. Everybody wins.
Smart speakers allow zero-friction access to high quality information
Friction is devastating to busy physicians. When you are responsible for 40 hospital patients and time delays are potentially deadly, making an additional click or opening an additional program becomes maddening.
Healthcare thus places a premium on immediate information accessed seamlessly. Sure, you could take 3 minutes to look up the latest recommendations for lung nodule management, but it is much better if you can simply “ask the room” and get an answer in 5 seconds.
This logic inspired me to build a radiology assistant that helps radiologists (my specialty) rapidly access useful information:

This tool allows radiologists to quickly access important, yet difficult-to-remember, bits of information that are commonly looked up. And it lets them do so in a split second by simply “asking the room.”
Hyper-efficient information retrieval is particularly valuable in the setting of a medical emergency.
Imagine a cardiac arrest with a critically ill patient getting chest compressions and electric shocks to restart their heart — it would be awkward and potentially unsafe to use a smartphone, but you can easily consult Alexa. She might retrieve key information from the medical record, ensure optimal timing of chest compressions, coordinate large teams distributed throughout the hospital, and do many other useful things.
Zero-friction information retrieval also facilitates the use of higher quality information. Try asking your doctor how much radiation you get from the body scanner at the airport, and they will probably respond with confused generalities.
But imagine if they could simply “ask the room” and get a better answer in a split second:
For those of us that have been in the airport scanner 10,000 times.
The human brain is imperfect, and voice tools help nudge physicians in the direction of accessing better information and providing better care. And they make it easy.
Conclusion
Hey engineers and healthcare innovators: I encourage you to think about how you can use this platform to build something cool and important that helps people. If you are curious about how I made my voice apps, see the nitty-gritty implementation details here.
If you have an idea for a project or want a physician’s perspective on anything in tech, feel free to reach out on Twitter or LinkedIn. Thanks for reading!
If you enjoyed the article, thought the demo videos were cool, or just appreciate my awesome dog, please hit the “clap” button and/or share…it helps a lot! Thanks. — Kevin
",1079,Healthcare,Kevin Seals,https://medium.freecodecamp.org/smart-speakers-and-a-i-will-give-your-physician-superpowers-38c17bc2f133,"Sensing potential, I decided to hunker down with my trusty corgi, drink a bunch of coffee, and start building the cool voice tools I want to use in my own clinical practice.
In this article I will synthesize my findings, show a bunch of fun demo videos, and explain why smart speakers represent a transformative technology in healthcare.
Why should people that care about healthcare innovation start thinking about smart speakers?
Smart speakers thus offer immense value to all surgeons and proceduralist physicians: they give them their modern technology back.
Many important applications can be built around sterility needs, from software allowing physicians to dictate paperwork during surgical downtime to this simple (but useful) sizing tool:
The tool I am building uses voice to empower the sterile.
The smart speaker allows surgeons to rapidly determine if a particular stent or other device fits within a particular catheter.
Smart speakers facilitate eye contact and patient connection
Smart speakers allow zero-friction access to high quality information
This logic inspired me to build a radiology assistant that helps radiologists (my specialty) rapidly access useful information:
This tool allows radiologists to quickly access important, yet difficult-to-remember, bits of information that are commonly looked up.
The human brain is imperfect, and voice tools help nudge physicians in the direction of accessing better information and providing better care.
Hey engineers and healthcare innovators: I encourage you to think about how you can use this platform to build something cool and important that helps people."
"Computer Vision, Image Processing and FotoNation, from Bucharest, Romania",Notes from the Bucharest.AI Meetup 6.0,"Computer Vision, Image Processing and FotoNation from Bucharest, Romania
Notes from the Bucharest.AI Meetup 6.0
🙌 Amazingly talented speakers, audience and venue location surely make for an amazing Bucharest AI meetup 6.0. Kudos to everyone that took part and exciting to see such great minds and people all in one place, Bucharest AI — the cluster for AI practitioners and enthusiasts.
Powerful things happen when like-minded people connect.
Andrei Țăranu (left), Product Owner and Value Researcher; Liviu Dutu (middle), Research Engineer specialised in Machine Learning technologies from FotoNation; and Marius Leordeanu (right), Lecturer @Facultatea de Automatică și Calculatoare, UPB teaching Computer Vision and Robotics for the MSc in #AI.
This edition we focused:
Challenges, as part of our Community Open Space. We welcomed Andrei Taranu with “Building an ICO-worthy team to support a marketplace for AI services based on distributed TensorFlow”. Want to present your challenge in future Bucharest AI events, kindly register by filling this short form.
Keynote “Vision in Words: in Search for the Link Between Vision and Language” — by Marius Leordeanu, Research Scientist & Associate Professor Computer Vision and Robotics. Marius presented his recent research on bridging the gap between vision and language. This research problem is only in its infancy in AI and is gathering a lot of interest in the field. Topics discussed: potential use cases with impact in everyday life & approach to this task and current results.
Keynote and demo “Revolutionizing Imaging with AI @ FotoNation” — by Liviu Dutu, Research Engineer specialized in Machine Learning technologies. Relying on its world-class imaging and computer-vision expertise, FotoNation is now developing advanced machine learning algorithms for a wide range of mobile and automotive applications. Follow them to explore some of their most captivating technologies, ranging from face analytics to photography enhancement.
Challenges — Building an ICO-worthy team to support a marketplace for AI services based on distributed TensorFlow
Andrei Taranu, Product Owner & Value Researcher @Kondiment in front of the curious Bucharest AI audience.
Andrei codes Java and Python and his challenge presented consists of:
Implementing the Markov-Chain MC (Tangle) security mechanism
Joining a Fin-tech Accelerator and organising an ICO.
He is looking to welcome, in his team, roles of Core Developer — Architect and Engineers.👆Click for slides deck.
Keynote: Vision in Words: in search for the link between vision and language
Marius Leordeanu, Research Scientist & Associate Professor Computer Vision & Robotics in front of Bucharest AI’s captivated audience.
Marius has a Robotics PhD from Hunter College, City University of New York. His contributions on “graph matching” started a distinct scientific branch in computer vision, dedicated to graph matching efficient algos development. Marius started a research group in computational imaging and automatic learning, gathering very talented and passionate students and PhDs from the Politehnica University of Bucharest and Romanian Academy. Marius and his teams are returning real scientific results that are considered internationally.
Under Marius’s guidance, we explored what would we like for a computer vision system to do with an image, as we better understood the differences between human and machine perception over what surrounds us or get to see. We found out about the ‘Discrete Continuum’ relation and were shown some of the results follwoing Marius & his team’s research.


It was grand to have Marius passionately walk us through his findings and results. We hope to enjoy his presence more and more and get exposed to his research often.
👆Click for slides deck.
Keynote & demo: Revolutionizing Imaging with AI @ FotoNation

Liviu Dutu, Research Engineer specialised in Machine Learning technologies demo-ing FotoNation’s face analytics tech that is working with a wide range of head orientations.
Throughout his career, Liviu contributed to the development of technologies such as object detection and tracking, classification, semantic segmentation and single image super-resolution. His work is estimated to be employed in 25% of the smartphones shipped worldwide. He holds a PhD in electrical engineering and has published in major international journals.:)
It’s amazing to see FotoNation’s progress with computational imaging algorithms and the sectors they serve. With over 18 years of breakthroughs they are picturing our world and can’t wait to welcome them at future events. We’ll keep them close and update you on their continuous progress.
👆Click for slides deck.
Working on a computer vision project / idea? We’d love to hear from you!
If computer vision, image recognition or detection is your main focus of a project or idea, we’d love to hear more from you. If you had specific challenges that you could share the learnings from, or are currently in need of a second thought, ping us. If it’s over our league, we’ll connect or pair you with someone that could help. Our local community is growing but so is our global City.ai community. Let’s benefit of all the knowledge around and help each other deliver successful AI products.
Who attended Bucharest AI 6.0?
This was a first! 200 of you AI practitioners and enthusiasts joined us to connect and be part of the local AI scene. All of this despite terrible weather conditions that could have easily kept you at home, chilling and sipping from some hot cocoa.:) Thank you.
This helped us acknowledge even more the massive interest and potential of the local AI scene, and we understand we need to aim for larger venues to properly accommodate you all. Challenge accepted! You keep showing up to our events and we will make sure you have a great time and leave with significant takeaways and connections.
Fun fact: 44% of you joined us for the first time. Thank you and see you soon again! :)

For the other 56% of you who have joined us previously as well, we hope you’re enjoying the meetups more and more, and also noticing the efforts made to implement every good feedback received, while becoming bigger and stronger as a community. Got some more good ideas about event setup, topics or speakers? Jot them here.

Looking to rewatch the streamed meetup content presented? Tune in.
When’s the next meetup? 21st of March!:)
You asked, we answered. Can’t wait to see you there. Get your tickets (they sell very, very fast & we have a tighter seat limitation).:)
What to expect? As EU is looking to build the ‘European AI Alliance’ (broad, diverse community of stakeholders) to set ethical guidelines for AI (fairness, safety, transparency, the future of work, democracy, impact on Charter of Fundamental Rights), we couldn’t see more fit that we allocate a full meetup event towards understanding more on the topic. AI Policy, Trends, Markets and AI-based products investors-worthy will be the topics discussed on the 21st of March. Agenda covers:
What is an investor looking for when it comes to AI ideas/products/services, by Eamonn Carey, Managing Director, Techstars London.
Citizen AI — Raising AI to benefit business and society, by Florin Soare, Accenture Technology Romania Lead. As Artificial Intelligence expands further into society, the business accountability around raising a responsible and explainable AI will rapidly grow. By raising AI for responsibility, fairness, and transparency, businesses can create a collaborative, powerful new member of the workforce. This session will cover the top 5 IT trends of the moment, discovered in Accenture’s 2018 Tech Vision.
Introducing the AI Policy Series by Yolanda Lannquist, AI Policy Researcher at The AI-Initiative, Harvard Kennedy School.
We kindly ask you to reserve a seat only if the topics above are right up your street or if you’re really into them. Given the venue space limitation, we are forced to receive up to 100 seats. Until we find larger venues, we’d appreciate your understanding. We welcome all of you to tune in to our streamed live event via our Facebook page.:)
Got your AI groove on? Get involved, join us, find the right AI tech for you and let’s enable more of us in understanding and practicing AI.
",1224,Machine Learning,Alexandra Petrus,https://medium.com/s/story/computer-vision-detection-machinelearning-bucharestai6-dcdd4fb8c67f,"🙌 Amazingly talented speakers, audience and venue location surely make for an amazing Bucharest AI meetup 6.0.
Andrei Țăranu (left), Product Owner and Value Researcher; Liviu Dutu (middle), Research Engineer specialised in Machine Learning technologies from FotoNation; and Marius Leordeanu (right), Lecturer @Facultatea de Automatică și Calculatoare, UPB teaching Computer Vision and Robotics for the MSc in #AI.
We welcomed Andrei Taranu with “Building an ICO-worthy team to support a marketplace for AI services based on distributed TensorFlow”.
Want to present your challenge in future Bucharest AI events, kindly register by filling this short form.
Keynote “Vision in Words: in Search for the Link Between Vision and Language” — by Marius Leordeanu, Research Scientist & Associate Professor Computer Vision and Robotics.
Marius presented his recent research on bridging the gap between vision and language.
Keynote and demo “Revolutionizing Imaging with AI @ FotoNation” — by Liviu Dutu, Research Engineer specialized in Machine Learning technologies.
Relying on its world-class imaging and computer-vision expertise, FotoNation is now developing advanced machine learning algorithms for a wide range of mobile and automotive applications.
Challenges — Building an ICO-worthy team to support a marketplace for AI services based on distributed TensorFlow
Andrei Taranu, Product Owner & Value Researcher @Kondiment in front of the curious Bucharest AI audience.
He is looking to welcome, in his team, roles of Core Developer — Architect and Engineers.👆Click for slides deck.
Marius Leordeanu, Research Scientist & Associate Professor Computer Vision & Robotics in front of Bucharest AI’s captivated audience.
Marius started a research group in computational imaging and automatic learning, gathering very talented and passionate students and PhDs from the Politehnica University of Bucharest and Romanian Academy.
Liviu Dutu, Research Engineer specialised in Machine Learning technologies demo-ing FotoNation’s face analytics tech that is working with a wide range of head orientations.
If computer vision, image recognition or detection is your main focus of a project or idea, we’d love to hear more from you.
As EU is looking to build the ‘European AI Alliance’ (broad, diverse community of stakeholders) to set ethical guidelines for AI (fairness, safety, transparency, the future of work, democracy, impact on Charter of Fundamental Rights), we couldn’t see more fit that we allocate a full meetup event towards understanding more on the topic."
Implementation of Gradient Descent in Python,"Every machine learning engineer is always looking to improve their model’s performance. This is where optimization, one of the most…","Implementation of Gradient Descent in Python
Every machine learning engineer is always looking to improve their model’s performance. This is where optimization, one of the most important fields in machine learning, comes in. Optimization allows us to select the best parameters, associated with the machine learning algorithm or method we are using, for our problem case. There are several types of optimization algorithms. Perhaps the most popular one is the Gradient Descent optimization algorithm. The first encounter of Gradient Descent for many machine learning engineers is in their introduction to neural networks. In this tutorial, we will teach you how to implement Gradient Descent from scratch in python. But first, what exactly is Gradient Descent?
What is Gradient Descent?
Gradient Descent is an optimization algorithm that helps machine learning models converge at a minimum value through repeated steps. Essentially, gradient descent is used to minimize a function by finding the value that gives the lowest output of that function. Often times, this function is usually a loss function. Loss functions measure how bad our model performs compared to actual occurrences. Hence, it only makes sense that we should reduce this loss. One way to do this is via Gradient Descent.
A simple gradient Descent Algorithm is as follows:
Obtain a function to minimize F(x)
Initialize a value x from which to start the descent or optimization from
Specify a learning rate that will determine how much of a step to descend by or how quickly you converge to the minimum value
Obtain the derivative of that value x (the descent)
Proceed to descend by the derivative of that value multiplied by the learning rate
Update the value of x with the new value descended to
Check your stop condition to see whether to stop
If condition satisfied, stop. If not, proceed to step 4 with the new x value and keep repeating algorithm
Implementing Gradient Descent in Python
Here, we will implement a simple representation of gradient descent using python. We will create an arbitrary loss function and attempt to find a local minimum value for that function.
Our function will be this — f(x) = x³ — 5x² + 7
We will first visualize this function with a set of values ranging from -1 and 3 (arbitrarily chosen to ensure steep curve)
Here is the result:

We will then proceed to make two functions for the gradient descent implementation:
The first is a derivative function: This function takes in a value of x and returns its derivative based on the initial function we specified. It is shown below:
The second is a Step function: This is the function where the actual gradient descent takes place. This function takes in an initial or previous value for x, updates it based on steps taken via the learning rate and outputs the most minimum value of x that reaches the stop condition. For our stop condition, we are going to use a precision stop. This means that when the absolute difference between our old and updated x is greater than a value, the algorithm should stop. The function will also print out the minimum value of x as well as the number of steps or descents it took to reach that value.
This function is shown below:
Next, we proceed to plot the gradient descent path as shown below:


The importance of Gradient Descent in Machine Learning is one that will be encountered all through your machine learning journey. This is why it is imperative that you understand the inner workings of this algorithm. This tutorial has introduced you to the simplest form of the gradient descent algorithm as well as its implementation in python. Now, you have an intuitive understanding of this algorithm and you are ready to apply it to real world problems.
Please check the complete iPython notebook code here.
Original Post: https://tech-quantum.com/implementation-of-gradient-descent-in-python/
",1086,Machine Learning,Deepak Battini,https://medium.com/s/story/implementation-of-gradient-descent-in-python-a43f160ec521,"Optimization allows us to select the best parameters, associated with the machine learning algorithm or method we are using, for our problem case.
Gradient Descent is an optimization algorithm that helps machine learning models converge at a minimum value through repeated steps.
Essentially, gradient descent is used to minimize a function by finding the value that gives the lowest output of that function.
Specify a learning rate that will determine how much of a step to descend by or how quickly you converge to the minimum value
Proceed to descend by the derivative of that value multiplied by the learning rate
If not, proceed to step 4 with the new x value and keep repeating algorithm
We will then proceed to make two functions for the gradient descent implementation:
This function takes in an initial or previous value for x, updates it based on steps taken via the learning rate and outputs the most minimum value of x that reaches the stop condition.
This tutorial has introduced you to the simplest form of the gradient descent algorithm as well as its implementation in python."
11 Lines Of Code Will Replace Millions Of Jobs. What Can We Do About It?,"AI is eating the industrial economy as we know it. But how, why, and what can we do about it?","

11 Lines Of Code Will Replace Millions Of Jobs. What Can We Do About It?
AI is eating the industrial economy as we know it. But how, why, and what can we do about it?
(Maddie Anderson/Rantt News)
Today, if you work in computer science, it’s hard to go a week without a new doomsday theory about machines either being the death of us or expediting our own destruction by giving us new and more efficient tools to kill each other. It’s been such a frequent occurrence in my field that debunking some of the more exotic predictions with real science was a fun side job for years. But given a recent advance in computer chips, I’m going to reverse course on my trademark skepticism and tell you that I have seen into the future and that future is grim.
For the last six years, public techies have been warning everyone in sight that machines are coming for their jobs. We’ve pointed out that the collapse of blue-collar manufacturing wasn’t caused primarily by outsourcing, but by automation. We sounded the alarm that the countries where manufacturing jobs were indeed outsourced were about to suffer the same fate. And we watched in horror as the Treasury secretary of the current administration said that AI able to take people’s jobs is 50 to 100 years away, much the same way we’d look at someone who was trying to explain to us that human spaceflight is at least a century down the line and the Moon landing was faked.
Don’t get me wrong, automation is already in full swing and I’ve personally replaced what once were, or would’ve been, full-time, white collar jobs in offices with a few hundred or thousand lines of code that run on a server no one aside from IT sees, thus making it seem as if those jobs were still around, just waiting for HR to start the hiring process. But with new computer chips which borrow heavily from the basic processes of the human brain, that automation is about to get a whole lot more sophisticated and replace a whole lot more jobs than we may have even thought possible. And it’s going to catch our leaders with their pants down. Again.
You’d probably forgive me if I didn’t dwell on the details and skipped to the part where we either try to figure out a solution to the steady encroachment of your digital replacements or rue our rotten fates and plan on how best to drown our sorrows. But skipping the details and a lack of understanding of what we’re doing at the cutting edge of computer science and AI is what caused this mess in the first place, and only by understanding what’s out there, how it works, and what it can do, can we come up with workable policy ideas, none of which will be simple or straightforward, and most of which will require a radical rethink of our society in the long run.
If you stick with it for just a little while, you’ll hopefully come away with an informed idea of how modern AI works, why many of the advancements no one is talking about are so critical, and learn some interesting things about your own brain in the process. Yes, this will involve math and science, the things far too many of our politicians proudly tell us they shy away from or giddily misuse, but we’re long past the point where ignoring what they have to say is a luxury any of us can afford. With that in mind, let’s get started.

How Androids Learn What Electric Sheep Are
As we already established, when people talk about jobs going away, they think foreign workers or a distant sweatshop. It’s an idea that has sticking power because the villains in question have faces and are easy to picture. Saying that nearly 9 in 10 manufacturing jobs were lost to automation is extremely abstract. What does said automation look like, other than large robots in factories we’re all familiar with by now? Well, it looks somewhat like this…

This is an algorithm written in a computer language called Python by a Ph.D. student from Oxford. It could be written in any language, however, because what matters is what it does, not how it’s written. And what it does is allow a computer to learn in a way that’s roughly similar to how your brain would, albeit with a few big caveats. Backpropagation is just one of the several ways machines can be trained, and this learning by trial and error resembles how your mind works, though only in the vaguest possible terms.
Your brain learns primarily by repetition. As you’re exposed to stimuli, a mix of electrical signals created by the interactions of sodium and potassium ions, and neurotransmitters, reinforce connections between certain brain cells, called neurons, creating a dedicated path in your mind that will help you execute a task. Or at least that’s the extremely simplified picture on which computer scientists in the 1960s based their ideas of how to make computers learn, realizing early on that entering all the data a machine would need to know to process data for decision making would take centuries.
The result of their efforts was something known as artificial neural networks, which use code like backpropagation to guess their way into doing something correctly. They’re fed a set of inputs and a desired set of outputs, then given tens of thousands of chances to guess what inputs are the most important in coming up with the right solutions so they can calculate future answers. Every time they make a mistake, they calculate their average degree of error, update their guesstimates of the inputs’ importance by this average, and try again until they get it right. This is backpropagation in a nutshell.
Now, the computer doesn’t understand what it’s doing or why, or any of the concepts involved. (That’s a topic in AI known as symbol grounding and it’s far out of the scope of this article.) It’s like a dog learning that shake means raising one of its front paws and letting you take this paw in your hand. It may have taken a while to understand this is what you want when you say “shake” and get it wrong a lot of times. It won’t understand the concept of shaking hands or why you want to teach it this, but it will remember the word and know what you want it to do now in exchange for a treat. Substitute its attempts to learn for statistical formulas, remove the treat because the computer doesn’t actually want anything to motivate it to keep solving the problem, and you’ll end up with a rudimentary AI.
Much, much larger versions of these artificial neural networks power Siri, Alexa, and Cortana, fuel your Netflix and Amazon recommendations and arrange your social media timelines. This algorithm, or some version of it, is behind virtually everything you do in the digital world. And if this is your first time hearing about artificial neural networks and backpropagation, you’ve been none the wiser to the software that’s been running your digital life for close to a decade now.

All Your Jobs Belong To Us
All right, all that’s fine and good, but what exactly does that mean for jobs? It means that if your job involves making decisions or taking actions based on well-defined criteria that rarely change, your job can now be taught to a computer. It can weigh the factors and decide for you, or ping a different piece of code to carry out an action. Different neural networks can even be chained together to make more and more complex decisions and plot more elaborate plans of actions. Best case scenario, you’ll be kept around to keep the software in check as it makes millions of decisions and carries out thousands of different workflows. Worst case? You’re obsolete.
That’s really the reason why you should be worried. It’s not that you’ve just been given some help to do your job faster or will move into a different job. Your job is gone with nothing to replace it planned. During the halcyon days of industrialization, humans were needed to make ever more complicated decisions about how the relevant work had to be done. In the current post-industrial era, the sheer scope and complexity of the work are so vast, we actually need to have computers finding inefficiencies and in the supply chain. Humans alone either aren’t enough or have become the inefficiencies in question.
At least artificial neural networks consume a lot of power and as a result, have a fairly limited mental bandwidth compared to humans. That means there’s hope for us yet, right? Well, not for long. A laboratory in UCLA is developing a hardware implementation of the algorithm we just dove into, perfect for robots, servers, and even household computers, sipping energy while putting the power of AI networks that could emulate the work done by multi-million dollar setups today in a device that will fit on the tip of your finger and consume the same amount of energy as your cell phone.
AI will not only become even more prevalent but get orders of magnitude cheaper. Even a local mom and pop could have the computing capabilities Facebook has today within a decade or so. Millions of what used to be entry-level jobs for humans are being replaced by cheap, intelligent machinery. With these artificially intelligent chips, millions of junior white collar jobs will be the next to go. Whatever jobs will be left will be either highly technical and either maintain and improve this automation, teach future generations how to do this, or stick to law, science, research, and creative tasks where machines will no doubt be used to aid all of them in some way.
None of this would be catastrophic if we actually took the warnings seriously and had a plan on how to transition people to new potential jobs. But we didn’t. Even worse, the party currently in power seems far more interested in saving coal mining, an industry being rapidly automated and employing fewer people than Arby’s, than investing in education and training for future jobs, so it’s unlikely any new ideas are going to come down the pipeline and translate into actionable policy anytime soon.

So this is where we are. Machines are steadily taking over job after job thanks to math that allows them to learn new tasks and can be scaled to meet growing logical and cognitive complexity, and they’re about to rapidly speed up their invasion of the human workplace. In the meantime, our leaders ignored the warnings and have been sitting on their hands for decades while peddling half-baked ideas about the economy, and severely undermining science and education with asinine overhauls and brutal budget cuts. Even worse, they’re pushing us in reverse on the issue.
The current tax plan in Congress will actually sabotage higher education by taxing scholarships and student loan payments, which will especially hurt future scientists and engineers by taxing their average $30,000 in stipends as nearly $80,000 in income by including their tuition waivers. Meanwhile, we’ll institute deductions for outsourcing and private jets, while taking away deductions for teachers’ buying classroom supplies and cutting healthcare to those who won’t be able to find jobs thanks to automation and inability to afford retraining. This lets you know exactly how backwards the priorities held by the majority of our leaders are.

Featured Petition
Sign the Petition: ACT NOW for 130 million girls out of school
Educate a girl in one of the world's poorest countries, and it won't just change her world. It could change yours too.…www.thepetitionsite.com

The Problem Downloaded Around The World
Globalization also played a big part in growing the AI threat. Gone are the days when executives of big companies worried about having enough employees to get the job done and making sure they could afford to buy the products they manufactured. Machines can now do more and more jobs, and they’re much cheaper and harder working than any human. And if their employers can’t afford to buy their products, better off foreigners can, and will. Basically, they don’t need to be good corporate citizens if they have a global customer base.
So while globalization is good at creating new opportunities for people with certain skill sets, anyone lacking those skills can be effortlessly cast aside with no side-effects to the bottom line. Without pressure to train new generations of employees, just compete for far fewer people, far too many companies feel little pressure to lobby for real reforms in education and job training, and at any rate, governments don’t seem to be listening. In America, out of touch senior citizens are making policy and it shows in their befuddled inaction and quarter-measures when it comes to AI, on top of their utterly backwards digital strategy.
For the companies that are actually worried, relocating to a nation better equipped to handle the post-industrial world is always an option, although that’s not great news for a lot of countries that are currently awash in manufacturing jobs. The same seismic shifts that rattled factories in the First World can play out exponentially faster in developing nations because the bugs have been worked out, the technology got cheaper, and adoption can be streamlined. This is why the UN predicts that two-thirds of all jobs in the developing world will be gone in less than a generation.
While this may seem like a scathing indictment of the corporate realm, that’s not really the case either. This is just the world we live in, and it’s not a company’s job to educate its future workers or craft policies that will allow us to adapt to the future. That’s why we pay taxes. Governments are running schools and determining budgets for crucial programs we need for future-proofing our workforce, not industry. The day when between half and two-thirds of humans are out of work thanks to AI and lack of investment in the future is still over two decades away, and most businesses are simply not designed to plan on multi-decade time scales. And even if they did, they’d need governments to go along with their recommendations.
But not only is the advice of tech companies which are implementing AI faster than the researchers, routinely ignored, our politicians are pushing for tax cuts and doubling down on failed policies. While this never made much sense, pushing for them right now is particularly bad for two reasons. First is that the cuts are predicated on the notion that companies will hire more people with the money they save, but with machines and AI eliminating jobs at an accelerating rate, for what jobs would they even hire? Second, the proposed tax plan would actually raise taxes on the middle class to pay for the loss of revenue as they’re squeezed by AI. So you see, this is a perfect storm of inaction on one end and ineptitude on the other.
Donald Trump and The Future of Globalization, Brookings
This is where the radical rethink mentioned earlier comes into play. We need to accept that a) many current jobs are gone and are either obsolete now, or will be obsolete soon because they no longer require a human to do them, b) numerous other jobs are on the chopping block, c) we need policies that make education and mobility affordable and easy, and d) the future involves a lot of curiosity and creatively-driven jobs that are only enhanced and accelerated by computers, and we need to start training people for them, not save failing industries.

How To Start Solving Tomorrow’s Problems Today
All this sounds common sense so far, right? Where’s the radical part? Well, the radical part is in the execution of the policies we need to implement based on these four basic new truths of the post-industrial world.
Truth #1: We need to make big changes both to our current system and our funding priorities. Business as usual and gradual change simply aren’t workable anymore, they haven’t been since our leaders twiddled their thumbs for the better part of three decades as warnings about automation and the need to set ourselves up for success in a machine-driven world poured in from experts.
Truth #2: We need to provide a form of universal healthcare, decoupling this benefit from employment. When we need a mobile, flexible workforce, ready to re-educate itself at a moment’s notice for the Next Big Thing, limiting their ability to change jobs, or take time to retrain by trying their benefits to who employs them makes no sense. Otherwise, we will make workers choose between training themselves for some new and desirable skillset and losing access to doctors if they or anyone in their families get sick, or risking going into crushing debt to pay for treatment.
Truth #3: We have to invest in education. We don’t just need to mint more college degrees mind you, but make training for new jobs easier and far more affordable. We need to stop teaching to standardized tests and return to a more discovery-driven, exploratory education style, and consider tracking in high school to help steer students towards careers for which they demonstrate a distinct aptitude.
Truth #4: Finally, we need to pump more money into basic, curiosity-driven research to tell us what’s possible and what new things we can try to invent, expanding government-funded labs and R&D departments in the corporate world. While conservatives seem to hold practical science, like AI, medications, weapons, and gadgets, in much higher esteem than basic science like biology, astronomy, and physics, without basic science, engineers wouldn’t know what they could build and doctors wouldn’t know how they could treat diseases and injuries. One cannot exist without the other over the long term.
Basically, we need to adopt the opposite of virtually every initiative we’ve seen over the past decade because what we’re doing is obviously and clearly not working. It’s enabling the worst actors in the developed world to grab power based on pitching fear and nostalgia, it’s setting up our kids for obvious failure, and it’s hobbling our future right before our eyes, all because the people we placed in power refuse to open their eyes and admit they don’t understand how the world works anymore. How many of them have heard of backpropagation? How many read a primer about AI? How many of them are aware of the artificial neural networks on a chip that will bring Facebook’s and Amazon’s setups to businesses of any size sooner rather than later?
Forget about liberal vs. conservative for a minute. Diving into AI and how it’s changing work for people across the world shows that the divide is more about people who fear the future and want to slam their nations in reverse and people who understand that if we don’t adapt to new challenges, we’ll be worse off while those who do will reap the benefits. No one is entitled to be number one in anything without working to preserve that status, and no amount of pretending that the future isn’t coming for us has ever saved anyone from eventually feeling its full brunt.
Our only two options are to adapt and try to succeed, or hide our heads in the sand and suffer. Americans have always tried to choose the former rather than the latter, so if this time we decide to cling to the past and let history overtake us, it will be on us. And is this really how we want a machine takeover to play out? Not with killer robots with access to nuclear weapons deciding to wipe us out but them taking our jobs and we meekly let them so our graying leaders could rummage through our pockets one last time and mew some conservative buzzwords while reciting what passes in their minds as a motivational speech?

Featured Petition
Sign the Petition: Protect the Clean Power Plan from the Trump Administration
Don't let the Trump Administration take us backwards. Tell Scott Pruitt: Revoking the Clean Power Plan is unacceptable…www.thepetitionsite.com


If you’d like to help support projects like these, please consider helping us fund our operation by joining our community
What We’re Offering
$3 — Make A Difference! Support Independent Journalism, get exclusive posts, and become a founding member of Rantt, Inc.
$10 — Join The Team! 24/7 access to Rantt’s #Newsroom on Slack. Here you’ll be able to spectate & speak with the writers, editors, and founders as we discuss news and current events in real-time.
$20 — Get Published! Work one-on-one with your own personal Rantt Editor to develop and write stories that you’d like to see published on our site. You come up with the story and we’ll make sure it’s Rantt ready in no time!
To see more of our offerings, check out our Patreon.
JOIN US
Rantt News is creating News Analysis and Investigative Journalism | Patreon
Become a patron of Rantt News today: Read posts by Rantt News and get access to exclusive content and experiences on…www.patreon.com
Thanks again for everything. We love and appreciate you!
Sincerely,
The Rantt Team
Stay in the know and subscribe to our newsletter by following us on Rantt
Follow us on Twitter: @RanttNews
Join us on Facebook: /RanttNews

",3587,Artificial Intelligence,Greg Fish,https://medium.com/s/story/11-lines-of-code-will-replace-millions-of-jobs-heres-what-we-can-do-about-it-a63746e0a7ec,"And we watched in horror as the Treasury secretary of the current administration said that AI able to take people’s jobs is 50 to 100 years away, much the same way we’d look at someone who was trying to explain to us that human spaceflight is at least a century down the line and the Moon landing was faked.
Don’t get me wrong, automation is already in full swing and I’ve personally replaced what once were, or would’ve been, full-time, white collar jobs in offices with a few hundred or thousand lines of code that run on a server no one aside from IT sees, thus making it seem as if those jobs were still around, just waiting for HR to start the hiring process.
But with new computer chips which borrow heavily from the basic processes of the human brain, that automation is about to get a whole lot more sophisticated and replace a whole lot more jobs than we may have even thought possible.
But skipping the details and a lack of understanding of what we’re doing at the cutting edge of computer science and AI is what caused this mess in the first place, and only by understanding what’s out there, how it works, and what it can do, can we come up with workable policy ideas, none of which will be simple or straightforward, and most of which will require a radical rethink of our society in the long run.
If you stick with it for just a little while, you’ll hopefully come away with an informed idea of how modern AI works, why many of the advancements no one is talking about are so critical, and learn some interesting things about your own brain in the process.
Yes, this will involve math and science, the things far too many of our politicians proudly tell us they shy away from or giddily misuse, but we’re long past the point where ignoring what they have to say is a luxury any of us can afford.
Backpropagation is just one of the several ways machines can be trained, and this learning by trial and error resembles how your mind works, though only in the vaguest possible terms.
Or at least that’s the extremely simplified picture on which computer scientists in the 1960s based their ideas of how to make computers learn, realizing early on that entering all the data a machine would need to know to process data for decision making would take centuries.
And if this is your first time hearing about artificial neural networks and backpropagation, you’ve been none the wiser to the software that’s been running your digital life for close to a decade now.
In the current post-industrial era, the sheer scope and complexity of the work are so vast, we actually need to have computers finding inefficiencies and in the supply chain.
A laboratory in UCLA is developing a hardware implementation of the algorithm we just dove into, perfect for robots, servers, and even household computers, sipping energy while putting the power of AI networks that could emulate the work done by multi-million dollar setups today in a device that will fit on the tip of your finger and consume the same amount of energy as your cell phone.
Whatever jobs will be left will be either highly technical and either maintain and improve this automation, teach future generations how to do this, or stick to law, science, research, and creative tasks where machines will no doubt be used to aid all of them in some way.
Even worse, the party currently in power seems far more interested in saving coal mining, an industry being rapidly automated and employing fewer people than Arby’s, than investing in education and training for future jobs, so it’s unlikely any new ideas are going to come down the pipeline and translate into actionable policy anytime soon.
Machines are steadily taking over job after job thanks to math that allows them to learn new tasks and can be scaled to meet growing logical and cognitive complexity, and they’re about to rapidly speed up their invasion of the human workplace.
Meanwhile, we’ll institute deductions for outsourcing and private jets, while taking away deductions for teachers’ buying classroom supplies and cutting healthcare to those who won’t be able to find jobs thanks to automation and inability to afford retraining.
Gone are the days when executives of big companies worried about having enough employees to get the job done and making sure they could afford to buy the products they manufactured.
Machines can now do more and more jobs, and they’re much cheaper and harder working than any human.
Without pressure to train new generations of employees, just compete for far fewer people, far too many companies feel little pressure to lobby for real reforms in education and job training, and at any rate, governments don’t seem to be listening.
For the companies that are actually worried, relocating to a nation better equipped to handle the post-industrial world is always an option, although that’s not great news for a lot of countries that are currently awash in manufacturing jobs.
This is just the world we live in, and it’s not a company’s job to educate its future workers or craft policies that will allow us to adapt to the future.
The day when between half and two-thirds of humans are out of work thanks to AI and lack of investment in the future is still over two decades away, and most businesses are simply not designed to plan on multi-decade time scales.
We need to accept that a) many current jobs are gone and are either obsolete now, or will be obsolete soon because they no longer require a human to do them, b) numerous other jobs are on the chopping block, c) we need policies that make education and mobility affordable and easy, and d) the future involves a lot of curiosity and creatively-driven jobs that are only enhanced and accelerated by computers, and we need to start training people for them, not save failing industries.
Well, the radical part is in the execution of the policies we need to implement based on these four basic new truths of the post-industrial world.
Business as usual and gradual change simply aren’t workable anymore, they haven’t been since our leaders twiddled their thumbs for the better part of three decades as warnings about automation and the need to set ourselves up for success in a machine-driven world poured in from experts.
When we need a mobile, flexible workforce, ready to re-educate itself at a moment’s notice for the Next Big Thing, limiting their ability to change jobs, or take time to retrain by trying their benefits to who employs them makes no sense.
We don’t just need to mint more college degrees mind you, but make training for new jobs easier and far more affordable.
Truth #4: Finally, we need to pump more money into basic, curiosity-driven research to tell us what’s possible and what new things we can try to invent, expanding government-funded labs and R&D departments in the corporate world.
Diving into AI and how it’s changing work for people across the world shows that the divide is more about people who fear the future and want to slam their nations in reverse and people who understand that if we don’t adapt to new challenges, we’ll be worse off while those who do will reap the benefits.
Not with killer robots with access to nuclear weapons deciding to wipe us out but them taking our jobs and we meekly let them so our graying leaders could rummage through our pockets one last time and mew some conservative buzzwords while reciting what passes in their minds as a motivational speech?
Work one-on-one with your own personal Rantt Editor to develop and write stories that you’d like to see published on our site.
You come up with the story and we’ll make sure it’s Rantt ready in no time!"
Team Profile: Meet Jungkap,"Machine learning engineering, two cats, and winters in Michigan","Team Profile: Meet Jungkap
As a yet minuscule startup, each member holds a significant power over the overall atmosphere of the team. And in our ultimate quest to make big waves in the data world, we need to make sure that the people at the helm are at least kind of cool. We think we’ve done a pretty good job so far in assembling a society of unique but equally driven members.
So we bring you this seven-part series, one of each devoted to interviewing each of our members in detail, to give you an in-depth glimpse into the people responsible for bringing you the future of machine learning with Daria. Plus, we peppered the interviews with questions from Dr. Aron’s “The 36 Questions that Lead to Love”*, cherry picked to make work appropriate and concise, but interesting.
(*actually falling in love with our members highly discouraged)
Jungkap, the most recent addition to our team, made the move from sunny Santa Clara to Seoul, a city that is slowly freezing over as you read this. But he is used to the cold, Jungkap assures us, having spent his doctorate years in the apocalyptic winters of Michigan. When he’s not busy helping build Daria’s machine learning engine, Jungkap devotes his time to re-exploring Korea and taking care of his cats Jolie and Brad (named so before the tragic dissolve of Brangelina). Learn more about him here!

Tell us about your role at XBrain.
JP: I joined the team as a machine learning engineer, and my main task is to develop our machine learning engine. I have the task of researching and finding solutions to obstacles that hinder people from using automated machine learning technology with ease.
What does a typical work day look like for you, morning to evening?
JP: I get to work at about 9:15 AM (*the earliest, we note, out of any of the members), and check the Slack messages and emails I got overnight. Since I concentrate the best in the morning, I take a look at relevant articles and dissertations then. Since I didn’t major in machine learning at school, there’s a lot I still have quite a bit to learn, learning’s still a big part of my work process. After I’ve warmed up a bit, I study the code that’s already been written, and develop the parts that need to be developed. Then I have lunch with the team, which is a part of our culture that I really enjoy — a set meal time and a chance to have a conversation with other members. Today I did investigation into an issue we had with the machine learning engine, and worked on how to solve that problem based on my discoveries. I think I’ll be working on constructing that idea into actuality, with a lot of validation, tests, trial and error.
What are the parts of your job that you enjoy the most?
JP:I enjoy enhancing and optimizing processes, and actually seeing improvement after I’ve worked on something. I’m working on improving the system that we have right now, but a long-term project we have in mind is developing technology of XBrain’s own, and figuring out the needs of our customers. In order to do that, I’m spending a lot of time looking into any issues that we have with our current technology, hoping to get insight from the process.
What are the least enjoyable/most challenging parts of your job?
JP:The most challenging, rather than the least enjoyable, is issue definition. There are four types of situations to what can happen: either I find a problem that’s already been found, or something that’s so insignificant that no one cares, something that’s unsolvable, and, finally, an issue that’s both important and solvable. The fourth is what we’re going after, and the process is long and arduous, but I do enjoy it to a certain extent.
Pick one item on your desk that tells us something about you.
JP:I don’t have much stuff on my desk, which is something I also noticed about some of the Silicon Valley companies I visited while I was working in the States, like Twitter or LinkedIn. Most engineers’ desks just had computers on them, and I appreciate that sort of simplicity.
Jungkap keeps things on his desk simple
What made you go into machine learning?
JP:I was on the user end of machine learning technology in grad school and at work thereafter, and felt that the process of utilizing and understanding tools was too complex and difficult. I thought that I might find it fulfilling to optimize this process myself by becoming a machine learning engineer myself.
Why XBrain?
JP:First off, I really liked how the team was set up, people-wise. I was also struck by the competency of the members and the company culture, which suited me well. The values that XBrain pursues, and the ideas that it had about the future of machine learning technology was in line with my own. Not to see it simply as a source of profit, but as something that could potentially bring a lot of people a great deal of help.
As our most recent member, what’s a vision you have for our team?
JP:It’s not so much a vision as a direction we should be heading in — despite how machine learning is such a huge buzzword now, I think it’s still in the process of research and development. A lot of work needs to be done before it can start having a real impact in the world. What our role is, then, is to look far ahead and start with the basics.
Recommend a movie for our next Cinema Society, please.
JP:Downsizing, which hasn’t come out in Korean theaters yet, but I think it presents a lot of points for discussion.
If you could sum up XBrain in three words or less?
Serious, but quirky.
If you could have dinner with any XBrain member, who would it be and why?
JP: JY — we haven’t really gotten a chance to share a meal, and I feel like he’d have some interesting stories
What can you tell us about the JP 10 years from now?
JP:He will probably be a more seasoned machine learning engineer, from his 10 years of research and studying. I’m a novice engineer now, but I’d like to be in a more senior position then, mentoring younger engineers.
Given the choice of anyone in the world, whom would you want as a dinner guest?
JP:Carl Sagan, who first got me interested in science and technology. In my head, he’s this benevolent father figure who would offer to mentor me.
Would you like to be famous? In what way?
JP:No…
What would constitute a “perfect” day for you?
JP:I think a “perfect” day is a day that’s yet to come. Is that too weird to publish?
If you were able to live to the age of 90 and retain either the mind or body of a 30-year-old for the last 60 years of your life, which would you want?
JP:The body, definitely. Minds can mature — bodies not so much.
For what in your life do you feel most grateful?
JP:Probably soundness of mind and body.
If you could wake up tomorrow having gained any one quality or ability, what would it be?
JP:Speedier comprehension upon reading something?
What is the greatest accomplishment of your life?
JP: Forging strong relationships with good people.
What, if anything, is too serious to be joked about?
JP:It depends on the audience, I think. Anything that they might consider offensive, or a weak spot, is off limits.
Your house, containing everything you own, catches fire. After saving your loved ones and pets, you have time to safely make a final dash to save any one item. What would it be? Why?
JP: My hard drive — it has everything on it.
",1318,Machine Learning,Eunsoo Kim (@XBrain),https://medium.com/s/story/team-profile-meet-jungkap-35d6d9a967bb,"We think we’ve done a pretty good job so far in assembling a society of unique but equally driven members.
So we bring you this seven-part series, one of each devoted to interviewing each of our members in detail, to give you an in-depth glimpse into the people responsible for bringing you the future of machine learning with Daria.
When he’s not busy helping build Daria’s machine learning engine, Jungkap devotes his time to re-exploring Korea and taking care of his cats Jolie and Brad (named so before the tragic dissolve of Brangelina).
I have the task of researching and finding solutions to obstacles that hinder people from using automated machine learning technology with ease.
Since I didn’t major in machine learning at school, there’s a lot I still have quite a bit to learn, learning’s still a big part of my work process.
Then I have lunch with the team, which is a part of our culture that I really enjoy — a set meal time and a chance to have a conversation with other members.
Today I did investigation into an issue we had with the machine learning engine, and worked on how to solve that problem based on my discoveries.
I’m working on improving the system that we have right now, but a long-term project we have in mind is developing technology of XBrain’s own, and figuring out the needs of our customers.
In order to do that, I’m spending a lot of time looking into any issues that we have with our current technology, hoping to get insight from the process.
JP:I was on the user end of machine learning technology in grad school and at work thereafter, and felt that the process of utilizing and understanding tools was too complex and difficult.
I thought that I might find it fulfilling to optimize this process myself by becoming a machine learning engineer myself.
The values that XBrain pursues, and the ideas that it had about the future of machine learning technology was in line with my own.
JP:It’s not so much a vision as a direction we should be heading in — despite how machine learning is such a huge buzzword now, I think it’s still in the process of research and development.
A lot of work needs to be done before it can start having a real impact in the world.
JP:He will probably be a more seasoned machine learning engineer, from his 10 years of research and studying."
"Simplicity, Not Suffering",We have been running through simplicity in my community group and it has been one of the more life-giving topics that we have covered over…,"
Simplicity, Not Suffering
We have been running through simplicity in my community group and it has been one of the more life-giving topics that we have covered over the last 3+ years. We’re focusing on Matthew 6: 25–34 and our key areas of focus include our Thoughts, Emotions, Will, Behavior, and Social Interactions.
www.stanway.org
An interesting question was posed in our group around “how do we live a simple life when we are naturally inclined to desire nice/expensive things?” She went on to explain her desire to buy expensive things for her house, but contrasted this desire with how she has walked into homes that are simple, yet beautiful — and wondered how we get to that in all phases of our life.
I think “simple”, needs to be followed with, “yet beautiful” in our Christian walk or else we will become burned out. What do I mean by this?
I’d say that 75% of the clothes that I wear our Bonobos. Bonobos had a simple mantra when starting the company — they set out to build the best fitting pair of pants in the world, and in my opinion they have succeeded. All of their clothes are relatively simple and fit really well. There are no logos. Contrast this choice of clothing with what I may have preferred in high school or college — there was a race to see who could find the shirt with the biggest Polo horse or who could own the most shirts with a Lacoste alligator on the front. I couldn’t afford the Burberry shirts with the full print pattern that took over the entire shirt, but probably would have worn one.
These shirts were expensive, but they looked bad — they didn’t fit.
I guess we were on a clothing discussion kick this week in our Bible studies because in our small group we also brought up the fact that if wear “loud” clothes, people notice if you wear the same thing twice — however, if you were to wear a plain blue, nice-fitting, button down every day to work, most probably wouldn’t notice if you wore the same thing for a year straight.
In these examples, if you have an epiphany and decide that you want to wear simpler, less brand-centric clothes, it doesn’t mean that you have to transition to wearing rags. Simplicity doesn’t have to sacrifice function.
In technology, Apple has won market share because of their simple design principles. A poster in our office reads something along the lines of “The hardest part isn’t adding something, it is taking something away” — referencing features.
Simplicity is good. It removes clutter.
Going through the list of focuses for our small group, here are some ideas around what we are going to focus on as a group around simplification. Church guidance is in parenthesis, and my thoughts are after:
Thoughts — (Meditate on Matthew 6:25–34) Remove sports talk in the morning on my way into work and meditate on this verse. Simplifies my decision on when to focus on this.
Emotions — (Give away any clothes that you haven’t worn in a year)Do this, and replace anything that I find that I am missing with nice, simple clothing.
Will — (Monitor behavior around material possessions) Plan out spending of bonuses prior to receiving with wife.
Behavior — (Tell a friend something you can simplify in life and do it) Set morning routine the night before — make smoothie and coffee, and set out clothes so that morning can be clear, structured and set up for a good day.
Social Interaction — (Simplify speech. Avoid bringing attention your way or flattery to others) Be transparent with progress at work and ask for help. Give honest feedback on where I need help and where I think others can do better. Fluff doesn’t make things better.
As I look at these five areas of simplification, the outcome of these goals is not to make my life terrible — which I think is where our mind goes whenever we think about taking things away — in every one of these cases, this simplification will make things better:
Thoughts — I’m a firm believer that if my mornings are clear, focused, and clutter free that the rest of my day will be better. Conversations will be more fulfilling, and if I feel productive during the day, work will not creep into the evening — making me a better husband and father.
Emotions — Giving away everything that I hadn’t worn in a year was a thought that initially brought about a lot of resistance in my head — “what about my college baseball gear!” But, if I give this all away, we will have more room in our house, I can replace my 10 year old suits with modern ones that I will actually wear, and we won’t have to fold mounds of laundry, which will save tons of time. I am now most excited about this one!
Will — We will most likely plan something along the lines of if we hit X in this quarter, will use it for A, B, and C. This will keep us honest with giving, and also make it more likely that we spend on long term important items instead of things that we may want at the time.
Behavior — I often don’t make my smoothie or coffee, or go for a run in the morning because I don’t want to wake the kids with the blender/grinder or feel that I don’t have time because I have to find clean cold gear for running. But, like my Thoughts, this routine helps my health and makes me better person.
Social Interaction — I really like the idea of simplifying speech. It isn’t something that we focus on in society, but “cutting the crap” allows us to get to the point, be more productive, and get to topics of importance. I am going to limit this to work at first, because I don’t think it would go over well with my wife! But, even at home, trying to make it a point to talk about things that are really on our hearts in a worthy endeavor.
My point in all of this is that in a culture of “stuff”, simplicity can get a negative connotation.
I am not going through these bullets and making it a point to meditate on scripture 100% of my day, move to Africa, give away 100% of my income, only eat plants, and be weak with my speech. Simplification does not have to be suffering.
I was listening to “Lead Me To The Cross” by Hillsong United this morning and was struck the request of the Lord to “quiet my soul”. What a beautiful thought — quiet our souls and simplify our lives. It is counterintuitive, but the best things in life often are.
",1152,Minimalism,Jake Klinvex,https://medium.com/s/story/simplicity-not-suffering-c5c1aa510694,"We’re focusing on Matthew 6: 25–34 and our key areas of focus include our Thoughts, Emotions, Will, Behavior, and Social Interactions.
I guess we were on a clothing discussion kick this week in our Bible studies because in our small group we also brought up the fact that if wear “loud” clothes, people notice if you wear the same thing twice — however, if you were to wear a plain blue, nice-fitting, button down every day to work, most probably wouldn’t notice if you wore the same thing for a year straight.
Thoughts — (Meditate on Matthew 6:25–34) Remove sports talk in the morning on my way into work and meditate on this verse.
Emotions — (Give away any clothes that you haven’t worn in a year)Do this, and replace anything that I find that I am missing with nice, simple clothing.
Behavior — (Tell a friend something you can simplify in life and do it) Set morning routine the night before — make smoothie and coffee, and set out clothes so that morning can be clear, structured and set up for a good day.
Thoughts — I’m a firm believer that if my mornings are clear, focused, and clutter free that the rest of my day will be better.
Conversations will be more fulfilling, and if I feel productive during the day, work will not creep into the evening — making me a better husband and father.
This will keep us honest with giving, and also make it more likely that we spend on long term important items instead of things that we may want at the time.
But, like my Thoughts, this routine helps my health and makes me better person.
Social Interaction — I really like the idea of simplifying speech."
‘Artificial Intelligence’ Because ‘REAL Stupidity’,Written By Hands,"‘Artificial Intelligence’ Because ‘REAL Stupidity’
Written By Hands

So — I’m going to give you all my data. Because that way you can sell me a whole lot more stuff and, while you are at it and because you will then know me that well, you will charge me a premium because you will be able to figure out that I can afford it. Right? Nope.
I’ve been very interested in new technology for years — my first proper job was helping friends launch the first microcomputer in the UKand ever since then I’ve been a very early adopter of just about every gadget available. Sad I know.
Thing is I’m an analogue person and I don’t actually know anything about the technology. Not a thing. I guess that puts me in the majority on something after all. Not sure I like that idea.
“I much prefer to draw and create — I want to make stuff using my own mind and hands far rather than have to figure out how why it works or code or wire stuff up — if that even a thing anymore. I’m more interested in the outcome — the result of a technology than the technology itself…”
A little surprising then that I’m cruelly addicted to the wave of AI (Artificial Intelligence)and ML (Machine Learning)stuff. Algorithms, statistical analysis and natural language processing — yum. I’m particular obsessed by the advent of pretty cool applications — many of them actually happening in our world right now.
You can add a bunch of other stuff to this addiction and they hide behind vogue terms like 3D printing and Robotics and what I’m saying is bring it on. I know there will be many a dissenting voice* about that being even a remotely good idea.
* There’s many folk who fear what it means and they have good reason because humans probably can’t be trusted to do the right thing with the incredible power now at our disposal. But hear me out.
YOU TAKE THE HIGHLINE AND I’LL TAKE THE LOWLINE
Given the incredible ability for machines to learn, mimic and then supersede our capacity for calculation we can solve problems that seemed improbable only a few years ago. Connect all these things together with robots and give them challenges that humans no longer want to do or were incapable of doing in the first place and you can see the attraction of this to large and small corporations.
The implications of this are almost guaranteed to be dire for some — perhaps for most of us. People increasingly out of work while we continue to fill up the world with scarily effective machines making a lot of stuff that we can live without and that might one day start killing us — its own makers. How dare they! A doomsday scenario.
“This genie is already out of Pandora’s Box. And it’s the human race that’s gonna need the bottle. What are we going to do with this new power? Will any good come from it?…”
It Depends
There’s a scenario that suggests the corporations and possibly governments will use these techniques to further enslave and control us. Extracting more money from us for things that are more convenient for them than for us*. Yes we are that gullible.
* Just so you know that’s already happening and with data that you are freely giving them to do it with.

Another scenario is that we say no to that and start owning our own data and choose how much of that we let the machines know about — this means staying in control of the choices we make and the things we get served. I can’t be the only one with an AdBlocker can I? And while it’s already sort of possible to control all our own data, we still need a lot of convincing that it’s a good thing to do.
“We’re moving away from a traditional consumer society where the value resided in ownership of the object itself to one where the value resides in the usage of the product and the impact that we can achieve from that usage.” — John Hagel
Quite right and when the balance tips away from that favouring us as an individual surely we will vote with our feet and switch off the tasty titbits that help the sneaky corporations sniff us out and abuse their privileges? Jury out on that one still. Real stupidity anyone?
Vast swathes of society are quite willing to assume that in the greater scheme of things mostly it’s gonna be OK. Hmm, while i kind of agree with that and have nothing to hide I’m in favour of sending a signal — making sure they can’t simply take our stupidity for granted. It can’t hurt.
John Hagel again —
“In this changing environment, vendors of products and services will need to become more adept at accelerating learning of consumers, both in the pre-purchase and post-purchase phases. This will involve a profound shift in mindset from doing transactions to building rich and sustaining relationships that will enable the vendor to become more and more helpful to the consumer.”
Well we certainly hope so right? And when he says learn that means WE HAVE TO LEARN.

H O W E V E R
I’m far less interested in keeping the capitalist commercial world alive with abundance and far more interested in solving some of the more important challenges of our world. It’s a passion of ours to try and help address some of the most complex.
Some examples of how AI can help — The needless poaching of Rhinos, tackling the near critical implications of climate change, the gaping chasms in our society of poverty, helping countries to transform agriculture — ending the slavery of children and helping the millions who live their lives in prostitution or in chains making cheap clothes for fat westerners.
“Won’t it be wiser, not only to use these new tools for such things — but because of the horizontal ability of these ‘machines’ to join themselves up — but bring into play the conscious corporations who will have by now heard their consumers voices — us…”
By John Hagel’s excellent definition they will be learning how we think — and if there is the intelligence we hope exists within the leaders of these corporations we can automatically embrace them into helping the whole process — at scale.
AYAHUASCA BY BJØRN LIE
Summary
These technologies allow us to make major inroads, create many new industries and precious jobs. This new power could allow us to help real people and restore humanity to our crazy societies.
With the right energy and the best thinking by smart people who know how to use these tools we can begin to dream that we can put an end to many of the inequities we see around us and help clean up the mess we’ve made of our world.
I’m not quite naive enough to think any of this happens overnight or is going to be easy — even though there’s a lot already happening — but I am optimistic enough to want to keep banging on about it. It’s the conversation we need.
It may be called artificial intelligence but I believe it stands a better chance than us of halting the very real stupidity.

",1220,Artificial Intelligence,John Caswell,https://medium.com/s/story/artificial-intelligence-because-real-stupidity-ddf80af6a71c,"Thing is I’m an analogue person and I don’t actually know anything about the technology.
A little surprising then that I’m cruelly addicted to the wave of AI (Artificial Intelligence)and ML (Machine Learning)stuff.
* There’s many folk who fear what it means and they have good reason because humans probably can’t be trusted to do the right thing with the incredible power now at our disposal.
Connect all these things together with robots and give them challenges that humans no longer want to do or were incapable of doing in the first place and you can see the attraction of this to large and small corporations.
Another scenario is that we say no to that and start owning our own data and choose how much of that we let the machines know about — this means staying in control of the choices we make and the things we get served.
And while it’s already sort of possible to control all our own data, we still need a lot of convincing that it’s a good thing to do.
Hmm, while i kind of agree with that and have nothing to hide I’m in favour of sending a signal — making sure they can’t simply take our stupidity for granted.
“Won’t it be wiser, not only to use these new tools for such things — but because of the horizontal ability of these ‘machines’ to join themselves up — but bring into play the conscious corporations who will have by now heard their consumers voices — us…”
By John Hagel’s excellent definition they will be learning how we think — and if there is the intelligence we hope exists within the leaders of these corporations we can automatically embrace them into helping the whole process — at scale.
This new power could allow us to help real people and restore humanity to our crazy societies.
With the right energy and the best thinking by smart people who know how to use these tools we can begin to dream that we can put an end to many of the inequities we see around us and help clean up the mess we’ve made of our world.
I’m not quite naive enough to think any of this happens overnight or is going to be easy — even though there’s a lot already happening — but I am optimistic enough to want to keep banging on about it."
Your Data is Being Manipulated,The following are remarks by Data & Society Founder and President danah boyd for her keynote at the 2017 Strata Data Conference in New York…,"Your Data is Being Manipulated
Excerpt from “The Anatomy of a Large-Scale Hypertextual Web Search Engine,” Sergey Brin and Larry Page (April 1998)
The following are remarks by Data & Society Founder and President danah boyd for her keynote at the 2017 Strata Data Conference in New York City. Full video of keynote is available at the bottom of page. — Ed.
In 1998, two graduate students at Stanford decided to try to “fix” the problems with major search engines. Sergey Brin and Larry Page wrote a paper describing how their PageRank algorithm could eliminate the plethora of “junk results.” Their idea, which we all now know as the foundation of Google, was critical. But it didn’t stop people from trying to mess with their system. In fact, the rise of Google only increased the sophistication of those invested in search engine optimization.
“google bombing” — diverting search engine rankings to subversive commentary about public figure
Fast forward to 2003, when the sitting Pennsylvania senator Rick Santorum publicly compared homosexuality to bestiality and pedophilia. Needless to say, the LGBT community was outraged. Journalist Dan Savage called on his readers to find a way to “memorialize the scandal.” One of his fans created a website to associate Santorum’s name with anal sex. To the senator’s horror, countless members of the public jumped in to link to that website in an effort to influence search engines. This form of crowdsourced SEO is commonly referred to as “Google bombing,” and it’s a form of media manipulation intended to mess with data and the information landscape.
Media Manipulation and Disinformation Online (cover), March 2017. Illustration by Jim Cooke
Media manipulation is not new. As many adversarial actors know, the boundaries between propaganda and social media marketing are often fuzzy. Furthermore, any company that uses public signals to inform aspects of its product — from Likes to Comments to Reviews — knows full well that any system you create will be gamed for fun, profit, politics, ideology, and power. Even Congress is now grappling with that reality. But I’m not here to tell you what has always been happening or even what is currently happening — I’m here to help you understand what’s about to happen.
At this moment, AI is at the center of every business conversation. Companies, governments, and researchers are obsessed with data. Not surprisingly, so are adversarial actors. We are currently seeing an evolution in how data is being manipulated. If we believe that data can and should be used to inform people and fuel technology, we need to start building the infrastructure necessary to limit the corruption and abuse of that data — and grapple with how biased and problematic data might work its way into technology and, through that, into the foundations of our society.
In short, I think we need to reconsider what security looks like in a data-driven world.
Shutterstock by goir
Part 1: Gaming the System
Like search engines, social media introduced a whole new target for manipulation. This attracted all sorts of people, from social media marketers to state actors. Messing with Twitter’s trending topics or Facebook’s news feed became a hobby for many. For $5, anyone could easily buy followers, likes, and comments on almost every major site. The economic and political incentives are obvious, but alongside these powerful actors, there are also a whole host of people with less-than-obvious intentions coordinating attacks on these systems.
Piechart example of Rick-Rolling
For example, when a distributed network of people decided to help propel Rick Astley to the top of the charts 20 years after his song “Never Gonna Give You Up” first came out, they weren’t trying to help him make a profit (although they did). Like other memes created through networks on sites like 4chan, rickrolling was for kicks. But through this practice, lots of people learned how to make content “go viral” or otherwise mess with systems. In other words, they learned to hack the attention economy. And, in doing so, they’ve developed strategic practices of manipulation that can and do have serious consequences.
A story like “#Pizzagate” doesn’t happen accidentally — it was produced by a wide network of folks looking to toy with the information ecosystem. They created a cross-platform network of fake accounts known as “sock puppets” which they use to subtly influence journalists and other powerful actors to pay attention to strategically produced questions, blog posts, and YouTube videos. The goal with a story like that isn’t to convince journalists that it’s true, but to get them to foolishly use their amplification channels to negate it. This produces a “Boomerang effect,” whereby those who don’t trust the media believe that there must be merit to the conspiracy, prompting some to “self-investigate.”
Hydrargyrum CC BY-SA 2.0
Then there’s the universe of content designed to “open the Overton window” — or increase the range of topics that are acceptable to discuss in public. Journalists are tricked into spreading problematic frames. Moreover, recommendation engines can be used to encourage those who are open to problematic frames to go deeper. Researcher Joan Donovan studies white supremacy; after work, she can’t open Amazon, Netflix, or YouTube without being recommended to consume neo-Nazi music, videos, and branded objects. Radical trolls also know how to leverage this infrastructure to cause trouble. Without tripping any of Twitter’s protective mechanisms, the well-known troll weev managed to use the company’s ad infrastructure to amplify white supremacist ideas to those focused on social justice, causing outrage and anger.
By and large, these games have been fairly manual attacks of algorithmic systems, but as we all know, that’s been changing. And it’s about to change again.
Part 2: Vulnerable Training Sets
Training a machine learning system requires data. Lots of it. While there are some standard corpuses, computer science researchers, startups, and big companies are increasingly hungry for new — and different — data.
Cognitive Psychology for Deep Neural Networks: A Shape Bias Case Study, June 29, 2017
The first problem is that all data is biased, most notably and recognizably by reflecting the biases of humans and of society in general. Take, for example, the popular ImageNet dataset. Because humans categorize by shape faster than they categorize by color, you end up with some weird artifacts in that data.
(a) and (c) demonstrate ads for two indvidual’s names, (b) and (d) demonstrate that the advertising was suggesting criminal histories based on name type, not actual records
Things get even messier when you’re dealing with social prejudices. When Latanya Sweeney searched for her name on Google, she was surprised to be given ads inviting her to find out if she had a criminal record. As a curious computer scientist, she decided to run a range of common black and white names through the system to see which ads popped up. Unsurprisingly, only black names produced ads for criminal justice products. This isn’t because Google knowingly treated the names differently, but because searchers were more likely to click on criminal justice ads when searching for black names. Google learned American racism and amplified it back at all of its users.
Addressing implicit and explicit cultural biases in data is going to be a huge challenge for everyone who is trying to build a system dependent on data classified by or about humans.
But there’s also a new challenge emerging. The same decentralized networks of people — and state actors — who have been messing with social media and search engines are increasingly eyeing the data that various companies use to train and improve their systems.
Consider, for example, the role of reddit and Twitter data as training data. Computer scientists have long pulled from the very generous APIs of these companies to train all sorts of models, trying to understand natural language, develop metadata around links, and track social patterns. They’ve trained models to detect depression, rank news, and engage in conversation. Ignoring the fact that this data is not representative in the first place, most engineers who use these APIs believe that it’s possible to clean the data and remove all problematic content. I can promise you it’s not.
No amount of excluding certain subreddits, removing of categories of tweets, or ignoring content with problematic words will prepare you for those who are hellbent on messing with you.
I’m watching countless actors experimenting with ways to mess with public data with an eye on major companies’ systems. They are trying to fly below the radar. If you don’t have a structure in place for strategically grappling with how those with an agenda might try to route around your best laid plans, you’re vulnerable. This isn’t about accidental or natural content. It’s not even about culturally biased data. This is about strategically gamified content injected into systems by people who are trying to guess what you’ll do.
If you want to grasp what that means, consider the experiment Nicolas Papernot and his colleagues published last year. In order to understand the vulnerabilities of computer vision algorithms, they decided to alter images of stop signs so that they still resembled a stop sign to a human viewer even as the underlying neural network interpreted them as a yield sign. Think about what this means for autonomous vehicles. Will this technology be widely adopted if the classifier can be manipulated so easily?
Practical Black-Box Attacks against Machine, March 19, 2017. The images in the top row are altered to disrupt the neural network leading to the misinterpretation on the bottom row. The alterations are not visible to the human eye.
Right now, most successful data-injection attacks on machine learning models are happening in the world of research, but more and more, we are seeing people try to mess with mainstream systems. Just because they haven’t been particularly successful yet doesn’t mean that they aren’t learning and evolving their attempts.
Part 3: Building Technical Antibodies
Many companies spent decades not taking security vulnerabilities seriously, until breach after breach hit the news. Do we need to go through the same pain before we start building the tools to address this new vulnerability?
If you are building data-driven systems, you need to start thinking about how that data can be corrupted, by whom, and for what purpose.
In the tech industry, we have lost the culture of Test. Part of the blame rests on the shoulders of social media. Fifteen years ago, we got the bright idea to shift to a culture of the “perpetual beta.” We invited the public to be our quality assurance engineers. But internal QA wasn’t simply about finding bugs. It was about integrating adversarial thinking into the design and development process. And asking the public to find bugs in our systems doesn’t work well when some of those same people are trying to mess with our systems. Furthermore, there is currently no incentive — or path — for anyone to privately tell us where things go wrong. Only when journalists shame us by finding ways to trick our systems into advertising to neo-Nazis do we pay attention. Yet, far more maliciously intended actors are starting to play the long game in messing with our data. Why aren’t we trying to get ahead of this?
On the bright side, there’s an emergent world of researchers building adversarial thinking into the advanced development of machine learning systems.
Consider, for example, the research into generative adversarial networks (or GANs). For those unfamiliar with this line of work, the idea is that you have two unsupervised ML algorithms — one is trying to generate content for the other to evaluate. The first is trying to trick the second into accepting “wrong” information. This work is all about trying to find the boundaries of your model and the latent space of your data. We need to see a lot more R&D work like this — this is the research end of a culture of Test, with true adversarial thinking baked directly into the process of building models.
White Hat Hackers — those who hack for “the right reasons.” For instance, testing the security or vulnerabilities of a system (Image: CC Magicon, HU)
But these research efforts are not enough. We need to actively and intentionally build a culture of adversarial testing, auditing, and learning into our development practice. We need to build analytic approaches to assess the biases of any dataset we use. And we need to build tools to monitor how our systems evolve with as much effort as we build our models in the first place. My colleague Matt Goerzen argues that we also need to strategically invite white hat trolls to mess with our systems and help us understand our vulnerabilities.
The tech industry is no longer the passion play of a bunch of geeks trying to do cool shit in the world. It’s now the foundation of our democracy, economy, and information landscape.
We no longer have the luxury of only thinking about the world we want to build. We must also strategically think about how others want to manipulate our systems to do harm and cause chaos.
danah boyd delivers a keynote address to the 2017 Strata Data Conference (NYC)
",2037,Artificial Intelligence,danah boyd,https://points.datasociety.net/your-data-is-being-manipulated-a7e31a83577b,"This form of crowdsourced SEO is commonly referred to as “Google bombing,” and it’s a form of media manipulation intended to mess with data and the information landscape.
Furthermore, any company that uses public signals to inform aspects of its product — from Likes to Comments to Reviews — knows full well that any system you create will be gamed for fun, profit, politics, ideology, and power.
If we believe that data can and should be used to inform people and fuel technology, we need to start building the infrastructure necessary to limit the corruption and abuse of that data — and grapple with how biased and problematic data might work its way into technology and, through that, into the foundations of our society.
In short, I think we need to reconsider what security looks like in a data-driven world.
Like search engines, social media introduced a whole new target for manipulation.
For example, when a distributed network of people decided to help propel Rick Astley to the top of the charts 20 years after his song “Never Gonna Give You Up” first came out, they weren’t trying to help him make a profit (although they did).
But through this practice, lots of people learned how to make content “go viral” or otherwise mess with systems.
They created a cross-platform network of fake accounts known as “sock puppets” which they use to subtly influence journalists and other powerful actors to pay attention to strategically produced questions, blog posts, and YouTube videos.
Addressing implicit and explicit cultural biases in data is going to be a huge challenge for everyone who is trying to build a system dependent on data classified by or about humans.
The same decentralized networks of people — and state actors — who have been messing with social media and search engines are increasingly eyeing the data that various companies use to train and improve their systems.
Computer scientists have long pulled from the very generous APIs of these companies to train all sorts of models, trying to understand natural language, develop metadata around links, and track social patterns.
Ignoring the fact that this data is not representative in the first place, most engineers who use these APIs believe that it’s possible to clean the data and remove all problematic content.
I’m watching countless actors experimenting with ways to mess with public data with an eye on major companies’ systems.
This is about strategically gamified content injected into systems by people who are trying to guess what you’ll do.
Right now, most successful data-injection attacks on machine learning models are happening in the world of research, but more and more, we are seeing people try to mess with mainstream systems.
Do we need to go through the same pain before we start building the tools to address this new vulnerability?
If you are building data-driven systems, you need to start thinking about how that data can be corrupted, by whom, and for what purpose.
And asking the public to find bugs in our systems doesn’t work well when some of those same people are trying to mess with our systems.
On the bright side, there’s an emergent world of researchers building adversarial thinking into the advanced development of machine learning systems.
Consider, for example, the research into generative adversarial networks (or GANs).
We need to see a lot more R&D work like this — this is the research end of a culture of Test, with true adversarial thinking baked directly into the process of building models.
We need to actively and intentionally build a culture of adversarial testing, auditing, and learning into our development practice.
My colleague Matt Goerzen argues that we also need to strategically invite white hat trolls to mess with our systems and help us understand our vulnerabilities.
We must also strategically think about how others want to manipulate our systems to do harm and cause chaos."
Machine Learning in iOS— for the noob,After working on a couple of occasions with handwritten recognition I’m in total awe of this piece of technology. Just by sending an image…,"Machine Learning in iOS— for the noob

After working on a couple of occasions with handwritten recognition I’m in total awe of this piece of technology. Just by sending an image to a REST endpoint, waiting for the magic to happen and then having a bunch of JSON data with your recognised text in to as the response — just wow.
But I already “awed” a lot about this in a blog post a couple of months ago. For everybody who is interested in it and doesn’t know how to get started — look right here.
https://medium.com/@codeprincess/the-doodling-workshop-3-70d8e360956a
So where is now the handwritten recognition link to machine learning — for the noobs. Easy! All the recognition stuff just works online. So if you are using this feature with a bad network connection in place you will experience at least long waiting times and in the worst case just timeouts on your requests. Moreover the possibility of having no network connection at all is also likely in certain scenarios. What do you do then? Have all of the recognition offline in place — Bingo!
TL;DR;
Before we start a quick side note to the impatient reader.
The whole project can be found on GitHub, including all the machine learning and model building stuff. So if you are more into looking into code instead of reading explanations, visit this repo, and see you soon :)
https://github.com/codePrincess/doodlingRecognition
THE IDEA
To be able to offer offline handwritten text recognition you might have already assumed that there needs to be a bunch of logic in place — packed into your application. And this is exaclty what this article is dealing with.
We will build our own handwritten text recognition model, convert it to a CoreML model and use it in a native iOS app to detect handwritten numbers — OFFLINE!
For two reasons we will just focus on handwritten number recognition:
Because the current Computer Vision OCR API can’t do it properly. For proper recognition there must always be a context in place (like grammar, a certain format, …)
To make things easier for this proof of concept — keep things small and easy (and believe me, it will already be complicated enough) :D

Great objective I would say, so let’s start!
CREATE AND TRAIN A MODEL
The data
Machine Learning is first of all about data. If you have valid data which can be categorised nicely you are already on the winning path. If you haven’t them — you are doomed. In this case you have to take a couple of steps back and get a good data set to start with.
In our case we have a nice set of handwritten numbers from the MNIST database (http://yann.lecun.com/exdb/mnist/) which contains incredible 60.000 images of numbers with an original size of 20x20 pixels. So with this set we will work then and train our machine learning model. But we don’t have any model yet?!
The model
The model shall not be our problem. There are such a lot of different algorithms out there which can help us building a valid model out of our data.
We’ll pick the …. SVM! A Support Vector Machine! Why this**? Because a colleague told me to. And because the example we’ll be using from the SciKit-Learn library told us to. The what? So let’s stop shortly to put things into the correct order.
Let’s visit http://scikit-learn.org/. This page offers a lot of Python based examples on how to solve problems with machine learning. Which is great!
So I searched for “hand-written digits” and found an already finished Python script on how to use an SVM (remember, the Support Vector Machine) on the number problem. With a machine like this, or an algorithm like this, we can classify things. In detail the SVM is a binary classifier, which can detect if a certain thing is part of group A or is something else. It doesn’t just necessarily say “This is A and not B but maybe C”, it tells you “This is A - and not B, C and D”.
http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py
As we have a look at the sample Python code, and depending how awesome you are with it (I wasn’t and aren’t — remember — noob!), the whole process is not that complicated though. And don’t be afraid if you don’t understand the very last detail of it. We will just run it and see it the result fits our need. In case it won’t we will come back and tweak our model. But first — let’s go down the easy and straight path to machine learning.

I want to point out a couple of things to get some orientation of that is happening in this script.
digits = datasets.load_digits()
First we load the data set from the MNIST set and save it onto the variable digits.
classifier = svm.SVC(gamma=0.001)
Here we are creating a SVC — so a Support Vector Classifier — which under the hood uses a Support Vector Machine for it’s work. It’s basically the skeleton of our model, which we will now train for our needs with the big set of digits.
classifier.fit(data[:n_samples // 2], digits.target[:n_samples // 2])
And after all we have to save or better to say dump the model somehow, which is done with this piece of code. The format is called Pickle and it’s a binary representation of the model iteself.
with open(‘mymodel.pkl’, ‘wb’) as file:
pickle.dump(classifier, file, protocol=pickle.HIGHEST_PROTOCOL)
Done! We now have a Pickle formatted model which should be able to detect handwritten numbers from images. A little step is missing though.
** If you are not a data scientist and know exactly what algorithm will fit best, have a look at this cheat sheet to get a first idea. The rest is then a bit trial and error and experiment what fits best on your set of data.

The conversion
We need to convert it for the usage in our iOS application. Why? Because Xcode just accepts CoreML models and no Pickle formatted ones. But no worries, the conversion is quite easy. We again use Python for this task and it’s done with those few lines of code.

We are done with the machine learning part. Easy right! 
All the noobs — including me — are cheering right now :)
Weeee’ve done machiiiiine learniiiiing!
THE USAGE
The integration of our CoreML model into Xcode is quite easy. Just drag and drop it as a resource, don’t forget to copy it, and you are good to go.
But now a bit of work is lying between our model and the predictions we want to do with it. Why, you are asking? I can cut images of my own handwriting and just ask the model what’s on them. Basically true. But not with our model. Because our model doesn’t take pictures as an input, it just takes an 8x8 multiarray of Doubles. You can find out what the model accepts as an input and offeres as an output by just selecting it in Xcode and having a look at it’s properties.
Xcode description of a CoreML model
What we have to do
To convert our handwritten numbers to 8x8 multiarrays we need a bit of math, but not that much. Here is what we essentially need to do:
Get the handwritten number
Save it on an UIImage
Then cut the UIImage 8x8
Look at every tiny piece
Get it’s color info and the alpha for the tiny piece
Multiply the resulting value by 16
Save this value to the corresponding index in our multiarray
Give the multiarray to the model for prediction
And print out the result of if

The details
So let’s look into three little details which might be interesting.
First, the retrieval of the currently written number. I’m checking each touch which is generated (see func drawStroke) during writing with the pencil and save the lowest x/y coordinate (top left) as well as the highest x/y coordinates (lower right) to know exactly what extent the latest writing has. After a “oh, the user stopped writing”-timeout (see the trackTimer in func setup) I draw the rectangle (see func drawDoodlingRect) and save exactly this area of the canvas into an UIImage (see func fetchOCRText) . And this image is the basis for the data we need to ask our model.

Second, the translation of the image into an 8x8 multiarray. The code consists exactly of the already listed steps above. All we do here is cut a piece out of our image (the number we’ve just written and cut out), have a look at it’s color, get the alpha and save it with a little modification to the multiarray. Doing this 8x8 or better to say 64 times we will have traced the whole image and can now ask the model for it’s guess on it.

Third, the model prediction. It’s just one line of code. We ask the model for an prediction and get a number back. 8x8 multiarray in, number out. And this number is, in the best case, exactly what we’ve written down seconds ago :)

A bit into testing the screen of the Scribble app then will look like this :) Getting back our own prediction from the model and printing it right on the canvas under our written number.

We’ve done it — We can recognise (most) handwritten numbers — offline!
IN THE END
… This is not the end!
Obviously. There are a couple of problems with the model.
First of all the training images are all taken from U.S. people. You guys write numbers in mini different way than we Europeans do, but this already has a significant effect on the prediction quality of the model. So the model would need additional training with handwritten numbers from e.g. Germans.
Another means of optimization would be to use another algorithm as the SVM. Maybe a neural network would fit better? This would be the next step to this first prototype.
And the last thing, but this is a restriction from CoreML itself: You can’t give feedback to the model. It’s there. It’s trained. It’s read-only or better to say prediction-only. To improve the model in this scenario we would have to save the feedback somewhere, then do a separate new train cycle and updated the model in the app afterwards — quite a struggle.
Further keep in mind that regarding with what you train the model it can get quite big. And as it is embedded in your app it will gain some data weight also.
What now?
It’s on you! Play around, build something awesome. Dive into Python and don’t be afraid. It’s not that complicated to code your scripts with it. And getting into the ML stuff seems to be very hard also. But you already managed to get this great in what you are doing. This is maybe just your next challenge you will master.

Take your time :) And take this for a good start.
https://github.com/codePrincess/doodlingRecognition
Happy coding!
PS: And I’d love to hear how you are doing :)
",1861,Machine Learning,manu rink,https://medium.com/s/story/machine-learning-in-ios-for-the-noob-6c2cdd04b00b,"So where is now the handwritten recognition link to machine learning — for the noobs.
The whole project can be found on GitHub, including all the machine learning and model building stuff.
To be able to offer offline handwritten text recognition you might have already assumed that there needs to be a bunch of logic in place — packed into your application.
We will build our own handwritten text recognition model, convert it to a CoreML model and use it in a native iOS app to detect handwritten numbers — OFFLINE!
Machine Learning is first of all about data.
In this case you have to take a couple of steps back and get a good data set to start with.
So with this set we will work then and train our machine learning model.
There are such a lot of different algorithms out there which can help us building a valid model out of our data.
This page offers a lot of Python based examples on how to solve problems with machine learning.
So I searched for “hand-written digits” and found an already finished Python script on how to use an SVM (remember, the Support Vector Machine) on the number problem.
As we have a look at the sample Python code, and depending how awesome you are with it (I wasn’t and aren’t — remember — noob!), the whole process is not that complicated though.
But first — let’s go down the easy and straight path to machine learning.
Here we are creating a SVC — so a Support Vector Classifier — which under the hood uses a Support Vector Machine for it’s work.
It’s basically the skeleton of our model, which we will now train for our needs with the big set of digits.
And after all we have to save or better to say dump the model somehow, which is done with this piece of code.
The format is called Pickle and it’s a binary representation of the model iteself.
We now have a Pickle formatted model which should be able to detect handwritten numbers from images.
** If you are not a data scientist and know exactly what algorithm will fit best, have a look at this cheat sheet to get a first idea.
We again use Python for this task and it’s done with those few lines of code.
The integration of our CoreML model into Xcode is quite easy.
You can find out what the model accepts as an input and offeres as an output by just selecting it in Xcode and having a look at it’s properties.
To convert our handwritten numbers to 8x8 multiarrays we need a bit of math, but not that much.
Give the multiarray to the model for prediction
And this image is the basis for the data we need to ask our model.
All we do here is cut a piece out of our image (the number we’ve just written and cut out), have a look at it’s color, get the alpha and save it with a little modification to the multiarray.
Doing this 8x8 or better to say 64 times we will have traced the whole image and can now ask the model for it’s guess on it.
We ask the model for an prediction and get a number back.
A bit into testing the screen of the Scribble app then will look like this :) Getting back our own prediction from the model and printing it right on the canvas under our written number.
So the model would need additional training with handwritten numbers from e.g. Germans."
Biased Algorithms and the False Sense of Democratization,"Artificial intelligence is biased. That’s a huge problem!
– Carmen Aguilar y Wedge, Interaction/Experience Engineer, Hyphen-Labs","Biased Algorithms and the False Sense of Democratization
Artificial intelligence is biased. That’s a huge problem!
– Carmen Aguilar y Wedge, Interaction/Experience Engineer, Hyphen-Labs
In 2014, Sundance Institute’s New Frontier program collaborated with MIT Media Lab’s Social Computing Group to embed seven storytellers in the research group over a year to explore data as a storytelling medium. One of the Fellows was journalist Lisa Biagotti. She spent her month-long residency asking computer scientists, “What’s an algorithm?”
This seemingly simple question helped her extract answers that gave her a fairly profound epiphany. The data produced by algorithms, which she relied on as objective truth for her journalistic reporting, was actually subjective. She realized that in terms of defining truth, algorithms are just as problematic as framing a photograph to include and exclude key subjects. What you leave out of the frame of the photo is just as important to a story’s truth as what you leave in. How you frame the question the algorithm answers can bias the resulting data. MIT graduate student Joy Buolamwini articulates the issue as the “Coded Gaze” and delivers an excellent talk about the problem via a TED Talk.
MIT graduate student Joy Buolamwini
An often-used example of biased algorithms was described in the New York Times in 2015 by journalist Claire Cain Miller: “Google’s online advertising system, for instance, showed an ad for high-income jobs to men much more often than it showed the ad to women, a new study by Carnegie Mellon University researchers found. Research from Harvard University found that ads for arrest records were significantly more likely to show up on searches for distinctively black names or a historically black fraternity. The Federal Trade Commission said advertisers are able to target people who live in low-income neighborhoods with high-interest loans. Research from the University of Washington found that a Google Images search for ‘C.E.O.’ produced 11 percent women, even though 27 percent of United States chief executives are women. (On a recent search, the first picture of a woman to appear, on the second page, was the C.E.O. Barbie doll.) Image search results determined 7 percent of viewers’ subsequent opinions about how many men or women worked in a field, it found.”
This was a top area that interviewees identified as problematic due to a failure by the tech and media industries to bring diverse groups of people, coming from varied fields of knowledge, into the discourse around these designs. Could breaking the silos of knowledge and discipline help to mitigate these kinds of egregious errors?
Another highly concerning example illuminates how algorithms are disproportionately affecting people of color in the US criminal justice system. As Harry Armstrong and Jared Robert Keller report in The Guardian, “In the U.S., criminal ‘risk assessments’ based on predictive analytics have already been shown to be biased against black people because the data used to build the system was inherently biased.”
Other major smart (or not so smart) algorithm failures include facial recognition software that failed to understand the diversity of humans, such as when Google’s code labeled black faces as “gorillas,” and assumed Asian faces had blinked during photo, when their eyes were, in fact, open.
To be fair, the cases of outright maliciously coded bias algorithms (or the robots they animate) is probably a small percentage. Most of the bias being discovered is a surprise to the coders. Part of the issue is a lack of different perspectives among the pool of people coding these algorithms, making use of such technologies vulnerable to blind spots. An example of this is Google failing to code the automatic orientation of YouTube videos on smartphone for left-handed people, because they happen to have only right-handed people on the team. Part of the issue is that technologists are tasked with teaching very complex concepts to machines that operate on neural networks, computer infrastructure modeled on the human brain and nervous system, that are impossible for the technologists to understand in human scale.
Forbes reports that “Brian Brackeen, CEO of facial recognition company Kairos, says that machines can make culturally inappropriate assumptions when not properly trained. ‘It’s scarily similar to how a child learns.’ ”
That may be the issue in a nutshell, especially when these increasingly intelligent algorithms are learning from the problematic behavior of humans. For example, Microsoft’s chatbot learned to be racist and sexist from Twitter within 24 hours. Are biased algorithms just a product of their environment?
Then there are more nuanced conversations to be had about reconciling different value systems and the gray area between protection and censorship. Christopher Trout reports in Engadget: “A 2006 report on internet filtering from NYU’s Brennan Center for Justice referred to early keyword filters and their AI successors as ‘powerful, often irrational, censorship tools.’ Filters force the complex and infinitely variable phenomenon known as human expression into deceptively simple categories, the report continued. ‘They reduce the value and meaning of expression to isolated words and phrases. An inevitable consequence is that they frustrate and restrict research into health, science, politics, the arts, and many other areas.’ The report found that popular filters inexplicably blocked sites belonging to Boing Boing, GLAAD, photographer Robert Mapplethorpe, and Super Bowl XXX, among others, and often reflected the political and social prejudices of their creators.”
Another concern about how algorithms operate is the filter bubbles or echo chambers they create (Blame Facebook for echo chambers. But blame yourself, too). We are creating billions of terabytes of data through our digital activity, so we need smart algorithms to help us sort through all the data and make it meaningful or useful. Designers write algorithms that create UX/IU features, such as autofill, or trending topics suggestions (“if you liked X, you’ll like Y”) from sponsored advertising and entertainment outlets. The problem with these features is they quickly skew the results or choices to the first things that were searched, leaving users ignorant of the broader spectrum of possibilities. They do not authentically represent what is popular, or of interest to people.
As Dave Gershgorn writes in QZ.com, “Algorithms are unseen forces that dictate much of our life on the internet: Google search results, Facebook, and even spam filtering in email. And these algorithms are getting smarter thanks to improving artificial intelligence. To do so, they need to learn from more and more information. More often than not, the data these machines learn from comes from humans. Google’s algorithms, for example, learn to recognize speech by listening to tons of Google voice searches performed by humans.”
Some observers have pointed out that the technology is not fundamentally the problem; it’s that the technology exponentially amplifies actual human behavior. We tend to flock together or seek those that are most like us. Although many have used the power of the internet to break out of their comfort zones, the internet also enables social silos.
“I am, in general, pretty bold and unapologetic in making big claims about what the future is going to bring,” says Adam Huttler of the Exponential Creativity Fund. “There are some areas, though, where I have become cautious about predicting where things are going to head. So, as an example, I’ve been a loud evangelist for a long time, 20 years now, for the promise of the internet for lowering various barriers to entry, [elevating] new voices. What we’re seeing right now is the very negative, unintended consequences of that. It never even occurred to me that we’d end up in these little, tiny, niche echo chambers where extreme voices are allowed to resonate indefinitely, whether they’re factually accurate or not.”
Luke DuBois of NYU’s Brooklyn Experimental Media Center provided an example of this dynamic. “Facebook is the worst at this, right? So there was this Wall Street Journal project (Blue Feed, Red Feed) where they were side-by-siding the news feeds of folks who would self identify as liberal and folks who would self identify as conservative…and it’s like, literally, zero overlap.”
Blue Feed, Red Feed. See Liberal Facebook and Conservative Facebook, Side by Side — Wall Street Journal
Psychologist Michal Kosinski created a system to analyze people using their Facebook activity. This information can be used in the other way to find people who fit certain descriptions, and target them with advertisements. This ability to reach people inside of their siloed echo chambers has had an impact on politics, as we’ve seen with the 2017 presidential election, in which both candidates and bots micro-targeted voters. Companies such as NationBuilder have made tools that allow campaigns to reach highly targeted audiences. While NationBuilder was created with the intention of boosting the success of grassroots campaigns, its low price point and accessibility drew in the Trump administration.
Streaming services such as Netflix and Amazon are also contributing to the narrowing of our worldview, because their algorithms designed to feature content that matches the previous content the user consumed. Additionally, media executives are adopting the decision-making model of: “talent with audience” and “subject on trend.” This means they will greenlight a production only if it has attached talent with heavy followers on social media and if the subject is trending on Google Trends. So, beyond the filtering algorithm (and a base of human micro-taggers), the content itself will be constructed by narrow parameters of what is already being consumed. For the media executive it’s a way to optimize revenue growth, but executives are setting the industry up for eventual backlash and the dumbing down of the audience’s worldview in the process.
Interviewees commented on these negative dynamics:
So, rather than being exposed to a real diversity of content through what could be real “democratic media,” companies and institutions are increasingly segmenting what we have access to. I think that’s a real danger to democracy, free expression, and self-determination, because all these mediums have the potential to allow a greater diversity of people to participate…demographic diversity, but also stylistic and aesthetic diversity. — Michael Premo, Multidisciplinary artist
The web was supposed to be a democratic place where everyone could make great things and be seen. The reality is that the web is increasingly controlled by corporations that help guide people to where the good content is. So, yes, you can have an interactive documentary that lives at its own URL, but you’re less likely for that to be seen if it doesn’t feed into something else, like the Facebook ecosystem. — Adnaan Wasey, former Executive Producer, POV Digital
With Netflix’s $6 billion being spent on TV content in 2017, we are seeing huge competition in the streaming television space — with much of it being driven by machine-driven algorithms that analyze and deliver your preferences. This is worrisome, given that it begs the question as to whether there should be humans curating these cultural decisions. The “big four” tech platforms (Google, Apple, Facebook, and Amazon) are all leading the charge because they have massive cloud computing capabilities, which gives them access to artificial intelligence, and, with it, the ability to engage in sophisticated data analytics, presumably to deliver what consumers want. However, at the end of the day, maybe we need to return to human intervention and curatorship of our shared cultural knowledge. — Denise Mann, Professor and head of the UCLA School of Theater, Film and Television’s Producers Program
Some are hopeful that we will learn to mitigate current issues such as biased algorithms. In fact, Jack Dorsey’s recent call for support from the Twitter community to help them design a healthier and less toxic environment was well received by those who have been calling for reform. Some think we might even be able to train AI to create “a reduction in discrimination, overall, since AI programs are inherently more easily audited than humans.” However, the current Trump administration is asserting AI disruption is 50 to 100 years away, which technology insiders such as Jeff Greene call “out of touch with reality.”
These issues have become an important part of the discourse around the future of code and its impact on our world. But as many interviewees expressed, the conversation is not as broad and urgent as it needs to be to resolve the issues. Interviewees were particularly concerned when they considered that learning algorithms will have access to data about almost every aspect of our daily life with the rise of Internet of Things and facial or DNA analysis tools, which can use your physical participation in the world to gather data to inform fundamental decisions about your financial services, loans, employment, and rental applications. We could get to Black Mirror’s dystopian view of our future quickly.
The Making a New Reality research project is authored by Kamal Sinclair with support from Ford Foundation JustFilms and supplemental support from the Sundance Institute. Learn more about the goals and methods of this research, who produced it, and the interviewees whose insights inform the analysis.
",2142,Artificial Intelligence,Making a New Reality,https://makinganewreality.org/biased-algorithms-and-the-false-sense-of-democratization-c2ab2bfa12f6,"In 2014, Sundance Institute’s New Frontier program collaborated with MIT Media Lab’s Social Computing Group to embed seven storytellers in the research group over a year to explore data as a storytelling medium.
The data produced by algorithms, which she relied on as objective truth for her journalistic reporting, was actually subjective.
How you frame the question the algorithm answers can bias the resulting data.
An often-used example of biased algorithms was described in the New York Times in 2015 by journalist Claire Cain Miller: “Google’s online advertising system, for instance, showed an ad for high-income jobs to men much more often than it showed the ad to women, a new study by Carnegie Mellon University researchers found.
This was a top area that interviewees identified as problematic due to a failure by the tech and media industries to bring diverse groups of people, coming from varied fields of knowledge, into the discourse around these designs.
Another highly concerning example illuminates how algorithms are disproportionately affecting people of color in the US criminal justice system.
Other major smart (or not so smart) algorithm failures include facial recognition software that failed to understand the diversity of humans, such as when Google’s code labeled black faces as “gorillas,” and assumed Asian faces had blinked during photo, when their eyes were, in fact, open.
Part of the issue is a lack of different perspectives among the pool of people coding these algorithms, making use of such technologies vulnerable to blind spots.
That may be the issue in a nutshell, especially when these increasingly intelligent algorithms are learning from the problematic behavior of humans.
Another concern about how algorithms operate is the filter bubbles or echo chambers they create (Blame Facebook for echo chambers.
We are creating billions of terabytes of data through our digital activity, so we need smart algorithms to help us sort through all the data and make it meaningful or useful.
More often than not, the data these machines learn from comes from humans.
Google’s algorithms, for example, learn to recognize speech by listening to tons of Google voice searches performed by humans.”
So there was this Wall Street Journal project (Blue Feed, Red Feed) where they were side-by-siding the news feeds of folks who would self identify as liberal and folks who would self identify as conservative…and it’s like, literally, zero overlap.”
So, beyond the filtering algorithm (and a base of human micro-taggers), the content itself will be constructed by narrow parameters of what is already being consumed.
Some are hopeful that we will learn to mitigate current issues such as biased algorithms.
Interviewees were particularly concerned when they considered that learning algorithms will have access to data about almost every aspect of our daily life with the rise of Internet of Things and facial or DNA analysis tools, which can use your physical participation in the world to gather data to inform fundamental decisions about your financial services, loans, employment, and rental applications.
Learn more about the goals and methods of this research, who produced it, and the interviewees whose insights inform the analysis."
The best of CoRL 2017,I was fortunate enough to attend the very first conference on robot learning (CoRL) held recently in Mountain View. The conference gathered…,"The best of CoRL 2017
I was fortunate enough to attend the very first conference on robot learning (CoRL) held recently in Mountain View. The conference gathered the top researchers in the field and presented the state of the art in terms of applying machine learning to robotic systems. In this post I’ll give a general overview of the works presented as well as my opinions regarding the field and the challenges we will be facing in the near future. You can find videos of the full conference here: Day 1, Day 2, Day 3

The Good News
If you ask most people about AI, one of the first things that will rise in the conversation are robots. However, somewhat surprisingly, most of the work being done in robotics, does not involve machine learning, which became almost a synonym for AI in the past few years. There is a growing gap between robotics and machine learning, which must be breached if our goal is to create autonomous systems with some physical existence in this world. Therefore, the very fact that this conference was established this year is a very good sign and emphasizes that both in the academy and the industry, there is a focus to develop learning capabilities for robots. Several of the works presented described some breakthroughs that can benefit the general ML community. The keynotes were inspirational, giving a fascinating overview of topics such as Human-Robotic Interaction, Active Learning and others. To conclude, the conference presented high quality research, and extensive progress in the field is only a matter of time.
The Not So Good News
One general notion that bothered me was that despite the latest proliferation in robotic platforms it seems that most of the work is still done using well known robotic arm platforms. This inevitably resulted in the fact that governing tasks are still the classic grasping, picking, pushing, reaching etc. Social robots for instance, almost did not get any representation in the works presented. The reasoning behind this is clear and understandable. Years of research around this area resulted in many tools to support using these research platforms from simulators to benchmarks. Yet, I personally feel (and predict) that we must extend our tools to new platforms. Some comfort can be gained by noting that despite the homogeneity in the tasks and platforms, a lot of the work can be generalized to other domains.
The talks
And now to the good stuff. The conference was a one track conference so I got to attend all talks. I cannot of course cover all the papers in this post but I will cover some of the topics talked about in the conference with references to works I found interesting. The papers I leave out are not in any sense inferior to the work that I do reference, but one must make hard choices.
Keynotes
The two talks I found the most interesting were the keynotes by Stefanie Tellex from Brown University and Anca Dragan from Berkley. Both talks discussed human robotics interactions, each from a different perspective. I will only give a brief description of each of their talks but if you find these topics interesting, I strongly encourage you to watch the videos of their talks and of course, take a look at some of their papers.
Anca Dragan’s talk discussed two main issues. The first one is what she called the problem of coordination: how to make a robot actively coordinate with humans rather than giving the humans constant priority. The toy example was cutting in front of a car in order to switch lanes. Classic algorithms will never try to switch lanes if they don’t have room however humans will try this a lot. The general solution is for the robot to account for the utility function of the human it interacts with, within its own utility function (Sadigh et al. 2016). The second part of her talk was devoted to the learning algorithms designers and specifically reward engineering. Here she presented the “inverse reward design” where the designed reward is considered as only an observation over the actual reward taking the training environment into consideration (Hadfield-Menell et al. 2017) .
Stefanie Tellex discussed the right way to model a robotic learning system and how it can incorporate interactions with humans. She described three approaches for robots collaborating with people: 
1. Communication for collaboration: allowing robots to better understand and express language(Whitney et al. 2017, Tellex et al. 2014).
2. Action for collaboration: how robots can plan under a very large, and uncertain state-action space(Gopalan et al. 2017, Arumugam et al. 2017).
3. Perception for collaboration: how to improve the perception (Oberlin and Tellex 2017), all in the purpose of better complying with the robot’s human collaborator needs.
Other keynotes were given by Rodney Brooks, talking about what he considers the seven sins of AI (you can read all about it in his blog post here). Yann Lecun gave a great brief of the “CNN for computer vision” world, emphasizing the importance of unsupervised learning. He weaved it into the conference main theme by presenting some nice robotics applications. I found the final section of his talk, about predictive models, fascinating. He mentioned some video prediction works, memory networks, and also a new work just released to arXiv called error encoding networks, where the network predicts her own errors and feeds them back as a signal for the next prediction. Andrew Bagnell from MIT was the final speaker, discussing RL as an adversarial game and designing methods that tries to find an equilibrium in the game rather than an optimum for the system.
Paper Talks
The paper talks can be roughly divided into 6 topics. I will briefly review each topic presenting a paper or two which I personally liked.
Transferring Knowledge from Simulations to the Real World
An ever-growing concern in the world of robotics is how to generalize models learned in the simulator to the real world. While simulators can capture a lot of the variability in the robotic environment, it can never completely model the real world and therefore models learned in the simulator often fails in real world scenarios. This concern was only amplified with the latest proliferation of deep learning that usually demand massive amounts of data for training. It is generally unrealistic to gather these amounts outside of the simulator. Several papers presented at the conference tackled this issue. One paper that I liked, from DeepMind, called Sim-to-Real Robot Learning from Pixels with Progressive Nets, used a framework called progressive nets. This is an interesting architecture used mainly for continuous learning, i.e. learning tasks in which new data is continuously introduced. The basic architecture is composed of a first network which is being trained on the first (and usually biggest) chunk of data. After training, all the weights in this network are frozen. When a new batch of data arrives, instead of fine tuning the first network, a second network is built, usually with a thinner architecture, and an adapter function is built from all the layers in the first network to the layers in the second network. This adapter function can be as simple as the identity function or it could have learned parameters. Then the second network is trained. This is an iterative process in which technically with each batch of new data, new networks can be built with an adapter function to all previous networks.
Depiction of a progressive network . recreated from figure 1 in the paper
In this paper, the progressive nets architecture was used to adapt a model trained on data from the simulator to the real world. The first network was trained on the simulated data (which was abundant) and it was than adapted using the second network to a small amount of real-world data. This approach gained better results then simply fine tuning the second network. Interestingly, the learned concepts could be adapted also for a different task using a third network trained on this new task.
Active Learning
Active learning is a fascinating field which I am personally very fond of. In my opinion this might be the single most important building block for HRI (human-robotics interaction) to be successful. The basic setup in active learning is of an agent that tries to learn a model of the world. While he is learning, he develops a belief regarding the state of the world with some uncertainty. The unique thing here is that the agent can interact with the world and “ask” for information that might help him. This is the key point — the ability to perform learning by actively gaining information. Robots fit very well with this framework as they are agents that can interact with the world and sometimes are able to ask questions using some modality. Research regarding active learning usually involves optimizing the decisions regarding what to ask and when to ask it. One interesting paper was Opportunistic Active Learning for Grounding Natural Language Descriptions. In this paper, the authors referred to a task where a robot tries to infer the intention of a user based on its natural language description. Specifically, the robot tries to understand which object the user wants it to pick, using only the human description of the object. For example, the user can say “Bring me that blue and narrow bottle”. The authors introduced a new policy, that does not limit the agent to asking questions regarding the task at hand, but allows it to explore using off-topic questions. They called the agent using this policy, the inquisitive agent. They claim that the inquisitive agent gained better results without annoying the users too much (this was measured by a set of questions regarding the interactions).
A description of the decision tree of the robot in the research. Recreated from figure 2 in the paper.
While I find the results and analysis somewhat lacking, I think that the approach is quite interesting and should be further explored.
Imitation Learning
This is a very active field where human knowledge is harnessed into the learning process. The basic idea is to teach robots to learn from human demonstrations. Usually a human will demonstrate how to perform a specific task and the robot is expected to learn and execute the task in a similar manner. The demonstration can include a variety of modalities, from visual (i.e. the robot looks at the demonstrator performing the task) to physical guidance (e.g. The robotic arm is being guided to pick and drop). This is a very powerful tool that could enhance transfer learning to new tasks and allow smart exploration of the state space. Many interesting papers were presented around this topic. IntentionGAN: Multi-Task Imitation Learning from Unstructured Demonstrations was one of my favorites. The authors basically adopted the concepts used in GAN into the imitation learning domain — which is classic in my opinion. A robot tries to infer the policy that imitates the action it sees and a discriminator network tries to discriminate between the robot’s action and a human action. The target of the robot is to minimize the discriminator success. Using this approach they manage to learn different skills from imitations from unlabeled data (demonstrations without the skill label). They experimented their methods on different simulated (only…) tasks yielding good results. Another very impressive paper was: One-Shot Visual Imitation Learning via Meta-Learning. Here the authors aimed for one shot demonstration learning from raw pixels (i.e. learning from one example fed as a video to the model), which is quite ambitious. To achieve this, they relied on a meta learning architecture. The idea is to use data collected from demonstrations of other tasks to learn the parameters of a meta model from which using one gradient step over one demonstration can reach a good performance on a new task. The meta training process includes sampling a batch of tasks and then sampling two demonstrations for each task. For each task: use the first demonstration to perform a GD step and create a set of specialized parameters. Use the specialized parameters from each task to try and imitate the second demonstration. And finally use the second demonstration to evaluate the performance and perform a GD step on the meta parameters. They also introduced some interesting architectural innovations such as a two-headed network that incorporates the meta learning and learning steps together as well as reparameterization of the bias they called “bias transformation” that allows some control over the bias in the shape of some more parameters.
The meta learning network architecture. Recreated from figure 2 in the paper
Their results on both simulations and real robots are quite impressive though one can argue that they did not introduce new tasks but rather used the same tasks with new objects.
Autonomous Driving
There is no need to introduce this topic. Autonomous cars are basically big robots and therefore can be treated as such. The papers presented were mainly focused on the planning and decision making components within the autonomous car algorithmic stack, and less on the perception aspects. Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation is a nice paper that discusses robust sensor fusion. The authors introduce two innovations in their policy network. First is a sensor dropout layer — which is basically dropout in the sensor level, i.e. randomly dropping sensors as part of the training process. This results in a network that is robust facing the loss of sensory input which is essential in an autonomous car setting.
The sensor dropout layer. Recreated from figure 1 in the paper
The other innovation is an auxiliary loss aiming at reducing the variance between policies inferred by different subsets of the same sensory input. So instead of training the network to infer the ground truth policy for each subset of sensors, also train it to produce similar outputs regardless of it being close or far from the GT.
One cool paper was: Emergent Behaviors in Mixed-Autonomy Traffic. It discusses the “Mixed autonomy traffic problem” in which both autonomous and human agents are taking part of the same system. The autonomous agents’ objective is to optimize traffic given the presence of the human agents. They showed some nice demos in which introducing only one autonomous car (!) into a system results in completely abolishing traffic jams.
Reinforcement Learning
As expected, the most prevalent learning framework in the conference was reinforcement learning. Many of the papers introduced innovations in RL. In Reverse Curriculum Generation for Reinforcement Learning the authors aim to deal with the problem of learning under a sparse reward function. In many tasks, there is a goal to be reached without getting any reward signal before reaching the goal. For instance, games can have only a win/loss signal at the end of the game without any intermediate reward. Several robotic tasks suffer from the same problem where reward is only given at the end of the task. The reward in these goal-oriented RL problems is binary and sparse over the state space. The problem with sparse reward is that most samples will have no signal and therefore will not be useful in learning, the only useful samples are those that reach the target and they are too few. The signal propagates slowly and learning is hard. The common solution to this problem is engineering the reward function to be denser. The problem with this approach is that when the reward does not actually represent the target, it could easily lead to an undesired outcome. Instead of reward engineering, the authors of this paper suggest changing the sampling regime. It requires the knowledge of one target-point, the algorithm samples starting points close to this target-point and uses them in its learning. Under the assumption that close to the target it is easily reachable, the local sampling should lead to several positive trajectories. After each iteration, the starting points that turned out to be “successful” are concatenated to the target point and the sampling process repeats itself, expanding the circle of possible starting points. The signal propagates in reverse and learning is fast and feasible. There are a few delicate points here worth mentioning. First is the notion of proximity: which states can be considered close to the target point? The approach taken in the paper is noise sampling in action space around the known “good points” (i.e. the target points, and all the starting points concatenated to it in following iterations). Another important issue is balancing between old and new starting points, in each iteration the authors discover “new” starting points from the sampling procedure, they then concatenate a group of “old” starting points from previous iteration to keep refining the estimation. The final starting point for each training iteration is sampled from the group of old and new starting points. The method is tested on four different challenging tasks, including trying to place a ring on a peg and key insertion, both usually fail under reward engineering. Good results are obtained on all tasks, some of them are only able to gain actual success by using this method.
Other interesting papers which I couldn’t fit into a specific category:
Self-Supervised Visual Planning with Temporal Skip Connections 
Using a CNN based Video prediction model to predict how an action will affect the state. Using an RNN that gets a frame and action and outputs the transformation matrices to the next frame. They introduce skip connections to enable looking to the past and deal with occlusions.
How Robots Learn to Classify New Objects Trained from Small Data Sets
Using Progressive Neural Networks for transfer learning from one task to another.
Learning to Fly by Crashing
Using standard RL but letting the drone explore and crush if needed. They had really funny videos and gained great results.
Harvesting Common-sense Navigational Knowledge for Robotics from Uncurated Text Corpora
Tries to apply the word2vec arithmetic trick (ie.g. king-man+woman=queen) in a better manner introducing a new operator that gives more weight to the direction of the difference vector on the expense of its length.
Datasets
Finally, there were some new datasets introduced in the conference, the most noteworthy is CARLA: An Open Urban Driving Simulator which looks like the next generation of simulators for autonomous car. It includes many environments, weathers, and even sensor inputs from the car, and it is completely open sourced, looks quite impressive. Other datasets were presented in the papers:
image2mass: Estimating the Mass of an Object from Its Image: Mapping between object from amazon and their mass.
Semi-Supervised Haptic Material Recognition for Robots using Generative Adversarial Networks: includes a dataset taken from touch sensors applied to many objects.
CORe50: a New Dataset and Benchmark for Continuous Object Recognition: Dataset composed of 50 temporally changing objects. Includes three benchmarks: New instances, new classes and new instances and classes.
If you survived so far, I guess you are really interested in these stuff and you should probably attend the conference next year :) My personal experience was great, I suspect we will see proliferation in this area in the upcoming years. With new platforms that are being introduced, new challenges arises, and with them, the interest of the community. I am really looking forward to seeing the progress in this fascinating field.
",3185,Machine Learning,Shay Zweig,https://medium.com/s/story/the-best-of-corl-2017-cfcd90dee9db,"I was fortunate enough to attend the very first conference on robot learning (CoRL) held recently in Mountain View.
The conference gathered the top researchers in the field and presented the state of the art in terms of applying machine learning to robotic systems.
However, somewhat surprisingly, most of the work being done in robotics, does not involve machine learning, which became almost a synonym for AI in the past few years.
There is a growing gap between robotics and machine learning, which must be breached if our goal is to create autonomous systems with some physical existence in this world.
Therefore, the very fact that this conference was established this year is a very good sign and emphasizes that both in the academy and the industry, there is a focus to develop learning capabilities for robots.
The keynotes were inspirational, giving a fascinating overview of topics such as Human-Robotic Interaction, Active Learning and others.
I cannot of course cover all the papers in this post but I will cover some of the topics talked about in the conference with references to works I found interesting.
Both talks discussed human robotics interactions, each from a different perspective.
I will only give a brief description of each of their talks but if you find these topics interesting, I strongly encourage you to watch the videos of their talks and of course, take a look at some of their papers.
The second part of her talk was devoted to the learning algorithms designers and specifically reward engineering.
Stefanie Tellex discussed the right way to model a robotic learning system and how it can incorporate interactions with humans.
An ever-growing concern in the world of robotics is how to generalize models learned in the simulator to the real world.
One paper that I liked, from DeepMind, called Sim-to-Real Robot Learning from Pixels with Progressive Nets, used a framework called progressive nets.
This is an interesting architecture used mainly for continuous learning, i.e. learning tasks in which new data is continuously introduced.
In this paper, the progressive nets architecture was used to adapt a model trained on data from the simulator to the real world.
Interestingly, the learned concepts could be adapted also for a different task using a third network trained on this new task.
One interesting paper was Opportunistic Active Learning for Grounding Natural Language Descriptions.
In this paper, the authors referred to a task where a robot tries to infer the intention of a user based on its natural language description.
Specifically, the robot tries to understand which object the user wants it to pick, using only the human description of the object.
The authors introduced a new policy, that does not limit the agent to asking questions regarding the task at hand, but allows it to explore using off-topic questions.
This is a very active field where human knowledge is harnessed into the learning process.
The basic idea is to teach robots to learn from human demonstrations.
Usually a human will demonstrate how to perform a specific task and the robot is expected to learn and execute the task in a similar manner.
This is a very powerful tool that could enhance transfer learning to new tasks and allow smart exploration of the state space.
Many interesting papers were presented around this topic.
A robot tries to infer the policy that imitates the action it sees and a discriminator network tries to discriminate between the robot’s action and a human action.
The idea is to use data collected from demonstrations of other tasks to learn the parameters of a meta model from which using one gradient step over one demonstration can reach a good performance on a new task.
They also introduced some interesting architectural innovations such as a two-headed network that incorporates the meta learning and learning steps together as well as reparameterization of the bias they called “bias transformation” that allows some control over the bias in the shape of some more parameters.
The meta learning network architecture.
Their results on both simulations and real robots are quite impressive though one can argue that they did not introduce new tasks but rather used the same tasks with new objects.
Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation is a nice paper that discusses robust sensor fusion.
In Reverse Curriculum Generation for Reinforcement Learning the authors aim to deal with the problem of learning under a sparse reward function.
The problem with sparse reward is that most samples will have no signal and therefore will not be useful in learning, the only useful samples are those that reach the target and they are too few.
How Robots Learn to Classify New Objects Trained from Small Data Sets
Using Progressive Neural Networks for transfer learning from one task to another.
Finally, there were some new datasets introduced in the conference, the most noteworthy is CARLA: An Open Urban Driving Simulator which looks like the next generation of simulators for autonomous car."
Bulletin February 9 2018. AI and the theory of exponential linearity,"You know that thing where, in a discussion, a concept comes seemingly out of nowhere, but then changes the whole dialogue? So it was in a…","Bulletin February 9 2018. AI and the theory of exponential linearity
You know that thing where, in a discussion, a concept comes seemingly out of nowhere, but then changes the whole dialogue? So it was in a recent conversation with my old friend and colleague Roger Davies, him who has written whole books on Value Management (kind of, what to do when you realise Balanced Scorecard can only take things so far) and who is currently working on his thesis.
“Of course, despite all this innovation, our progress has only been linear,” I blurted, immediately finding myself in a position where I had to justify my remark. “Is that true?” asked Roger. “Yes,” I said, sounding much less confident than I felt. But as we explored the idea, it did seem to hold water — are we any better at communicating, for example, or really any more productive than we were two or three decades ago?
This question is writ large when it comes to Artificial Intelligence, whose impact, we are told, is to be transformative. I admit to being flummoxed in another conversation, this time just before Christmas at the Great Telco Debate, when I was told in very clear terms that AI would, indeed, change just about everything. He clearly interpreted my uncertainty as ignorance, as he undertook to explain (as one might to a child) why this should be.
Interestingly, the example he cited was telecommunications complexity, what with Network Function Virtualisation (NFV), 5G and the like. I was almost immediately taken back to the early Noughties, when I presented on neural networks in enterprise management, on behalf of CA. Also on the bill was an Italian researcher with several PhDs — “Maybe we could share research,” he suggested. I politely declined, not wanting to show him up with my hastily collated notes and superficial thinking.
The thing about all this fantastic innovation is that it has a fractal effect on the problem space. I know, that’s me getting pompous again, but it happens every time: we are as sorcerers’ apprentices, generating data as quickly as we work out new ways to harness it. The impact on most technological fields is that breakthroughs tend to be quite tightly scoped — at the moment, voice, image and other signal processing are seeing the most progress, and will rightly yield some great results alongside the dumb animations we can add to video chats.
But nobody has yet created an algorithm that can somehow get ahead of the game. To my delight and relief, someone far smarter than me has captured this in an article, “The impossibility of intelligence explosion,” i.e. that point at which our own intellects get left behind. By extension, as long as they (our intellects) do not (get left behind), we are stuck with them in all their wonderful, dumb glory. In the meantime, AI will serve to augment, not replace capabilities including that litmus-test topic, radiology.
Don’t get me wrong, these are transformative times. But even so, our relationship with technology remains that of a craftsperson and their tools — it is up to us to prioritise, to architect, to make things happen. We should not expect our environments, or relationships, our abilities or anxieties to be that much different in ten years’ time, compared to now. And that is a good thing.
And so, to some articles.
The digital world needs new lawmakers
</soapbox on>In case it isn’t clear enough from this article, I’m none too comfortable with the repeated, deal-with-problems-after-they-happen approach to lawmaking in the information age. Moves are being made to change this, notably in the financial industry, but here in the UK for example, our general legislative processes still take place over months or years. Not only is this too slow, but it is massively inefficient and open to exploitation by corporations and cybercriminals alike.
As Einstein said, “We can’t solve problems by using the same kind of thinking we used when we created them.” Word.</soapbox off>
On luxmobiles and flying unicorns: how diversification and proliferation will rule the routes
On a much lighter note, yes, I very much enjoyed writing this — but it has a serious point. If vehicles are no longer constrained by needing a driver, what will they become? I posited some examples as a staring point:
Luxmobiles and budget pods. Like Apple vs Android, the market tends to diverge
Pizza box scooters. Expect transport the shape and size of its load
Drone swarms. No doubt flying in some sponsored logo formation
Super Strings. I should really patent the coupling mechanism now…
Public Subs. What about underwater package delivery? Really, why not?
Vompods. Awkward but who wants to be the third drop-off after a late night out?
Flying Unicorns. For multi-functional transport, the aerial has to go somewhere
The last one was first postulated in our ‘techsplaining’ podcast, the ironic title of which seems strangely appropriate.
Webinar: DevOps in the Real World
Yes, I’ve been that person who tries to get people to do things in different ways to previously. Indeed, in the course of this bulletin it was good to hook up again with Mary Lynn Manns, co-author of the book Fearless Change: patterns for introducing new ideas, to which I was delighted to contribute something back in 2004 (my proposed pattern was “Champion Skeptic” — who would have guessed!). I’m rambling, but the long and short is that I am well aware you can’t just tell people to ‘do’ agile, or DevOps, or anything else that makes them have to think differently. Even if it’s a really good idea. And even if you are a guru.
That’s why I’m really looking forward to having a virtual fireside chat about it all with Nigel Kersten of Puppet Labs. If you’re interested you can sign up here.
Extra-curricular
The whirlwind romance with piano continues — still two hours a night, as the latest video blog shows I’ve started to crack two hands though I’m a long way from being able to just launch into playing a piece. Fur Elise is getting a lot of play at the moment, (at least the first, easier bit).
And meanwhile that august institution, the Society of Authors has seen fit to re-post my blog on “How Not To Be A Biographer. Which is nice.
Finally, thank you to all my subscribers. Any questions or feedback, let me know.
Until next time, Jon

",1082,Artificial Intelligence,Jon Collins,https://medium.com/s/story/bulletin-february-9-2018-ai-and-the-theory-of-exponential-linearity-96331b949a8c,"You know that thing where, in a discussion, a concept comes seemingly out of nowhere, but then changes the whole dialogue?
So it was in a recent conversation with my old friend and colleague Roger Davies, him who has written whole books on Value Management (kind of, what to do when you realise Balanced Scorecard can only take things so far) and who is currently working on his thesis.
I admit to being flummoxed in another conversation, this time just before Christmas at the Great Telco Debate, when I was told in very clear terms that AI would, indeed, change just about everything.
I know, that’s me getting pompous again, but it happens every time: we are as sorcerers’ apprentices, generating data as quickly as we work out new ways to harness it.
The impact on most technological fields is that breakthroughs tend to be quite tightly scoped — at the moment, voice, image and other signal processing are seeing the most progress, and will rightly yield some great results alongside the dumb animations we can add to video chats.
To my delight and relief, someone far smarter than me has captured this in an article, “The impossibility of intelligence explosion,” i.e. that point at which our own intellects get left behind.
</soapbox on>In case it isn’t clear enough from this article, I’m none too comfortable with the repeated, deal-with-problems-after-they-happen approach to lawmaking in the information age.
As Einstein said, “We can’t solve problems by using the same kind of thinking we used when we created them.” Word.</soapbox off>
On luxmobiles and flying unicorns: how diversification and proliferation will rule the routes
Yes, I’ve been that person who tries to get people to do things in different ways to previously.
Indeed, in the course of this bulletin it was good to hook up again with Mary Lynn Manns, co-author of the book Fearless Change: patterns for introducing new ideas, to which I was delighted to contribute something back in 2004 (my proposed pattern was “Champion Skeptic” — who would have guessed!).
I’m rambling, but the long and short is that I am well aware you can’t just tell people to ‘do’ agile, or DevOps, or anything else that makes them have to think differently.
The whirlwind romance with piano continues — still two hours a night, as the latest video blog shows I’ve started to crack two hands though I’m a long way from being able to just launch into playing a piece."
Making your own Face Recognition System,Face recognition is the latest trend when it comes to user authentication. Apple recently launched their new iPhone X which uses Face ID to…,"Making your own Face Recognition System

Face recognition is the latest trend when it comes to user authentication. Apple recently launched their new iPhone X which uses Face ID to authenticate users. OnePlus 5 is getting the Face Unlock feature from theOnePlus 5T soon. And Baidu is using face recognition instead of ID cards to allow their employees to enter their offices. These applications may seem like magic to a lot of people. But in this article we aim to demystify the subject by teaching you how to make your own simplified version of a face recognition system in Python.
Github link for those who do not like reading and only want the code
Background
Before we get into the details of the implementation I want to discuss the details of FaceNet. Which is the network we will be using in our system.
FaceNet
FaceNet is a neural network that learns a mapping from face images to a compact Euclidean space where distances correspond to a measure of face similarity. That is to say, the more similar two face images are the lesser the distance between them.
Triplet Loss
FaceNet uses a distinct loss method called Triplet Loss to calculate loss. Triplet Loss minimises the distance between an anchor and a positive, images that contain same identity, and maximises the distance between the anchor and a negative, images that contain different identities.
Figure 1: The Triplet Loss equation
f(a) refers to the output encoding of the anchor
f(p) refers to the output encoding of the positive
f(n) refers to the output encoding of the negative
alpha is a constant used to make sure that the network does not try to optimise towards f(a) - f(p) = f(a) - f(n) = 0.
[…]+ is equal to max(0, sum)
Siamese Networks
Figure 2: An example of a Siamese network that uses images of faces as input and outputs a 128 number encoding of the image. Source: Coursera
FaceNet is a Siamese Network. A Siamese Network is a type of neural network architecture that learns how to differentiate between two inputs. This allows them to learn which images are similar and which are not. These images could be contain faces.
Siamese networks consist of two identical neural networks, each with the same exact weights. First, each network take one of the two input images as input. Then, the outputs of the last layers of each network are sent to a function that determines whether the images contain the same identity.
In FaceNet, this is done by calculating the distance between the two outputs.
Implementation
Now that we have clarified the theory, we can jump straight into the implementation.
In our implementation we’re going to be using Keras and Tensorflow. Additionally, we’re using two utility files that we got from deeplearning.ai’s repo to abstract all interactions with the FaceNet network.:
fr_utils.py contains functions to feed images to the network and getting the encoding of images
inception_blocks_v2.py contains functions to prepare and compile the FaceNet network
Compiling the FaceNet network
The first thing we have to do is compile the FaceNet network so that we can use it for our face recognition system.
We’ll start by initialising our network with an input shape of (3, 96, 96). That means that the Red-Green-Blue (RGB) channels are the first dimension of the image volume fed to the network. And that all images that are fed to the network must be 96x96 pixel images.
Next we’ll define the Triplet Loss function. The function in the code snippet above follows the definition of the Triplet Loss equation that we defined in the previous section.
If you are unfamiliar with any of the Tensorflow functions used to perform the calculation, I’d recommend reading the documentation (for which I have added links to for each function) as it will improve your understanding of the code. But comparing the function to the equation in Figure 1 should be enough.
Once we have our loss function, we can compile our face recognition model using Keras. And we’ll use the Adam optimizer to minimise the loss calculated by the Triplet Loss function.
Preparing a Database
Now that we have compiled FaceNet, we are going to prepare a database of individuals we want our system to recognise. We are going to use all the images contained in our images directory for our database of individuals.
NOTE: We are only going to use one image of each individual in our implementation. The reason is that the FaceNet network is powerful enough to only need one image of an individual to recognise them!
For each image, we will convert the image data to an encoding of 128 float numbers. We do this by calling the function img_path_to_encoding. The function takes in a path to an image and feeds the image to our face recognition network. Then, it returns the output from the network, which happens to be the encoding of the image.
Once we have added the encoding for each image to our database, our system can finally start recognising individuals!
Recognising a Face
As discussed in the Background section, FaceNet is trained to minimise the distance between images of the same individual and maximise the distance between images of different individuals. Our implementation uses this information to determine which individual the new image fed to our system is most likely to be.
The function above feeds the new image into a utility function called img_to_encoding. The function processes an image using FaceNet and returns the encoding of the image. Now that we have the encoding we can find the individual that the image most likely belongs to.
To find the individual, we go through our database and calculate the distance between our new image and each individual in the database. The individual with the lowest distance to the new image is then chosen as the most likely candidate.
Finally, we must determine whether the candidate image and the new image contain the same person or not. Since by the end of our loop we have only determined the most likely individual. This is where the following code snippet comes into play.
If the distance is above 0.52, then we determine that the individual in the new image does not exist in our database.
But, if the distance is equal to or below 0.52, then we determine they are the same individual!
Now the tricky part here is that the value 0.52 was achieved through trial-and-error on my behalf for my specific dataset. The best value might be much lower or slightly higher and it will depend on your implementation and data. I recommend trying out different values and see what fits your system best!
Building a System using Face Recognition
Now that we know the details on how we recognise a person using a face recognition algorithm, we can start having some fun with it.
In the Github repository I linked to at the beginning of this article is a demo that uses a laptop’s webcam to feed video frames to our face recognition algorithm. Once the algorithm recognises an individual in the frame, the demo plays an audio message that welcomes the user using the name of their image in the database. Figure 3 shows an example of the demo in action.
Figure 3: An image captured at the exact moment when the network recognised the individual in the image. The name of the image in the database was “skuli.jpg” so the audio message played was “Welcome skuli, have a nice day!”
Conclusion
By now you should be familiar with how face recognition systems work and how to make your own simplified face recognition system using a pre-trained version of the FaceNet network in python!
If you want to play around with the demonstration in the Github repository and add images of people you know then go ahead and fork the repository.
Have some fun with the demonstration and impress all your friends with your awesome knowledge of face recognition!
",1431,Tech,Sigurður Skúli,https://medium.freecodecamp.org/making-your-own-face-recognition-system-29a8e728107c,"FaceNet is a neural network that learns a mapping from face images to a compact Euclidean space where distances correspond to a measure of face similarity.
Figure 2: An example of a Siamese network that uses images of faces as input and outputs a 128 number encoding of the image.
Then, the outputs of the last layers of each network are sent to a function that determines whether the images contain the same identity.
fr_utils.py contains functions to feed images to the network and getting the encoding of images
inception_blocks_v2.py contains functions to prepare and compile the FaceNet network
The first thing we have to do is compile the FaceNet network so that we can use it for our face recognition system.
Once we have our loss function, we can compile our face recognition model using Keras.
Now that we have compiled FaceNet, we are going to prepare a database of individuals we want our system to recognise.
We are going to use all the images contained in our images directory for our database of individuals.
NOTE: We are only going to use one image of each individual in our implementation.
The reason is that the FaceNet network is powerful enough to only need one image of an individual to recognise them!
The function takes in a path to an image and feeds the image to our face recognition network.
Once we have added the encoding for each image to our database, our system can finally start recognising individuals!
Our implementation uses this information to determine which individual the new image fed to our system is most likely to be.
The function processes an image using FaceNet and returns the encoding of the image.
If the distance is above 0.52, then we determine that the individual in the new image does not exist in our database.
Once the algorithm recognises an individual in the frame, the demo plays an audio message that welcomes the user using the name of their image in the database."
How to build and deploy a lyrics generation model — framework agnostic,You’ll find tons of article about how to build a machine learning model. You’ll find a bit less article on how to consume it intelligently…,"How to build and deploy a lyrics generation model — framework agnostic

You’ll find tons of article about how to build a machine learning model. You’ll find a bit less article on how to consume it intelligently. And you’ll find almost no article about how to serve it from scratch.
I’ll detail the steps that took us to the product you can see above: raplyrics.eu
All codes are open source and available on GitHub. 
- RapLyrics-Scraper
- RapLyrics-Back
- RapLyrics-Front
What?
With a good friend of mine we really love to listen to rap music. Rap music is powerful because it has the power of creating savage punchline with only a few words.
Since it is still hard to generate long texts with RNN, we believed rap music was a great candidate.
The final product
How?
The big picture
Project’s architecture
I won’t be too descriptive about implementation in the post since we tried to be exhaustive inside the code repositories, see READMEs. I will insist on the tipping points that were challenging for us.
Basic sysadmin knowledge and unix proficiency will help.
1- Data Extraction and processing
GitHub repository: RapLyrics-Scraper
— scraping

First, we need a dataset to train our neural network.
Luckily enough Genius.com has tons of lyrics available online and even a nice API.
It may not be designed to scrap lyrics but with some workarounds we managed to build a lyrics scraper on top of it.
Check the source code or reach out in comments if you need technical details.
After multiple shots, we realized that it’s really important to focus on a high-quality dataset for natural language processing. We decided to focus on the 60 most popular songs of 40 US artists.
✔ That’s it for the scrapping.
— pre-processing
The scraping part provides us with a .txt dataset. We now have to clean it — i.e. remove non lyrical content: ©, ®, Credits, typos and various spellings of the same word. Think about gettin', getting and stuff like this.
Methodology we followed:
1. Identify patterns to eliminate 
2. Craft regex catching those patterns — resource for regex testing: pythex.org
3. Use a text editor to perform those regexes directly on the dataset
If you want to automate regex cleaning, be aware that it is risky. You’ll have to thoroughly consider the order in which you perform your regular expressions.
— augmenting the dataset [optional]
We chose only artists with really meaningful lyrics and we selected their most popular songs. That does not make a huge corpus. Hence, we decided to perform a step of data augmentation to virtually increase the size of our dataset.
📖 Data augmentation means increasing the number of data points. In our context, it means increasing the number of sentences.
We copied our dataset, shuffled all the verses and pasted this back at the end of the original dataset.
You can find a snippet on how to shuffle paragraphs here.
With this trick we double the size of our dataset. This will have a positive impact on the training of the neural networks. Indeed each new batch is different due to the shuffling, so the network weights are updated with different inputs.
✔ That’s it for the data augmentation.
2- Building a lyrics generative model
GitHub repository: RapLyrics-Back
— dimensioning the text generative model

Many neural networks implementation are available online. We chose one and fine-tuned it to fit our need: textgenrnn — a python project for text-generation using neural networks.
You can find a basic description of the model’s hyperparameters and the training settings in our code repositories READMEs
The purpose of this article is not to deep-dive into neural networks design. The implementation won’t be detailed. You can check in the source code or ping us in the comments.
— training the text generative model
Depending on your dataset and your configuration, you may consider cloud computing to speed-up the training. We used aws — Amazon Web Service.
If you train your model locally — you can skip this part. Otherwise, consider that the following part will get a bit technical.
I will detail our training setup in more details since it is something which took us time to get right.
We launched an aws ec2 spot instance to reduce our cost. We need at least 3gb ram and the 8gb default ssd is enough. The training was not GPU accelerated (a point of amelioration).
83% savings on a spot instance comparing to a classic ec2 instance
How is an ec2 spot instance different from a classical ec2 instance?
You bid for an ec2 instance with certain specs and, as long as your bid is above the average market price, you have an instance behaving like a classic ec2.
If your bid is below the the market price you instance is terminated after a short notice. More info on spot instances.
We made a spot request, it was fulfilled in no time and then we cloned our repo and installed a python3 virtual env with all the project requirements.
Note: You need to enable your instance to write on s3 bucket if you want to save your model checkpoints (as seen 👇)
texgenrnn saves a model checkpoint at each epoch.
To cope with the risk of instance termination and save our checkpoints in a safe place, we use aws cli to copy the checkpoints in an aws s3 bucket. cd to your checkpoint files and copy them to your s3 bucket.
Note: To make this possible, you need to grant write access to your ec2 to instance. To do this, add a role to your ec2 instance with s3 full access and ec2 full access policies as described in the screenshot below.
Attach those 2 policies to the IAM role attached to your ec2 instance.
There are many sneaky details with policy handling, don’t hesitate to ask us in comments.
— testing the text-generation
Once you have trained your model you can use the Jupyter notebook RapLyrics-Back/exploration/sample_explorator.ipynb to generate your first AI-powered lyrics.
3. Serving the text generative model
For the purpose of providing users with better lyrics, we use a custom generation function.

We serve the app using gunicorn over Flask. The idea is not to reload the model at each API call — which would lead to long response time.
We restore the session only once at the app initialization and it persists between API calls.
Demo of the call to the API and its response.
curl on back-end
If you haven’t implemented the model yet, feel free to call our API:
See the get_model_api_us function in api/serve_us.py on how we setup a persistent tensorflow session. Simply run gunicorn app:app from the shell to launch the app. The model is served on 127.0.0.1:8000 by default.
You can now clone RapLyrics-Back on the machine that will be used as a web-server.
4- Plugging in the front end
GitHub repository: RapLyrics-Front
This article describes the necessary steps for an apache web-server. If you don’t have it sudo apt-get install apache.
Move all the files to be served from RapLyrics-Front to /var/www/html/
Remember to update the ""url"" settings of your endpoint in the index.html 
That’s it, you’re done (kind of).
You can now access the website by accessing your server ip in a web browser.
— production set-up [optional]
These are the next steps if you want to have the front-end and back-end on the same machine with an https connection.
1. Let’s encrypt our website 🔒 → follow the steps in How To Secure Apache with Let’s Encrypt (Digital Ocean has really awesome tutorials)
2. Our index.html served by apache calls raplyrics.eu/apiUS when the user submits an input. In fact, there is no /apiUS route on apache. We need to redirect this call to the gunicorn server running on this very same machine. This is what is called reverse-proxying.
Let’s handle these two steps.
Since the code is related to apache configuration, it is not version controlled.
Go to /etc/apache2/sites-available
You should see a 000-default.conf and a 000-default-le-ssl.conf file. They are template files handling configuration on how apache will serve your http and https (le-ssl) website.
We make a copy of them for our website. (replace raplyrics.eu with your domain name 👇)
sudo cp 000-default.conf raplyrics.eu.conf 
sudo cp 000-default-le-ssl.conf raplyrics.eu-le-ssl.conf
1. Redirect traffic from http to https
Edit raplyrics.eu.conf to include the rewrite conditions below:
Rewrite rules to redirect traffic from http to https. Remember to replace raplyrics.eu by your site name.
2. Reverse proxy the API call
Edit raplyrics.eu-le-ssl.conf to include the proxy reverse instructions.
Reverse proxy to redirect the API call to the gunicorn app
It is here that we handle the proxy pass from raplyrics.eu/apiUS to the local gunicorn server at 127.0.0.1:8000
Now we tell apache to update the website configuration:
sudo a2ensite raplyrics.eu.conf 
sudo a2ensite raplyrics.eu-le-ssl.conf
Finally, sudo systemctl restart apache2 to take the changes into account.
That’s it, you’re in production. 🚀 
You can check ours on raplyrics.eu
References 📚
Interesting blog post on Serving a python app on heroku (Heroku dynos could not handle our app — not enough ram) steps are well described.
Reverse proxying on apache2
Digital ocean reverse proxying on apache, again Digital Ocean does a much appreciated documentation job, very detailed.
Very interesting SO post on How to build a CI / CD pipeline with GitLab and AWS
Inspiration for the neural networks parameters fine-tuning
Interesting Cross-Validated post on fine tuning batch-size training parameter.
Google Brain paper proposing a set of hyper parameters for a text-generative LSTM (especially the 4. Experiment and 4.1 Language modeling for our use case.)
",1610,Python,François Paupier,https://towardsdatascience.com/how-to-build-and-deploy-a-lyrics-generation-model-framework-agnostic-589f3026fd53,"How to build and deploy a lyrics generation model — framework agnostic
You’ll find tons of article about how to build a machine learning model.
I’ll detail the steps that took us to the product you can see above: raplyrics.eu
Since it is still hard to generate long texts with RNN, we believed rap music was a great candidate.
First, we need a dataset to train our neural network.
Check the source code or reach out in comments if you need technical details.
3. Use a text editor to perform those regexes directly on the dataset
Hence, we decided to perform a step of data augmentation to virtually increase the size of our dataset.
2- Building a lyrics generative model
— dimensioning the text generative model
We chose one and fine-tuned it to fit our need: textgenrnn — a python project for text-generation using neural networks.
You can find a basic description of the model’s hyperparameters and the training settings in our code repositories READMEs
— training the text generative model
Note: You need to enable your instance to write on s3 bucket if you want to save your model checkpoints (as seen 👇)
Once you have trained your model you can use the Jupyter notebook RapLyrics-Back/exploration/sample_explorator.ipynb to generate your first AI-powered lyrics.
3. Serving the text generative model
The model is served on 127.0.0.1:8000 by default.
You can now clone RapLyrics-Back on the machine that will be used as a web-server.
This article describes the necessary steps for an apache web-server.
2. Our index.html served by apache calls raplyrics.eu/apiUS when the user submits an input.
We need to redirect this call to the gunicorn server running on this very same machine.
They are template files handling configuration on how apache will serve your http and https (le-ssl) website.
sudo cp 000-default.conf raplyrics.eu.conf 
Edit raplyrics.eu-le-ssl.conf to include the proxy reverse instructions.
Reverse proxy to redirect the API call to the gunicorn app
It is here that we handle the proxy pass from raplyrics.eu/apiUS to the local gunicorn server at 127.0.0.1:8000
sudo a2ensite raplyrics.eu-le-ssl.conf
Reverse proxying on apache2"
Product Breakdown: Spotify,"In this article I provide a detailed analysis of Spotify as a company (music industry direction, market competition, growth strategies) and…","Product Breakdown: Spotify

In this article I provide a detailed analysis of Spotify as a company (music industry direction, market competition, growth strategies) and as a product (feature analysis, overview of ML at Spotify, UI suggestions).
Even though Spotify is busy cleaning up its lawsuits and debts before its projected public offering, investors are valuing the music streaming giant at $16 billion. Having 70 million out of 146.6 million total paying music subscribers worldwide, Spotify sits at owning just under 50% share of the music streaming market.
Know thy history…and repeat it.
So what brought Spotify to where it is today? There are a few key strategies that worked and continue to work well:
Social features:
Subscribing to your friends to see what they are listening to. Since Spotify is integrated with Facebook, finding your friends is trivial. Seeing what your friends listen to makes you feel more connected and adds an extra point to music discovery.
Sharing songs/playlists with anyone via a simple link directly to the platform. Each share acts like an invite, promoting growth.
Public playlists account for a major part of music discovery on Spotify. Having 30 million songs available, Spotify has a whopping 2 billion playlists. Such an enormous number is no surprise, considering the platform design promotes making playlists and keeping them public. Playlists are public by default, creating friction to hide your music tastes. Making a playlist either from scratch or by adding onto an existing one is just a click away.
Smart pricing (regular subscription is 10$/month):
Discounted 5$ subscription for students to capture the market segment that is the most present in online social networks, thus being the most likely to cause cascades of invites to use the service.
Family subscription at 15$ for up to 6 people, incentivizing the whole family to switch to one platform, reducing the market share of competitors.
Locked-in design. As users grow their collection of saved songs and tailored playlists, they are increasingly more likely to keep using Spotify because there is no feature to export user libraries outside of Spotify’s ecosystem. While this may act as a deterrent to joining Spotify, it is a smart strategy to keep as it acts as a strong barrier of entry to new competitors. Unless the competitor can offer something drastically different and better, most users who already have a collection on Spotify will not switch. To compensate for the discomfort of not owning your music, Spotify implicitly promotes the belief that there is an enormous ocean of music that should be our actual interest, not just keeping the music you already have.
Authenticity. It is tempting to monetize the music selection algorithms by allowing songs to be “promoted” on them, but so far Spotify has been right to fear losing listeners’ trust. By keeping listeners’ interests at heart, Spotify quickly got everyone comfortable with its music finding features such as Discover Weekly and artist/song radios.
Data-driven culture: Spotify almost immediately started pursuing the goal of getting to know music algorithmically, seeing it as the best way to develop its features. While Apple Music was boasting its “hand-curated playlists”, Spotify was busy hiring as many Machine Learning/Data Science experts from the field as it could afford.
Spotify — your guide into the world of music.
More than anything, Spotify seeks to act like and be thought of as a tour guide, an algorithmic guru that holds your hand on your journey to music enlightenment. That is reflected in Spotify’s marketing and UI.
source
Many chuckle at Spotify’s witty campaigns (2016 – 2018) that often parallel important news (“Dear 3,749 people who streamed “It’s the End of the World As We Know It” the day of the Brexit vote, Hang in there.”), or light-heartedly ridicule social norms (“Exercise more conventionally than the 46 people who put “Slow Hands” on their running playlists.”). Aside from astute comedy, common themes among them are data-informed knowledge of music, and attention to users.
The UI is designed in such a way that every element acts like a door waiting to be opened, a new territory waiting to be explored. If you like the song — right-click to start the song radio to find similar music. Looking for something new — check your Discover Weekly or click on one of your friends’ feeds. Want to tune out to your favorite jams — choose one of your Daily Mix playlists.
More formally, Spotify’s UI is a mere surface of the Machine Learning iceberg that lives at the heart of Spotify. There are 3 main sources of data it has:
playlists (information about what music is treated as similar),
individual listening histories (patterns in the order that music is being played),
likes/dislikes/skips from the radio stations (information about how songs mix together).
All of that is content-agnostic data, but by using neural networks to predict that data only from the audio signal (i.e. song content itself), Spotify’s algorithms also become content-aware.
Spotify is using a whole array of collaborative filtering methods (a decomposition of an artist-user, song-user matrix that becomes compact by finding patterns in the data) [Spotify’s slides] that gives it latent representations (vectors of numbers) of each user and each artist/song. That gives Spotify a comprehensive map of music, where similar songs are positioned together and users tend to listen to music only in certain areas of that map.
t-SNE visualization of found latent space (i.e. map of music), [A. van den Oord, S. Dieleman, B. Schrauwen]
By traversing that map, Spotify can find songs that are close, yet novel, relative to your indicated preferences (e.g. Discover Weekly, suggested songs to playlists) or to a given query (e.g. radio stations of a playlist, song, artist, etc.). [verbose explanation]
To summarize, most of Spotify’s navigation features are just a wrapper for navigating the music map, only going to areas that are or are likely to be of interest to you.
Quality music requires happy artists.
Spotify is not one to boast the highest pay per stream rate. At $0.0038 per stream, it is behind Google play ($0.0059), Apple Music ($0.0064), but ahead of Pandora ($0.0011) and YouTube ($0.0006). [informationisbeautiful] Yet Spotify is not being stingy — it pays out around 70% of its revenues to the industry, mostly in royalties. The reason lies in the market history and aggressive artist-label contracts.
Economist article
Back in 2008, Spotify inherited the state of the music industry at its decline as the internet was chewing away music profits with a culture of free downloads. However, even at the music industry’s peak in 1999, consumers spent an average of $28/year(41.46$ if adjusted for inflation) on music. Today, many are willing to pay for music again, even at $120 per year of subscription. Therefore, when more consumers embrace the new music model, Spotify will triple the music industry’s revenue potential.
Until that happens, it is essential for Spotify to maintain a good relationship with its artists. Spotify is wise to already provide artists with free data about where to look for their active fans to host a concert, how many people are streaming each song, etc. It can use the same data to aid artists’ marketing campaigns. Knowing the audience for each song, artists can tailor their performance content based on location, as well as decide where is it best to rent advertising spots and what sentiment to aim for.
Labels are becoming an obsolete middleman in the payment chain, taking a lion’s share of all the royalties. Thus, in the long-run, Spotify could aim to become its own label. That will cut the wasted profits and allow Spotify to attract artists with a more generous share of royalties. It already has all the data necessary to maintain artists’ optimal concert schedule, suggest collaborations with relevant artists, help deliver artists’ work to its most likely audience, etc. That will cause a backlash from the label industry. However, considering Spotify is already under pressure to pay more royalties from CRB, and it has no alternative revenues to rely on like Apple or Amazon, Spotify is essentially forced to cut out any middleman it can.
Currently, Spotify can create a more explicit dialogue between the fans and the artists. There is a clear demand from fans to be in-tune with their favorite artists, as demonstrated by the popularity of Genius (already integrated in Spotify) — song lyrics explanation platform, and podcasts featuring the artists explaining their creative process(e.g. Song Explorer). Creating a story around the song is often as important as making the song itself. Spotify can create platform features that are conducive to sharing such stories (either by integrating existing platforms or by designing the features from scratch):
A simple upvote/downvote based Q/A tab can act like an easy way for artists to answer the most pressing questions from their fans, providing them with the comforting feeling of “being heard”. (Think r/AMA on reddit.)
A specialized feed of what the artist is working on, where they are performing, and how their collaboration is going. Perhaps, an integration with artist’s Twitter account would do the job, but it runs the risk of not being music-related.
Depending on how much demand there is and how resource heavy it ends up being, short recording session updates (i.e. “snapchats” of 15–45 seconds in length) can act as brief moments of intimacy between artists and their fans. Instagram integration might do the job to require less maintenance across various platforms for artists.
By providing more explicit data analytics tools, and by giving artists a chance to intimately connect with their audiences, Spotify can compensate for its lower pay rate and maintain a healthy relationship with artists, which eventually should flourish into artists seeing Spotify as not only a streaming platform, but a necessary all-in-one tool that can truly aid their career.
Final remarks.
statista
Spotify is a long way from completely securing its position in the marketplace. It is ahead of the game due to smart initial promotions, socially tailored design, ML focus, and free data services to artists. However, its competitors are aggressively investing into the music streaming business, willing to put up with paying higher royalties, and cutting corners by pre-installing their platforms on hardware (e.g. Amazon, Google, Apple) or even integrating with software (e.g. Apple). But if Spotify can maintain good relationships with its artists by offering more than just a streaming platform, and keep attracting more listeners with its authentic music discovery design, it will have an opportunity to reshape the music industry even further in the future.
",1751,Music,Denis Kazakov,https://towardsdatascience.com/in-this-article-i-provide-a-detailed-analysis-of-spotify-as-a-company-music-industry-direction-eeb945d7257c,"In this article I provide a detailed analysis of Spotify as a company (music industry direction, market competition, growth strategies) and as a product (feature analysis, overview of ML at Spotify, UI suggestions).
Even though Spotify is busy cleaning up its lawsuits and debts before its projected public offering, investors are valuing the music streaming giant at $16 billion.
Having 70 million out of 146.6 million total paying music subscribers worldwide, Spotify sits at owning just under 50% share of the music streaming market.
Public playlists account for a major part of music discovery on Spotify.
Such an enormous number is no surprise, considering the platform design promotes making playlists and keeping them public.
As users grow their collection of saved songs and tailored playlists, they are increasingly more likely to keep using Spotify because there is no feature to export user libraries outside of Spotify’s ecosystem.
By keeping listeners’ interests at heart, Spotify quickly got everyone comfortable with its music finding features such as Discover Weekly and artist/song radios.
Data-driven culture: Spotify almost immediately started pursuing the goal of getting to know music algorithmically, seeing it as the best way to develop its features.
While Apple Music was boasting its “hand-curated playlists”, Spotify was busy hiring as many Machine Learning/Data Science experts from the field as it could afford.
Many chuckle at Spotify’s witty campaigns (2016 – 2018) that often parallel important news (“Dear 3,749 people who streamed “It’s the End of the World As We Know It” the day of the Brexit vote, Hang in there.”), or light-heartedly ridicule social norms (“Exercise more conventionally than the 46 people who put “Slow Hands” on their running playlists.”).
If you like the song — right-click to start the song radio to find similar music.
To summarize, most of Spotify’s navigation features are just a wrapper for navigating the music map, only going to areas that are or are likely to be of interest to you.
Spotify is wise to already provide artists with free data about where to look for their active fans to host a concert, how many people are streaming each song, etc.
It already has all the data necessary to maintain artists’ optimal concert schedule, suggest collaborations with relevant artists, help deliver artists’ work to its most likely audience, etc.
There is a clear demand from fans to be in-tune with their favorite artists, as demonstrated by the popularity of Genius (already integrated in Spotify) — song lyrics explanation platform, and podcasts featuring the artists explaining their creative process(e.g. Song Explorer).
It is ahead of the game due to smart initial promotions, socially tailored design, ML focus, and free data services to artists.
However, its competitors are aggressively investing into the music streaming business, willing to put up with paying higher royalties, and cutting corners by pre-installing their platforms on hardware (e.g. Amazon, Google, Apple) or even integrating with software (e.g. Apple).
But if Spotify can maintain good relationships with its artists by offering more than just a streaming platform, and keep attracting more listeners with its authentic music discovery design, it will have an opportunity to reshape the music industry even further in the future."
RL — Model-based Reinforcement Learning,"In reinforcement learning RL, we maximize the rewards for our actions. By simply looking at the equation below, rewards depend on the…","RL — Model-based Reinforcement Learning
Photo by Jonny Caspari
In reinforcement learning RL, we maximize the rewards for our actions. By simply looking at the equation below, rewards depend on the policy and the system dynamics (model).

In Model-free RL, we ignore the model. To accomplish this, we depend on sampling and observation heavily so we don’t need to know the inner working of the system. In Model-based RL, if we can define a cost function ourselves, we can calculate the optimal actions using the model directly. So basically, RL can be roughly divided into Model-free and Model-based methods. In this article, we will discuss how to establish a model and use it to make the best decisions.
Terms
Model-based RL has a strong influence from the control theory and often explained in terms from different disciplines. To get the concepts when explained in different ways, let’s go through some of the terms that refer to similar ideas.
In reinforcement learning, we find an optimal policy to decide actions. In control theory, we optimize a controller.

Control is just another term for action in RL. An action is often written as a or u with states as s or x. The controller uses the model (the system dynamics) to decide the optimal controls for a trajectory expressed as a sequence of states and controls.

In model-based RL, we optimize the trajectory for the least cost instead of the maximum rewards.
Model-free RL v.s. Model-based RL
As mentioned before, Model-free RL ignores the model and care less about the inner working. We fall back to sampling to estimate rewards.

We use Policy Gradients, Value Learning or other Model-free RL to find a policy that takes the best actions for maximum rewards.

On the contrary, Model-based RL focuses on the model.

With a cost function, we find an optimal trajectory with the lowest cost.


Known models
In many games, like GO, the rule of the game is the model.
AlphaGO
Sometimes, it is the law of Physics which can be modeled easily. We even build simulators for it.
Source: Vehicle dynamics model & Kinematic Bicycle Model
Mathematically, the model predicts the next state.

Besides using an explicit model with rules or equations, we can represent it with a generic model, like the Gaussian Process, Gaussian Mixture Model (GMM) or Deep networks. To fit those models, we run a controller to collect sample trajectories and train them with supervised learning.
Motivation
Model-based RL has a strong advantage of being sample efficient. For example, if it is appropriate to approximate the dynamics as locally linear, it will take much fewer samples to learn the model. Once the model and the cost function is known, we can plan the optimal controls without further sampling. As shown below, On-policy Policy Gradient methods can take 10M training iterations while Model-based RL is in the range of hundreds. To train a physical robot for a simple task, a Model-based method may take about 20 minutes while a Policy Gradient method may take weeks. However, such an advantage diminishes if the sampling cost is low. In particular. the optimization methods in the Model-based methods are more complex than Model-free methods in a few orders of magnitude. If sampling can be done in a computer simulation, Model-free methods could finish faster. Also, to simplify the computation, Model-based methods have more assumptions and approximations and therefore, may limit themselves to specific types of tasks.

Learn the model
In Model-based RL, the model may be known or can be learned. We run a base policy, like a random or any educated policy, and observe the trajectory. Then, we fit a model using the sampled data.
Source
In step 2, we use supervised learning to train a model to minimize the least square error from the sampled data. In step 3, we use any trajectory optimization method, like iLQR, to calculate the optimal trajectory using the model and a cost function. The cost function can measure how far we are from the target location and the amount of effort spent.
Learn the model iteratively
However, the last solution is vulnerable to drifting. Tiny errors accumulate fast along the trajectory. The search space is too big for any base policy to cover fully. We may land in areas where the model has not been learned yet. Without a proper model around these areas, we cannot plan the optimal controls.

To address that, instead of learning the model at the beginning, we continue to sample and fit the model as we move along the path.
Source
So we repeat step 2 and step 4 and continue collecting samples and fitting the model around the searched space.
MPC (Model Predictive Control)
Nevertheless, the previous method executes all planned actions before fitting the model again. We may be off-course too far already.

In MPC, we optimize the whole trajectory but we take the first action only. We observe and replan again. The replan gives us a chance to take corrective action after observed the current state again. For a stochastic model, this is particularly helpful.
Source
Backpropagate to policy
What is the difference between a controller and a policy? Are they both serve the same purpose of selecting actions based on states?
The controls produced by a controller is calculated using a model and a cost function through the trajectory optimization methods like iLQR.

However, we can also model a policy directly using a deep network or a Gaussian Process. For example, we can train the policy by backpropagating the cost function and the model.

Source
PILCO
Here is the PILCO algorithm of training a policy directly through backpropagation.
Simplified from source
However, consecutive states in a trajectory are highly correlated. Backpropagation functions with correlated values often lead to vanishing or exploding gradients. So its promise is limited.

Global Model
What kind of models can we use to represent the system dynamics?
Gaussian Process
One possibility is the Gaussian Process. The intuition is simple. If two inputs are similar, their output should be similar too. For two data points, if one is closer to a known training data, its prediction is more certain.

Say, we sampled two data points x1 and x2 with observed values f(x1)=150 and f(x2)=200 respectively. The plot above shows the possible values for f(x) within one standard deviation. The output of other data points, like f1 and f2, can be modeled as a Gaussian Distribution in the following form.


where 175 is the mean of f. For details on how to calculate K, please refer to here. As another example, the right figure below collects 5 data points and plot the mean prediction with the shaded area represents values within one standard deviation.
Source: Wikipedia
Here is the algorithm of the policy search using a Gaussian Process (GP) model:
Source
Gaussian Mixture Model GMM
Another possibility is the GMM. Gaussian Mixture Model is a mixture of K Gaussian distributions. We assume the model has k most likely outcomes and we weight them according:


To identify those k Gaussian distributions, we use Expectation Maximization (EM) to cluster the sample data into k clusters (modes) each represented by a mean and a variance.

Deep network
Finally, we can use a deep network.
Global model
Previously, we model the dynamics with a global model. If the dynamic is complex, we need a more expressive model like the deep network. But it requires a lot of samples to train it. If we land in space which is not yet trained properly, the model can be erroneous and lead us to bad actions that destroy the progress of the training.
Local Model
Alternatively, we can adopt an on-demand approach in developing models locally when we need it.
Source
The local model is Gaussian distributed with linear dynamics.

The parameters for the linear dynamics is computed as:

Controller
Modified from source
We run the controller p on the robot and observe the trajectory. With the collected samples, we can fit the model locally by estimating the derivative using samples.

However, the potential error increases as we move away from the trajectory where the local model is developed. Hence, we add a constraint to make sure the trajectory optimization is done within a trust region. Outside this region, we will not trust the optimization result.

This trust region is measured by the KL-divergence which measures the difference between the new controller and the old controller. If the trajectory is too different from the sampled one, we may get too aggressive and the calculated value can be too far away from the real value.
Image source
So what controller should we use? It will be too boring if the controller explores the same action over and over again for the same state during training. The repeating samples provide no new information to make the model better. Hence, we use a linear Gaussian controller to explore actions better. (linear dynamic with a Gaussian distributed output actions)

and Q is the cost function:

Σ is large when the cost Q is small. So we allow the exploration to go off-course more if the estimated cost is low. So how can we solve:

Before solving that, we need to check out some of the properties of the KL-divergence between the new and the old controller. This will involve some maths but it should be easy to follow or feel free to browse through them quickly.
KL-divergence
We want to establish:

First, we expand the distribution of the trajectory using the new and the old controller:

(Note: both trajectories have same initial and final state.)
Let’s compute the log term below first which is part of the KL-divergence definition:
Source
KL-divergence is defined as:

Therefore,
Source
Dual Gradient Descent
Next, we need optimization methods that work with constraints below:

DGD is our choice. Let’s have a quick summary here. For those want more details, you can read DGD later. First, we need to find its Lagrangian 𝓛 and the Lagrange dual function g which is defined as:

Then the optimal x* of our objective can be solved by:
Source
Intuitively, we start with some random value of λ and find the corresponding optimal values of 𝓛 using an optimization method. In our context, it is often a trajectory optimization method like LQR. g is actually the lower bound of our objective. So we find the optimal value of g by updating λ iteratively using the gradient ascent and the corresponding optimal 𝓛.
Model-based RL with local model
To solve our objective, we are going to reuse LQR. Our objective is:

LQR is quite complicated but for this context, you just need to know LQR is a trajectory optimization method. You can read LQR later if you want more.
The Lagrangian for our objective is:

We divide the objective by λ and create a new surrogate cost function

Here is the objective with the Lagrangian:

As explained in a previous article, LQR solves:

With a Linear Gaussian controller,

LQR minimizes

This looks almost the same as our objective but using the original cost function. Hence, we can reuse LQR exactly but with a new surrogate cost function.
Optimize trajectory with a local model
This is the final algorithm:
Source
Planning
We can also use planning to generate simulated experience and use them to fit the value functions or the policy better.
Source
The difference between learning and planning is one from real experience generated by the environment and one from simulated experience by a model. We use the real experience to fit the value function. We build a model of the transition and sample experience from it. Later, we can fit the value function again with the sampled experience. This improves sample efficiency because sample data are reused and it produces a more accurate value for V.
Model-based RL using sample-based planning (source)
Dyna-Q Algorithm
Here is the Dyna-Q algorithm which used the sampled data and the model to fit the Q-value.
Source
Thoughts
Instead of fitting a policy or a value function, we develop a model to understand the inner workings better. Knowing how things work, we end up with fewer required samples which is the key selling point when the physical simulation is expensive. Here, we discuss the basic of the model learning. But I wish it is that simple. Model learning can be very hard and not generalize well for other tasks. Learning directly for raw images is difficult when information is all tangled. Can we deploy the solution that is economically feasible? Stay tuned for these questions for more in-depth Model-based RL methods in later articles.
Credit and references
UC Berkeley Reinforcement Learning Class
UCL Course on RL
",2110,Machine Learning,Jonathan Hui,https://medium.com/s/story/rl-model-based-reinforcement-learning-3c2b6f0aa323,"In Model-based RL, if we can define a cost function ourselves, we can calculate the optimal actions using the model directly.
Model-based RL has a strong influence from the control theory and often explained in terms from different disciplines.
In reinforcement learning, we find an optimal policy to decide actions.
The controller uses the model (the system dynamics) to decide the optimal controls for a trajectory expressed as a sequence of states and controls.
In model-based RL, we optimize the trajectory for the least cost instead of the maximum rewards.
We use Policy Gradients, Value Learning or other Model-free RL to find a policy that takes the best actions for maximum rewards.
To fit those models, we run a controller to collect sample trajectories and train them with supervised learning.
Model-based RL has a strong advantage of being sample efficient.
For example, if it is appropriate to approximate the dynamics as locally linear, it will take much fewer samples to learn the model.
Once the model and the cost function is known, we can plan the optimal controls without further sampling.
As shown below, On-policy Policy Gradient methods can take 10M training iterations while Model-based RL is in the range of hundreds.
To train a physical robot for a simple task, a Model-based method may take about 20 minutes while a Policy Gradient method may take weeks.
If sampling can be done in a computer simulation, Model-free methods could finish faster.
Also, to simplify the computation, Model-based methods have more assumptions and approximations and therefore, may limit themselves to specific types of tasks.
In Model-based RL, the model may be known or can be learned.
Then, we fit a model using the sampled data.
In step 2, we use supervised learning to train a model to minimize the least square error from the sampled data.
In step 3, we use any trajectory optimization method, like iLQR, to calculate the optimal trajectory using the model and a cost function.
Without a proper model around these areas, we cannot plan the optimal controls.
Nevertheless, the previous method executes all planned actions before fitting the model again.
The controls produced by a controller is calculated using a model and a cost function through the trajectory optimization methods like iLQR.
However, we can also model a policy directly using a deep network or a Gaussian Process.
For example, we can train the policy by backpropagating the cost function and the model.
The output of other data points, like f1 and f2, can be modeled as a Gaussian Distribution in the following form.
Here is the algorithm of the policy search using a Gaussian Process (GP) model:
To identify those k Gaussian distributions, we use Expectation Maximization (EM) to cluster the sample data into k clusters (modes) each represented by a mean and a variance.
If the dynamic is complex, we need a more expressive model like the deep network.
The local model is Gaussian distributed with linear dynamics.
Hence, we use a linear Gaussian controller to explore actions better.
(linear dynamic with a Gaussian distributed output actions)
In our context, it is often a trajectory optimization method like LQR.
Optimize trajectory with a local model
We can also use planning to generate simulated experience and use them to fit the value functions or the policy better.
The difference between learning and planning is one from real experience generated by the environment and one from simulated experience by a model.
We use the real experience to fit the value function.
Later, we can fit the value function again with the sampled experience.
Model-based RL using sample-based planning (source)
Here is the Dyna-Q algorithm which used the sampled data and the model to fit the Q-value.
Instead of fitting a policy or a value function, we develop a model to understand the inner workings better."
Reflecting on F# in 2017,Some words about F# from the PM at Microsoft who does F# stuff.,"Reflecting on F# in 2017
I’m really excited to take part in this year’s FsAdvent. Thank you, Sergey, for all of the work you do with this and F# Weekly.
Three disclaimers:
I’m not a creative person, especially when it comes to naming things, so I’m stealing Stephen Diehl’s title for this post.
Although I work for Microsoft, I’m not writing this post on Microsoft’s behalf. All views things expressed here are my own views.
This is a long post split into three sections. I don’t have a TL;DR.
2017 has been a very exciting year for F#.
To begin, F# has grown to be bigger than ever, at least as far as we can measure, through product telemetry, twitter activity, GitHub activity, and F# Software Foundation activity.
Active unique users of F# we can measure are in the tens of thousands.
Measured unique users of Visual Studio Code with Ionide increased by over 50% this year, to become far larger than ever.
Measured unique users of Visual Studio who use F# increased by over 20% since last year to be larger than ever, despite quality issues earlier in the year that we believe have inhibited growth.
Much of the measured growth coincides with the release of .NET Core 2.0, which has shown significant interest in the F# community.
Telemetry is a complicated topic, and we do not try to account for existing users who are using F# in environments without telemetry, so it’s never perfect. Actual usage of F# in the world is strictly higher than what we can measure.
But numbers and metrics are limited, because they tell only a small part of the story. I’ll attempt to summarize some of the major things that happened for F# this year.
F# 4.1 was released, most notably as the first version of the language with .NET Core support, and has had multiple updates since its initial release.
Visual Studio 2017 and its F# tooling shipped five major updates, including support for .NET Core and .NET Standard projects.
Incredible members of the F# community, such as Vasily Kirichenko, Saul Rennison, Anh-Dung Phan, Steffen Forkmann, and others made significant improvements to our Visual Studio 2017 F# tooling including adding a dizzying array of features.
F# is now installed into Visual Studio 2017 by default if you are installing .NET Core.
F# tooling in Visual Studio 2017 now has a nightly release channel.
Visual Studio for Mac launched, with F# support in-box, and it has continually improved throughout the year.
Azure Functions now supports F#, with even better things coming soon.
Azure Notebooks now supports F#.
FSharp.Core is now the official package for the F# core library.
FSharp.Compiler.Tools is now the official place where the F# Compiler SDK is deployed, and is the recommended way to attain the SDK
F# is now in-band with the .NET Core SDK.
We expanded our team and hired Will at the end of the year!
The F# language suggestions and F# RFC repositories are now the official place for F# language and tooling evolution.
Ionide, a plugin suite that turns Visual Studio Code into a fully-featured F# IDE, shipped at least 100 releases (or 200; jeez, it’s so many).
Paket, the awesome NuGet client used in many F# projects, seemingly shipped billions of releases this year.
JetBrains Rider shipped with excellent and ever-improving F# support.
Much of the F# OSS ecosystem has migrated to .NET Standard and .NET Core (which represents an incredible amount of work on the part of the maintainers).
Suave and Giraffe emerged as two dominant libraries to use when writing web services on .NET Core, with Freya as a brilliant alternative that everyone should try out as well.
Fable rapidly evolved from an interesting project with potential into an impressive and fully-fledged alternative for JavaScript in the browser, allowing you to write Full-Stack F# applications.
The F# Software Foundation(FSSF) has continued its steady growth and become one of the best organizations to be a part of if you’re just beginning with F#.
The FSSF launched its diversity program and had another strong round of its free mentorship program.
OpenFSharp was created and sold out (quickly and despite low-key advertisement), which gets me all kinds of excited about F# in the U.S.
F# had a notable presence at //Build 2017, NDC conferences, .NET Conf, and other Functional Programming conferences.
Wew, that’s a lot. I probably missed some things that matter to people, so please let me know if you feel I should list something.
If there’s a single thing I feel most when looking at the above list, it’s pride. Not just in myself or my immediate colleagues, but in the members of the F# community who have done so many incredible things across such a wide spectrum of the entire F# ecosystem. Every single person involved in the above items should feel proud of themselves and their accomplishments. I’m American, and thus prone to superlatives, but you are all rock stars and I’m humbled to work with you all.
I’d also like to mention a few of the things that matter to me on a more personal level:
I was one honored to be one of the Community Heros announced at OpenFSharp:

I got to stay up until 3:30 a.m. after OpenFSharp talking all kinds of craziness over beers with Rich Minerich, Dmitry Morozov, Gien Verschatse, Mathias Brandewinder, Chet Husk, Cezary Wojcik, Marnee Dearman, and Marcus Griep. Mathias was kind enough to provide his apartment as the venue for these shenanigans.
I got to hang out with Krzysztof Cieślak, Tomas Petricek, Evelina Gabasova, Marcus Griep, Riccardo Terrell, Gien Verschatse, Mark Gray, Paulmichael Blasucci, Enrico Sada, Alfonso Garcia-Caro, Jérémie Chassaing, and others in Cambridge and London. Beer, food, taking over a bar, cavorting around each city, enjoying the sunshine, talking smack about Golang, and just about everything else happened. So much fun.
I got to witness the Pink Pezi himself, Ross McKinlay, steal Eirik Tsarpalis’ badge at F# eXchange, and then walk around calling himself a Greek programmer named Eirik. Ross, will there be an EirikTsarpalisProvider Type Provider library I can use some day? Will it only provide types named “Ross”, or will it also provide types named “Eirik”?
I was one of the honored few to be cow-herded, Don Syme style, by Andrea Magnorsky to leave the F# eXchange venue. Hup hup!
I got to have Andrea Magnorsky pull me aside at F# eXchange for a wonderful and serious conversation about the sorts of things the community needed to hear from me at the conference and beyond. Oh, and she gave me a FunctionalCat sticker for my laptop, which I proudly display to this day!
I got to meet Scott Wlaschin, whose prolific blogging taught me why F# is so great when I was still a college student in 2014.
I got to watch Alfonso Garcia Caro launch Fable 1.0, code-named Narumi, and be honored to have him present not once, but twice on Channel9 about Fable.
I got to hang out, multiple times, with some of the wonderful folks form G-Research while in London.
I got to engage, daily, with some of the amazing OSS contributors to F# and its ecosystem.
I got to have Don Syme think numerous suggestions of mine were good ideas. For someone who only got a college degree in June 2015, that’s a pretty big deal me!
I think it’s rare to find a job in my field that is this rewarding. There is a lot of stress involved, and the problems that need solving and issues that need addressing are never simple. But it’s worth it every time.
And now, for something entirely different.
I don’t think I’ve ever clarified, for anyone in the F# community, what my precise role is at Microsoft. My title is currently “Program Manager II”, but that doesn’t really mean much. I’ll start with a list of things I’m not:
Software Engineer. I’m a Program Manager! But I do sometimes write code for the product.
Project manager. Nope nope nope nope nope. I’m terrible at process.
Manager. Nobody works for me.
Program. I’m a human, not a program.
Evangelist. Nope. Although I sometimes do things that count as evangelism.
Salesperson. You really don’t want me selling things to anyone.
Data scientist. Not even close. But I sometimes use querying languages to muck about in the mud of telemetry systems.
Product Manager. This is the closest, but it’s not the same as what other product managers do at other tech companies. I have some KPIs that I track, but I keep them low in number and tend not to focus too much on them. I’m actually quite fed up with the silly focus on marginally useful KPIs our industry has adopted, let alone the incorrect means that so many of them are derived and measured.
Every action I take is to make sure that F# is a pleasure to work with and moving in the correct strategic direction so it will grow and flourish for years to come. That means a few things:
There’s a good chance that for any given product or topic that is related to F# in some way, I have some context. The depth of that context may vary wildly.
I help make a lot strategic and tactical decisions about what Microsoft does for F# with the resources it has.
I spend a significant amount of time interacting with other teams at Microsoft. Sometimes, these interactions are merely giving someone context that they asked for. Other times, it’s a fully-fledged negotiation process, where we come to an understanding about who is responsible for the delivery and maintenance of assets under certain circumstances.
I am responsible for some things that are not directly related to F#. Right now, this includes certain parts of Visual Studio 2017 setup and our organization’s efforts around Azure growth. The most recent, tangible result of this work has been to have F# be installed by default with Visual Studio whenever you install .NET Core.
I am always advocating on behalf of F# developers when I am involved in discussions and decisions regarding .NET, Visual Studio, and Azure. My default position on things is if they would make sense to F# users.
I am always learning more about our codebase, and I always strive to understand the moving parts at a high level so that I can describe them to anyone. The codebase is vast and dense, so there are things I am utterly clueless about. My hope is that by the end of 2018, I can contribute reliably to some of the “deeper” areas there.
When I have the time, I evangelize F#, the tooling that Microsoft delivers for F#, and various F# open source projects. I hope to expand this effort moving forward.
Finally, I am ultimately on the hook for the success of F#, Visual Studio users who use F#, Visual Studio for Mac users who use F#, Visual Studio Code and Ionide users who use F#, and Azure users who use F#. Although much of what I just listed are not my direct responsibility, my review at the end of the year depends on my involvement beyond the Visual F# tools which ship as a part of Visual Studio.
This point is key: my role eclipses just the Visual F# tools which ship in Visual Studio. F# is so much more than just the assets Microsoft delivers in Visual Studio, and limiting my actions to just those would do a disservice to F# users. The holistic picture of the entire “F# experience” is what I care about and try to improve.
And now, for something entirely different.
I’m very excited about what 2018 will bring for F#. So many things in the F# ecosystem that were in flux in 2017 have calmed down and have been set up for success. These are some of my assorted thoughts on the coming year:
I believe that F# will achieve the vision of being the best-tooled functional programming language on the planet, with excellent support in any IDE you like, excellent support in Azure, and the ability to use the tooling you want in the entire ALM experience.
I believe that the F# community, especially through the F# Software Foundation, will continue to grow and become an active home for newcomers to the language.
I believe that Fable will garner even larger interest in the JavaScript community than it has this year.
I believe that F# on the server via .NET Core will be a compelling option for many people and organizations.
I believe that F# will have an even stronger conference presence than 2017.
I believe that the increase in F# OSS activity, .NET Core, and Fable will drive F# to new heights in terms of usage and activity.
I’ll also share some of the things that I personally want to focus on in terms of what my team delivers:
Finalizing Type Providers as .NET Standard 2.0 components.
Delivering F# Interactive support for .NET Core, as a .NET Core application, accessible via fsi on the command line, installed via a .NET CLI global tool.
Package management via #r “packagename"" in F# Interactive, running on .NET Core.
Completing the .NET Core SDK-based project support in Visual Studio 2017, including multi-targeting and file ordering, templates for Azure Functions and ASP.NET Core, and incorporating relevant templates from the community.
Completing detailed performance analysis of the F# Compiler Service, with engineering work to address the findings.
Continuing the F# 4.1 work towards better error messages.
Working towards shipping a new language version.
New and improved features, such as the ability to Go to Definition on types defined in metadata, or revamped autocompletion.
I view the above as a fairly realistic set of things that you’ll all enjoy over the coming year. I’m quite hopeful that every one of these things will be something to brag about by next Christmas.
2018 is going to be an incredible year for F#. Until then!
",2351,Fsharp,Phillip Carter,https://hackernoon.com/reflecting-on-f-in-2017-5ac67fb138ff,"I’m not a creative person, especially when it comes to naming things, so I’m stealing Stephen Diehl’s title for this post.
Although I work for Microsoft, I’m not writing this post on Microsoft’s behalf.
Measured unique users of Visual Studio Code with Ionide increased by over 50% this year, to become far larger than ever.
Measured unique users of Visual Studio who use F# increased by over 20% since last year to be larger than ever, despite quality issues earlier in the year that we believe have inhibited growth.
Much of the measured growth coincides with the release of .NET Core 2.0, which has shown significant interest in the F# community.
F# 4.1 was released, most notably as the first version of the language with .NET Core support, and has had multiple updates since its initial release.
Visual Studio 2017 and its F# tooling shipped five major updates, including support for .NET Core and .NET Standard projects.
Incredible members of the F# community, such as Vasily Kirichenko, Saul Rennison, Anh-Dung Phan, Steffen Forkmann, and others made significant improvements to our Visual Studio 2017 F# tooling including adding a dizzying array of features.
F# is now installed into Visual Studio 2017 by default if you are installing .NET Core.
Visual Studio for Mac launched, with F# support in-box, and it has continually improved throughout the year.
Azure Functions now supports F#, with even better things coming soon.
Ionide, a plugin suite that turns Visual Studio Code into a fully-featured F# IDE, shipped at least 100 releases (or 200; jeez, it’s so many).
Much of the F# OSS ecosystem has migrated to .NET Standard and .NET Core (which represents an incredible amount of work on the part of the maintainers).
Suave and Giraffe emerged as two dominant libraries to use when writing web services on .NET Core, with Freya as a brilliant alternative that everyone should try out as well.
OpenFSharp was created and sold out (quickly and despite low-key advertisement), which gets me all kinds of excited about F# in the U.S. F# had a notable presence at //Build 2017, NDC conferences, .NET Conf, and other Functional Programming conferences.
If there’s a single thing I feel most when looking at the above list, it’s pride.
Not just in myself or my immediate colleagues, but in the members of the F# community who have done so many incredible things across such a wide spectrum of the entire F# ecosystem.
I got to have Andrea Magnorsky pull me aside at F# eXchange for a wonderful and serious conversation about the sorts of things the community needed to hear from me at the conference and beyond.
I’ll start with a list of things I’m not:
Every action I take is to make sure that F# is a pleasure to work with and moving in the correct strategic direction so it will grow and flourish for years to come.
The most recent, tangible result of this work has been to have F# be installed by default with Visual Studio whenever you install .NET Core.
I am always advocating on behalf of F# developers when I am involved in discussions and decisions regarding .NET, Visual Studio, and Azure.
Although much of what I just listed are not my direct responsibility, my review at the end of the year depends on my involvement beyond the Visual F# tools which ship as a part of Visual Studio.
F# is so much more than just the assets Microsoft delivers in Visual Studio, and limiting my actions to just those would do a disservice to F# users.
I believe that the increase in F# OSS activity, .NET Core, and Fable will drive F# to new heights in terms of usage and activity.
I’ll also share some of the things that I personally want to focus on in terms of what my team delivers:
Package management via #r “packagename"" in F# Interactive, running on .NET Core.
Completing the .NET Core SDK-based project support in Visual Studio 2017, including multi-targeting and file ordering, templates for Azure Functions and ASP.NET Core, and incorporating relevant templates from the community."
K-Nearest Neighbors,"In my previous article, I went over the Bayes’ classifier, which is proved to be optimal. But the problem here is that statistical approach…","K-Nearest Neighbors
In my previous article, I went over the Bayes’ classifier, which is proved to be optimal. But the problem here is that statistical approach requires a lot of extra work. So one option is to consider another classifier, which does not have parameters.
Parametrized classifiers like MLE classifier or Bayes’ classifier require the decision on probability models, which results in extra work in the beginning; thus, it’s error-prone. But with that underlying assumption of the probability model, one can get so much information about the data such as if a given point is an outlier or not.

Here, we will take a look at K-Nearest Neighbors classifier, which is an unparametrized classifier.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
In the figure above, we are trying to figure out if the white dot belongs to the red or blue. We can simply look at its nearest neighbors and predict what classification it belongs to. That is K-nearest neighbors classifier. K differs depending on how many data you want to look at before making a decision.
But the problem is how are we going to calculate the distance between data points?
Euclidean distance:
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
2. Other normed distances:
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
p = 2: euclidean distance
p = 1: manhattan distance (no diagonal distance is calculated; only vertical and horizontal distance is calculated)
p = ∞: Only maximum distance is of interest
p = 0: count the number of non-zero distance e.g. if there are two dimensions and if there is a difference in dimension 1 and there is no difference in dimension 2, the count is 1.
Instead of calculating distance, can we calculate something else?
Yes, we can calculate similarities between data.
Typically, it is some sort of a relation from the distance calculation like this:
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
But instead of that, we can use the angles between the data using cosine similarity:
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
Then, there is no distance calculated between data points at all. This is what’s actually used for recommendation for similar videos in Netflix.
K-NN’s Optimality
For fixed k, as and n → ∞, k-NN classifier error converges to no more than twice Bayes classifier error.
If k → ∞, k/n → 0, and n → ∞, k-NN classifier converges to Bayes classifier. Therefore, if Bayes’ is optimal, k-NN is optimal.
But there is a drawback of K-NN that it is sensitive to noise in data, which would result in noise in my prediction as well. This problem can be solved by taking majority of k-nearest neighbors. Also, there is no best of way to measure approximate closeness. More importantly, all the training data need to be kept in order to give relation between the given data and training data.
Another drawback is it takes time to find the k-nearest neighbors. The running time it takes is O(nd) where n is the number of training data and d is the dimension. In order to make this problem simpler, how about we turn this into a decision question: What are the k-nearest neighbors (optimization problem) -> Does an element x exist in the nearest approximation (decision problem). In order to answer the decision problem, a naive approach would result in O(n).
In order to do it quickly, a k-d tree can be implemented. The k-d tree is “a binary tree in which every node is a k-dimensional point”. Areas are partitioned and represented in a tree form to implement the binary search. If the points are sorted, the search time significantly decreases from O(n) to O(log n).
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
Drawback of K-d tree
It doesn’t give the most accurate nearest neighbor.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
For example, if I am looking for the nearest neighbor of the red dot from the above diagram, instead of giving me the blue dot, it will give the green dot on the left hand side of the red dot, because the binary search implemented with k-d tree will result in the T1,1 area and will look for the nearest neighbor in that subregion. However, this drawback doesn’t stop people from using k-d tree, because O(log n) running time is more beneficial over O(n) and preciseness can be sacrificed.
Another way to find the neighbor quickly: Binary search with median
The diagram below is a 2-dimensional example. Instead of looking for the middle partitioned area, we can look for the medean data point instead.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
On top of that, in order to accommodate to the 2-dimensional aspect, we can start with the dimension that has more data points than the other dimension. If the size of the data point matrix is m by n, and m > n, we can do binary partition in the m dimension. Then the next time, if m > n, we do the partition again in the m dimension. If m < n, then we move onto the partition in the n dimension
Metric learning
How to make measurement robust depending on the parameters? We can give weights to each parameter and add the weights when we calculate the distance. Then now the question becomes how can we find the optimal weights?
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
We can define a cost function for the algorithm to learn the weight on its own.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
But the problem is now we need to keep all the data around. So motivated by k-nearest neighbors implemented with k-d tree, there is another classifier, which is called decision tree.
Decision Tree
Decision trees in general are highly used in the industry. For instance, Amazon uses decision trees for their fraud order detection.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
With decision trees, we can choose the features and the thresholds that directly optimize for classification accuracy! The point is we want to select the feature and threshold that maximally reduces label uncertainty, rather than selecting arbitrary feature and splitting at the median.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
The drawback of decision tree
We can overfit the data, which means the model gets used to the training data too much so that the error rate goes up.
http://www.cs.columbia.edu/~verma/classes/ml/lec/lec1_intro_mle_bayes_naive_evaluation.pdf
In order to alleviate the issue, we stop splitting the tree in the testing phase if the error rate goes up.
",1022,Machine Learning,Suhyun Kim,https://medium.com/s/story/k-nearest-neighbors-e45c5193da9e,"Parametrized classifiers like MLE classifier or Bayes’ classifier require the decision on probability models, which results in extra work in the beginning; thus, it’s error-prone.
We can simply look at its nearest neighbors and predict what classification it belongs to.
K differs depending on how many data you want to look at before making a decision.
But the problem is how are we going to calculate the distance between data points?
Then, there is no distance calculated between data points at all.
This problem can be solved by taking majority of k-nearest neighbors.
Another drawback is it takes time to find the k-nearest neighbors.
The running time it takes is O(nd) where n is the number of training data and d is the dimension.
In order to make this problem simpler, how about we turn this into a decision question: What are the k-nearest neighbors (optimization problem) -> Does an element x exist in the nearest approximation (decision problem).
In order to answer the decision problem, a naive approach would result in O(n).
Areas are partitioned and represented in a tree form to implement the binary search.
For example, if I am looking for the nearest neighbor of the red dot from the above diagram, instead of giving me the blue dot, it will give the green dot on the left hand side of the red dot, because the binary search implemented with k-d tree will result in the T1,1 area and will look for the nearest neighbor in that subregion.
If the size of the data point matrix is m by n, and m > n, we can do binary partition in the m dimension.
So motivated by k-nearest neighbors implemented with k-d tree, there is another classifier, which is called decision tree.
The drawback of decision tree"
The symbiotic relationship between humans and artificial intelligence,An exploration on the human-computer relationship when computers outsmart us.,"The symbiotic relationship between humans and artificial intelligence
Often AGI, or ‘human-level AI’ is considered the holy grail of AI research. It’s even in the name, human level AI. Why are we so obsessed with getting to a human level? What’s the relation of AI to humans? And what’s our role in a symbiotic relationship between human and computer?
Robot Sophia and Einstein on stage at Web Summit
How to built a machine that can improve itself, not on one task, but on many tasks? Brain researchers and AI researchers alike note that the only model we currently have of anything close to AGI (artificial general AI, a.k.a. human-level AI) is the human brain. The way our brain is built, how each neuron has thousands of synapses, is a great source of inspiration as long as we lack better alternatives. Our brain for instance filters very effectively to allow us to have a lot of input from our surroundings (for instance sensory input), but compute these on the necessary speed with limited capacity. We are able to learn without enormous amounts of data available to us. Plus the brain is very flexible, especially compared to current AI systems, that are currently very narrow. It’s not for nothing that we measure artificial intelligence to our own intelligence. The Turing Test is the most literal form. Goertzel et al. introduced two new tests in “The Architecture of Human-Like General Intelligence”.
The coffee test, and the robot college test. Perhaps the most interesting variation on the Turing Test comes from Nilsson, the employment test. 
“To pass the employment test, AI programs must be able to perform the jobs ordinarily performed by humans. Progress toward human-level AI could then be measured by the fraction of these jobs that can be acceptably performed by machines.” The real challenge is to make a self-improving general purpose intelligence, not to built a non biological human.
A popular argument to diminish advancement in AI can be summarized by this quote from Peter Thiel.
“In 2012, one of their supercomputers made headlines when, after scanning 10 million thumbnails of YouTube videos, it learned to identify a cat with 75% accuracy. That seems impressive — until you remember that an average four-year-old can do it flawlessly. When a cheap laptop beats the smartest mathematicians at some tasks but even a supercomputer with 16,000 CPUs can’t beat a child at others, you can tell that humans and computers are not just more or less powerful than each other — they’re categorically different.” Peter Thiel — Zero to One.
Or in other words ‘yes, it’s impressive that computers can do that, but we will be able to do things better’. I think this is anthropomorphising intelligence. It’s also a matter of moving the goalposts. It’s not that long ago that we thought that computers would never beat us in chess and certainly not Go. If we will ever achieve general artificial intelligence is a debate itself. The vast majority of researchers seem to agree that it will be possible somewhere in time. Whether that is in 2030, 2050, 2070 or 2090 is very hard to predict. According to this survey, by Nick Bostrom in 2013 (and later replicated by the participants of the AI Safety conference in Puerto Rico in 2015) researchers concluded that in the most pessimistic estimation, (with 90% certainty) AGI will arrive by 2075 the latest. The median for 50% certainty was 2040. At the same time, AI researchers often have been wrong in the past, and seem to have a hard time making accurate predictions. It’s worth noting that a very small minority (2% on the Puerto Rico conference) thinks AGI will never happen. Next to that there is group of researchers that warns to be cautious with hyping AI technology. Gary Marcus wrote a great paper on the limitations of AI, now, and in the upcoming future. In 2015 Wait But Why wrote two excellent summarising blogs (part 1, part 2) about this topic, and I don’t think there is much to add. (Read also this piece on aeon.co.) In this post I would like to focus on the relationship between humans and computer when it occurs, and the road towards it. In my previous post I already argued that the road towards AI is just as interesting. Whether we will achieve super intelligence in the long run will make a great difference to our lives. However, even if we don’t achieve it, AI will impact our lives greatly in the upcoming decades. Most analyses, like the above seem to perceive our relationship with computers as static. But history shows us, this relationship has constantly changed so far, and is very unlikely to be static from now on. When AI capabilities change, it’s relationship to humans will too. If we look at the current differences, I think there are 4 reasons why humans still outperform computers on most tasks. Especially simple ones. It seems logical that in the short term we develop and enhance our symbiotic relationship, where computers and humans will strengthen each other. So let’s dive into the 4 biggest differences.
First, we humans have a very good understanding of our contextual environment. Empathy and contextual awareness are essential to human interaction, and we are born with a great intuition for both. Throughout our lives, we’ll further develop these skills greatly. This skill converts our intelligence into wisdom (at least occasionally). Currently, AI systems are notoriously bad in understanding the question behind the answer. Let alone the question behind the question. They would be even worse in the 5 why’s. Still, in its essence, ‘human traits’ like empathy, contextual awareness and wisdom are ‘just’ a set of rules. According to many studies (1 & 2 )like “the future of employment” your best shot of long-term employment is to train yourself for jobs that put a high premium on creativity and empathy. But what is creativity in the end? According to Dictionary.com creativity is the ability to transcend traditional ideas, rules, patterns, relationships, or the like, and to create meaningful new ideas, forms, methods, interpretations, etc. At first sight, this might sound exactly contrary to what algorithms try to do. They follow patterns. However, it doesn’t take that much imagination to let the algorithm explore the edges of the norm though. And with that creativity becomes a matter of degrees, how far do you stray from the normal? In the end, I don’t see a compelling reason why AI can’t learn creativity. I think the same can be said for other ‘human traits’. Empathy, social cohesion, debating, comedy, things that according to many reports (1, 2, 3) are the things we humans are good at. These human traits are very complex, and the set of rules is lengthy with many exceptions. But in the long run, I don’t see a really compelling reason why AI systems cannot translate our social norms, our preferences, and debating skills into algorithms to perform them for us. We already see the first examples of empathy, comedy, and creativity.

Secondly, we have and can process more relevant information in a shorter amount of time. The abundance of data that we are born with, and our ability to obtain much more data through our life is something unique to humans today. We, homo sapiens, have a 200.000 years head start on AI in processing data in the most effective way. Our sensory data is very effectively filtered through our brain. We store only a very tiny percentage of the data we process. It’s not just hard computational power, but also the input that matters. Similarly like a bat is able to locate itself with echolocation. The bat doesn’t have more brain power in general, but by having different sensory data, it trained itself to understand echolocation. The human brain is very complex, and when we look at the universal applications we use our brain for, there is no computer that comes near our skills. It’s estimated that a human brain comes pre-programmed with about 1.6GB of data as part of our DNA. Electrically we store about 10GB of data, and 100TB of chemically/biologically (numbers derived from Life 3.0). AI systems are usually fed with only a couple of datasets, and are learned to process it with a very specific narrow goal in mind. AI lacks the intuition and as computer scientist Knuth puts it
‘AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking.’ — Donald Knuth
However, many of the breakthroughs in deep learning usually stem from trying to learn the AI to develop an intuition. By stapling multiple algorithms together the AI learns to do things in a smarter way, instead of brute-forcing its way through the dataset. This also allows us to achieve more with fewer data.
Thirdly, our world is shaped towards human cognition. We externalised a big part of our intelligence to our society. We are smart because we are modules in a big world, whether it is financial or political. It’s comparing apples to oranges if you compare us as a whole, the hive mind, to one AI system alone. But this is a big factor in the differences on how we perform. AI systems usually stand alone. There also seems to be a limit on what we as a society can consume on intelligence. Even the smartest people with IQ’s of 130+ don’t seem to be much more successful throughout their lives over people with ‘just’ an IQ of 130. If others do not understand, and cannot consume that wisdom it is partly pointless. This is limiting the true potential of AI for now. AI can only advance as fast as humans can keep up with it. To find the balance between human and AI will definitely be an important part of developing AI towards an AGI. The human factor might be the one that limits AI development the most. And therefore it’s important that people with different backgrounds help AI systems and AI researchers understand how these systems work precisely. It’s likely that we have to recreate some fundamentals of society to fully embed AI’s full potential. The technical perspective is a lot easier to imagine compared to the societal changes that are probably required.
Fourthly, and perhaps the most important difference is conscientiousness. Can a computer become (or is it already) conscious? I want to sidestep the moral implications of consciousness right now. (It’s a fascinating topic, and I would encourage you to watch the Westworld series, and listen to this podcast between Sam Harris and Max Tegmark for starters.) According to Steven Pinker, a psychologist at Harvard, in the end, consciousness can be boiled down a mathematical structure and always has a physical correlate.
Westworld, a series about a fictional theme park for psychopaths that explores the boundaries of ethical behaviour towards robots.
“The philosophical problem of sentience or qualia or (sometimes called) the hard problem of consciousness I think might ultimately be a quirk of our own way of analyzing the world — that is, the mind reflecting on itself is naturally going to be puzzled by some aspects of itself.We know from neuroscience that there is no aspect of consciousness that does not have some physical correlate. There’s no ESP. There’s no life after death. There’s no mysterious action at a distance. It’s all information-processing and neurons. Why it should feel like something to me to be that network of neurons, I don’t think we have a satisfying answer to.“ — Steven Pinker
Steven Pinker on intelligence
Consciousness is not binary, it’s a matter of degree. There is a difference between the level of consciousness between, humans and other animals, adults and children, and even between adults. Consciousness is a structure of thoughts, or at a deeper level just neurons. All these things combined make me think that it’s not impossible to make a machine conscious. It will be a matter of degrees, but a computer doesn’t necessarily have to stop at the human level. We can train our brain to become more conscious, but there will be a different physical limit than that of a computer. I do believe that this is hardest of the four differences between humans and computers for computers to solve. Not in the last place because this is also the area where we understand the least of our own capabilities. Therefore I think this will also be the area where humans will contribute the most to the long-term symbiotic relationship between humans and computers.
In the long term, it’s unclear to me what the human role will be. Will we just be the monkey in the loop that slows progress down? Will evolution take care of that problem? And if so, how? Clearly, we don’t have answers to those questions yet. Our biggest strength over AI is our conscious experience. We can give meaning and purpose to what AI will invent for and with us. Computers are very task driven if they don’t have a purpose they don’t do anything. We are the ones that give it purpose. AI might be able to simulate empathy (and potentially consciousness), but it doesn’t have a clear path to experience it as well. As Brynjolfsson noted, the saddest outcome would be if we invent AI systems as a way to kill each other and or make each other miserable. If we don’t derive pleasure and meaning out of our world, then wouldn’t it be just a colossal waste of space?
Are we able to design our own future? We currently have already big systems in place, like the financial market, and democracy that we barely seem able to control. We have a hard time aligning our values with these systems. Partly because we, humans, differ in what we want from those systems. But it also stems from the fact that we don’t fully comprehend the finer details of its inner workings. With AI this challenge will be even bigger. By design, AI will amplify human behavior. At least today, it’s mostly trained on data created by humans (manually or mechanically). It trains itself by looking at our current world and tries to mimic it. While the technology might be neutral, the designers and users are never neutral. That intuition that sets humans apart from computers, relies on biasses and subjectivity to work. If we don’t deliberately design AI to correct for human bias and fix our worst characteristics it will amplify the worst that humans brought to this world. It seems to me that once we are further down the road we will further deviate from this path of mimicking human society. As the Alpha Go Zero experiment with Go showed, the human brain is not the most efficient form of cognition. So as an inspiration it’s fine, but I don’t think it should or will be the final goal. And yes, humans have terrible traits as well. We don’t want AI’s to manipulate, murder and be self-centered. Therefore, it will be increasingly important to think about the purpose of our innovations. If we make the goal of improving the quality of human life as a baseline, and the overall goal of AI innovation, we can measure innovation towards that goal. ‘Yes, your automated solution sounds cool, but does it really improve human life?’ It seems to me that we currently adopted a more technological deterministic approach. Where the technical capabilities drive innovation, instead of purpose. We need to focus on shared positive visions for the future as a basis to start the collaboration.
Over the long term, the future is decided by optimists. — Kevin Kelly
In my next blog, I will go deeper into the symbiotic relationship between human and computer. What is the current relationship, and what does the foreseeable future look like?
",2636,Artificial Intelligence,Han Rusman,https://medium.com/s/story/the-symbiotic-relationship-between-humans-and-artificial-intelligence-66c0add19081,"The symbiotic relationship between humans and artificial intelligence
It’s even in the name, human level AI.
Progress toward human-level AI could then be measured by the fraction of these jobs that can be acceptably performed by machines.” The real challenge is to make a self-improving general purpose intelligence, not to built a non biological human.
When a cheap laptop beats the smartest mathematicians at some tasks but even a supercomputer with 16,000 CPUs can’t beat a child at others, you can tell that humans and computers are not just more or less powerful than each other — they’re categorically different.” Peter Thiel — Zero to One. Or in other words ‘yes, it’s impressive that computers can do that, but we will be able to do things better’.
It’s not that long ago that we thought that computers would never beat us in chess and certainly not Go. If we will ever achieve general artificial intelligence is a debate itself.
(Read also this piece on aeon.co.) In this post I would like to focus on the relationship between humans and computer when it occurs, and the road towards it.
Whether we will achieve super intelligence in the long run will make a great difference to our lives.
When AI capabilities change, it’s relationship to humans will too.
If we look at the current differences, I think there are 4 reasons why humans still outperform computers on most tasks.
It seems logical that in the short term we develop and enhance our symbiotic relationship, where computers and humans will strengthen each other.
Empathy and contextual awareness are essential to human interaction, and we are born with a great intuition for both.
According to many studies (1 & 2 )like “the future of employment” your best shot of long-term employment is to train yourself for jobs that put a high premium on creativity and empathy.
According to Dictionary.com creativity is the ability to transcend traditional ideas, rules, patterns, relationships, or the like, and to create meaningful new ideas, forms, methods, interpretations, etc.
But in the long run, I don’t see a really compelling reason why AI systems cannot translate our social norms, our preferences, and debating skills into algorithms to perform them for us.
It’s not just hard computational power, but also the input that matters.
The bat doesn’t have more brain power in general, but by having different sensory data, it trained itself to understand echolocation.
It’s estimated that a human brain comes pre-programmed with about 1.6GB of data as part of our DNA.
The human factor might be the one that limits AI development the most.
And therefore it’s important that people with different backgrounds help AI systems and AI researchers understand how these systems work precisely.
(It’s a fascinating topic, and I would encourage you to watch the Westworld series, and listen to this podcast between Sam Harris and Max Tegmark for starters.) According to Steven Pinker, a psychologist at Harvard, in the end, consciousness can be boiled down a mathematical structure and always has a physical correlate.
Why it should feel like something to me to be that network of neurons, I don’t think we have a satisfying answer to.“ — Steven Pinker
There is a difference between the level of consciousness between, humans and other animals, adults and children, and even between adults.
All these things combined make me think that it’s not impossible to make a machine conscious.
It will be a matter of degrees, but a computer doesn’t necessarily have to stop at the human level.
We can train our brain to become more conscious, but there will be a different physical limit than that of a computer.
Therefore I think this will also be the area where humans will contribute the most to the long-term symbiotic relationship between humans and computers.
In the long term, it’s unclear to me what the human role will be.
Partly because we, humans, differ in what we want from those systems.
At least today, it’s mostly trained on data created by humans (manually or mechanically).
So as an inspiration it’s fine, but I don’t think it should or will be the final goal.
In my next blog, I will go deeper into the symbiotic relationship between human and computer.
What is the current relationship, and what does the foreseeable future look like?"
Machine learning fundamentals (II): Neural networks,In my previous post I outlined how machine learning works by demonstrating the central role that cost functions and gradient descent play…,"
Machine learning fundamentals (II): Neural networks
In my previous post I outlined how machine learning works by demonstrating the central role that cost functions and gradient descent play in the learning process. This post builds on these concepts by exploring how neural networks and deep learning work. This post is light on explanation and heavy on code. The reason for this is that I cannot think of any way to elucidate the internal workings of a neural network more clearly that the incredible videos put together by three blue one brown — see the full playlist here.
These videos show how neural networks can be fed raw data — such as images of digits — and can output labels for these images with amazing accuracy. The videos highlight the underlying mathematics of neural networks in a very accessible way, meaning even those without a heavy math background can begin to understand what goes on underneath the hood in deep learning.

This post is intended as a “code-along” supplement to these videos (full Tensorflow and Keras scripts are available at the end of the post). The purpose is to demonstrate how a neural network can be defined and executed in Tensorflow such that it can identify digits such as those shown above.
TensorFlow (for those who do not know) is Google’s deep learning library and while it is quite low-level (I typically use the higher-level Keras library for my deep learning projects), I think it is a great way to learn. This is simply because, although it does incredible amounts of magical things behind the scenes, it requires you (yes you!) to explicitly define the architecture of the NN. In doing so you’ll gain a better understanding of how these networks work.
Neural Networks
Neural networks are mathematical and computational abstractions of biological processes that take place in the brain. Specifically, they loosely mimic the “firing” of interconnected neutrons in response to stimuli — such as new incoming information. I don’t find biological analogies particularly helpful for understanding neural networks, so I won’t continue down this path.
Neural networks work by computing weighted summations of input vectors that are then passed through non-linear activation functions, thereby creating a mapping from input to output via a non-linear transformation layer. The weights (represented by neutrons) in the transformation, or hidden, layer are iteratively adjusted such that they come to represent relationships in the data that map inputs to outputs.
Defining Layers and activations
In the first step we define the architecture of our network. We will create a four layer network comprised of one input layer, two hidden layers and one output layer. Note how the output from one layer is the input to the next. This model is quite simple as far as neural nets go, it is comprised of dense or fully connected layers, but is still quite powerful.

The input layer — also sometimes referred to as the visible layer — is the layer of the model that represents the data in its raw form. For example, for the digit classification task the visible layer is represented by numbers corresponding to pixel values.

In TensorFlow (all code is below) we need to create a placeholder variable to represent this input data, we will also create a placeholder variable for the correct label corresponding to each input. This effectively sets up the training data — the X values and y labels we will use to train the neural network.
The hidden layer(s) enable the neural network to create new representations of the input data that the model uses to learn complex and abstract relationships between the data and the labels. Each hidden layer is comprised of neurons, each representing a scalar value. This scalar value is used to compute a weighted sum of the input plus a bias (essentially y1 ~ wX + b) — creating a linear (or more specifically an affine) transformation.
In Tensorflow you have to explicitly define the variables for the weights and bias that comprise this layer. We do so by wrapping them in the tf.Variable function — these are wrapped as variables because the parameters will update as the model learns the weights and bias that best represent relationships in the data. We instantiate the weights with random values with very low variance, and fill the bias variable with zeros. We then define the matrix multiplication that takes place in the layer.

This transformation is then passed through an activation function, (here I am using ReLU or rectified linear units) to make make the output of the linear transformation non-linear. This allows the neural net to model complex non-linear relationships between input and output — check out Siraj Raval’s excellent video explainer on activation functions here.
The output layer is the final layer in the model and, in this case, is size ten, one node for each label. We apply a softmax activation to this layer so that it outputs values between 0 and 1 across the final layer nodes — representing probabilities across the labels.
Cost function and optimisation
Now that the neural net architecture is defined, we set the cost function and optimiser. For this task I use categorical cross entropy. I also define an accuracy measure that can be used to evaluate the model’s performance. Finally, I set the optimiser as stochastic gradient descent and call its minimise method once it is instantiated.

Finally, the model can be run — here 1000 iterations are run. On each iteration a mini-batch of the data is fed to the model, it makes predictions, computes the loss and through backpropogation, updates the weights and repeats the process.

**from three blue one brown’s “But, what is a neural network?” video**
This simple(ish) model gets to around 95.5% accuracy on the test set, which isn’t too bad, but could be a lot better. In the plots below you can see the accuracy and cost for each iteration of the model, one thing that clearly stands out is the discrepancy between the performance on the train set and performance on the test set.

This is indicative of overfitting — that is, the model is learning the training data too well, which is limiting its generalisability. We can handle overfitting using regularisation methods, which will be the subject of my next post.
Thank you for reading 🙂
P.S The full Tensorflow script can be found here and the same model defined in Keras here.
Originally published at dataflume.wordpress.com on December 21, 2017.
",1091,Machine Learning,Conor McDonald,https://towardsdatascience.com/machine-learning-fundamentals-ii-neural-networks-f1e7b2cb3eef,"These videos show how neural networks can be fed raw data — such as images of digits — and can output labels for these images with amazing accuracy.
Neural networks work by computing weighted summations of input vectors that are then passed through non-linear activation functions, thereby creating a mapping from input to output via a non-linear transformation layer.
The weights (represented by neutrons) in the transformation, or hidden, layer are iteratively adjusted such that they come to represent relationships in the data that map inputs to outputs.
This effectively sets up the training data — the X values and y labels we will use to train the neural network.
The hidden layer(s) enable the neural network to create new representations of the input data that the model uses to learn complex and abstract relationships between the data and the labels.
In Tensorflow you have to explicitly define the variables for the weights and bias that comprise this layer.
We do so by wrapping them in the tf.Variable function — these are wrapped as variables because the parameters will update as the model learns the weights and bias that best represent relationships in the data.
This allows the neural net to model complex non-linear relationships between input and output — check out Siraj Raval’s excellent video explainer on activation functions here.
We apply a softmax activation to this layer so that it outputs values between 0 and 1 across the final layer nodes — representing probabilities across the labels.
Now that the neural net architecture is defined, we set the cost function and optimiser."
Notes on Matrix Calculus for Deep Learning,Based on this paper by Parr and Howard.,"Notes on Matrix Calculus for Deep Learning
Based on this paper by Parr and Howard.
Deep learning is an exciting field that is having a great real-world impact. This article is a collection of notes based on ‘The Matrix Calculus You Need For Deep Learning’ by Terence Parr and Jeremy Howard.

Deep Learning is all about Linear algebra. It is the use of neural networks with many many layers to solve complex problems. The model inputs, the neuron weights in multiple layers, the activation functions etc can all be defined as vectors. The operations/transforms needed to train or utilize a neural network are very parallel in nature, applied to all the inputs simultaneously. The vector/matrix representations and linear algebra operations that can be used on them, lend themselves very well to the pipelined data-flow model of a neural network. The math becomes greatly simplified when the inputs, weights, and functions are treated as vectors and the flow of values can be treated as operations on matrices.
Deep Learning is also all about differentiation! Calculating derivatives/having some way to measure the rate of change is critical in the training phase to optimize the loss functions. Starting with an arbitrary set of network model weights w, the goal is to arrive at an ‘optimal’ set of weights so as to reduce a given loss function. Nearly all neural networks use the backpropagation method to find such a set of weights. This process involves determining how a change in the weight value might affect the output. based on this we may decide to increase or decrease the value of the weight by a proportional amount. Measuring how the output changes with respect to a change in weight is the same as calculating the (partial) derivative of the output w.r.t weight w. This process is repeated many times, for all the weights in all the layers, for all the training examples.
Matrix calculus marries two fundamental branches of mathematics - linear algebra and calculus. A large majority of people have been introduced to linear algebra and calculus in isolation. These two topics are heavyweights in their own right. Not many undergraduate courses focus on matrix calculus. People usually rely on intuition to bridge the gaps in understanding while looking into concepts like backpropagation. The backpropagation step in most machine learning algorithms is all about calculating derivatives and updating values in vectors and matrices. Most machine learning frameworks do the heavy-lifting themselves and we never end up seeing the actual derivatives being calculated. However, it is always good to understand how this works internally, and it is essential if you are planning to be a serious practitioner or develop an ML library from scratch.
While the paper is geared to DL practitioners and coders, it is mathematical in nature. It is really important to pay attention to notation to cement your understanding. It was essential to pay particular attention to things like the shape of a vector (long or tall), is the variable scalar or vector, the dimensions of a matrix. Vectors are represented by bold letters. The untrained eye would probably not notice the difference between bold f and italicized f font but this makes a great difference while trying to understand the equations. The same thing goes for the shape and orientation of vectors. I went down multiple rabbit-hole paths trying to understand something, only to learn that my initial reading of the terms was incorrect.
One good thing is the manner in which the concept of functions (and ways to calculate their derivatives )are defined from the simple to the more complex. First, we start with functions of simple parameters represented by f(x). The function and the parameter x are scalars (represented in italics), and we can use the traditional derivative rules for finding derivative of f(x). Second, the kind of functions we’ll see often have many variables associated with it; of the form f(x,y,z). To calculate the derivatives of such functions, we use partial derivatives which are calculated with respect to specific parameters. The branch of calculus dealing with such functions would be multivariate calculus.
Grouping the input variables x, y, z as a vector in bold x, we can represent the scalar function of a vector of input parameters as f(x). The calculus for this field would be vector calculus, wherein the partial derivatives of f(x) are represented as vectors themselves and are amenable to various vector operations. Lastly, what will be most useful for deep learning is to represent multiple such functions at the same time. We use f(x) to represent a set of scalar functions of the form f(x). The field of calculus for this is the most general, namely matrix calculus.
To recap, f(x) is a scalar function of a scalar variable (use simple derivative rules), f(x) is a scalar function of vector variable x (use vector calculus rules) and f(x) is a vector of many scalar valued functions, with each function depending on a vector of inputs x (use matrix calculus rules). The paper demonstrates how to calculate derivatives of simple functions, and the relationships between partial derivatives in multivariate calculus (∂/∂x ), the gradient ∇ f function in vector calculus, and the Jacobian J in matrix calculus. Loosely put, the ∇ f(x) function is the collection of partial derivatives of f in a vector form. The Jacobian of f(x) is basically a stack of the individual ∇ f(x)’s in rows.
In the process of calculating the partial derivatives, the paper makes a few assumptions. It is important to keep in mind the end product to which these concepts will be applied eventually i.e. calculating the partial derivatives for the output function (y = w.x +b) and the loss function. The paper does provide a glimpse of these by foreshadowing where they will be used. The first assumption is that the cardinality of vector x is equal to the number of scalar functions in f. This results in having a nice square Jacobian. If you’re wondering why they need to be equal, consider the case where each input to a neuron xi is associated with a weight wi (the scalar function here is akin to xi*wi) and so we have as many x’s as there are w’s.
Another important assumption is in regards to the element-wise diagonal property. Basically, the property states that the ith scalar function in f(x) is a function of (only) the ith term in vector x. Again this makes more sense when you think of the common neuron mode use case. The contribution of input xi is in proportion to a single parameter wi. Assuming the element-wise diagonal property makes the Jacobian ( made square by the first assumption) into a diagonal matrix, with all the non-diagonal terms being zero.
The next few sections of the paper explain the process of calculating derivatives for more complicated functions. There are a few ways in which functions can go from the simple to the complex. First, consider functions that are derived by applying element-wise binary operators on two vectors (of the same size, of course). These are functions of the form f(x,y) = x + y, or max(x, y). Note that x, y are vectors in this case. Next, there are scalar expansion functions that are derived by multiplying/adding scalars to a vector (might remind some of us of broadcasting in Numpy). This operation involves ‘expanding’ the scalar to the same dimension as the vector and then performing the element-wise multiplication.addition operation. For example y = x + b ( y, x are vectors) means b is expanded to vector b and added element-wise to x.
Third, consider functions that collapse values in a vector to a single value. The most common example is calculating the loss of neural networks, usually of the form y = sum(f(x)). Here y is a scalar value got by adding up the elements of the vector f(x). The derivatives for these three cases are calculated in the paper. There are functions that can be more complex, for which the chain rule of derivatives are used. The paper describes the chain rule for simple scalar functions and gradually extends it all the way to the most general purpose of all, the vector chain rules.
",1384,Machine Learning,Nikhil Balaji,https://towardsdatascience.com/notes-on-matrix-calculus-for-deep-learning-b9899effa7cf,"The model inputs, the neuron weights in multiple layers, the activation functions etc can all be defined as vectors.
The vector/matrix representations and linear algebra operations that can be used on them, lend themselves very well to the pipelined data-flow model of a neural network.
The math becomes greatly simplified when the inputs, weights, and functions are treated as vectors and the flow of values can be treated as operations on matrices.
Calculating derivatives/having some way to measure the rate of change is critical in the training phase to optimize the loss functions.
The backpropagation step in most machine learning algorithms is all about calculating derivatives and updating values in vectors and matrices.
One good thing is the manner in which the concept of functions (and ways to calculate their derivatives )are defined from the simple to the more complex.
The function and the parameter x are scalars (represented in italics), and we can use the traditional derivative rules for finding derivative of f(x).
Lastly, what will be most useful for deep learning is to represent multiple such functions at the same time.
We use f(x) to represent a set of scalar functions of the form f(x).
The paper demonstrates how to calculate derivatives of simple functions, and the relationships between partial derivatives in multivariate calculus (∂/∂x ), the gradient ∇ f function in vector calculus, and the Jacobian J in matrix calculus.
Loosely put, the ∇ f(x) function is the collection of partial derivatives of f in a vector form.
In the process of calculating the partial derivatives, the paper makes a few assumptions.
If you’re wondering why they need to be equal, consider the case where each input to a neuron xi is associated with a weight wi (the scalar function here is akin to xi*wi) and so we have as many x’s as there are w’s.
First, consider functions that are derived by applying element-wise binary operators on two vectors (of the same size, of course)."
How To Spot the Next Winning AI B2B Startup,Rosie from The Jetson’s is to blame for my early attraction to tech. And it’s Silicon Valley and the enterprise world’s fault for dragging…,"How To Spot the Next Winning AI B2B Startup
Source: Grastisography
Rosie from The Jetson’s is to blame for my early attraction to tech. And it’s Silicon Valley and the enterprise world’s fault for dragging me into a world that lives, breathes, and eats tech optimism. Yeah, I too believe in the idea of investing in, creating, and propagating the future through technology.
For 9 years, I pushed existing technology in the B2B Enterprise Tech space, at IBM, Cisco, and at Silicon Valley’s Glassbreakers. The thing is that as soon as I jumped into my last role as VP of Sales at Glassbreakers, I was introduced to this crazy group of people who, instead of pushing existing technology, were pushing for the technology of the future. Since then, I’ve been working towards becoming one of those crazies in the world of venture capital (it’s the - Steve Jobs - good kind of crazy).
Pivoting into this new road, I knew I needed to focus on a particular space, and I chose to put my eggs in the basket of AI — after all, it’s the one that I believe (with my body and soul) will bring the future to the world. And that’s what led way to a column on AI in the Huffington Post, an internship at K Fund, IE Business School and Kauffman Fellows Venture Capital (VC) programs, countless speaking engagements, and the opportunity to join Portfolia’s Enterprise Fund as an investor. And, I’m just getting started. Next (and post-MBA), I want to work for a VC fund full-time and invest in AI and emerging tech.
And so… as an aspiring VC associate in the AI B2B Enterprise space, I thought it would be interesting to analyze the AI B2B startup space. I asked myself, “what would be most valuable as a VC investing in AI?” My answer came down to: identifying features that make for a winning AI B2B Enterprise startup investment. And, right then, I set out to figure out how to spot the next winning AI B2B startup.
Methodology
Using reverse engineering thinking, it occurred to me that looking at AI B2B Enterprise startup exits may provide some insight. So, I went out to analyze all available AI B2B exits.
Source: Grastisography
Exit Analysis
In this post I share my analysis of the AI B2B startup exits. I include insight on the analysis process, insight into the acquired startup, acquirer, investors, and perspective on the factors that may help you spot the next winning AI B2B startup. Going the extra mile, I share my predictions of next exits and share my perspective on what VCs need to keep in mind when looking for a high-potential AI B2B startup.
Analysis Process
I used Mattermark to identify all enterprise software startups that have exited in all history; keywords used included: artificial intelligence, machine learning, and deep learning. The algorithm spat out 31 AI B2B startups — 2 IPOed and 29 were acquired. Keywords in the Mattermark startup description included: analytics. I then used Crunchbase to find the founders of the startups and, finally, used LinkedIn to find founders’ educational backgrounds — degrees and schools they attended.
Exits
List of exited startups: Criteo, OpenSpan, Right Hemisphere, Proximal Labs, Koemei, Analyze Re, Cognitive Security, Metafor Software, Progress Software, QPID Health, Invincea, Acquisio, Cloudmeter, Wise.io, Kanjoya, Boomtrain, Algorithms.io, Nutonian, Prelert, Via Science, Brainspace, CyberFlow Analytics, SignalSense, Retail Optimization, ClearForest, Unified Inbox, Pattern, SkyData Systems, MetaMind, Cognea, and NudgeSpot (acquired by Boomtrain, which was also acquired).
More than a third focused on machine learning and analytics. The median number of founders was 2. The education of founders ranged from college to PhDs. Almost a third of co-founders had attended the same university. A large proportion of founder teams had computer science and engineering backgrounds. A third of startups had co-founders with MBAs.
The median time for an exit since startup launch was 5 years. The range of last funding before the exit ranged from seed to Series D. Post acquisition, over 75% of co-founders continued to work under the acquirer. Geographically, startups who exited were primarily headquartered in North America. 35% of them were from the Bay Area and over 15% were from Boston.
Money
Exit amounts were not widely available (although we do know that MetaMind was acquired for $32.8 Million and OpenSpan was acquired for $52.3 Million). The median of total funds raised by the startups before exits was $6.5 Million. Nearly half of the startups raised under $10 Million. OpenSpan, Invincea, and Criteo were the anomalies having raised $31 Million, $47.3 Million, and $63.4 Million, respectively. Before the startups exited, the last round of funding ranged from $100 Thousand (Koemei) to $40 Million (Criteo). The median amount of last funding raised before exits was $4 Million. Over 45% of startups fell in an annual revenue range of $5 Million to $10 Million at time of exit; and over 21% of them fell in a revenue range between $1 Million and $5 Million.
Acquirers
The acquirers list included: Jive Software, Lumendata, Salesforce, Boomtrain, SAP, Webroot, eviCore Healthcare, Splunk, GE, Crelogix, Zeta Global, DataRobot, Elastic, Workday, Sophos, Web.com, Ultimate Software, Pegasystems, Cisco, Reuters, Cyxtera Technologies, Revionics, and IBM. Splunk rose as top acquirer in the bunch with 3 startup acquisitions: SignalSense, Metafor Software, and Cloudmeter.
Investors
There was no pattern identified in the investor list; although, it was curious to see that Marc Benioff invested in MetaMind before Salesforce acquired the startup. Sequoia, In-Q-Tel, 500 startups, Matrix Partners, and Sierra Ventures, all invested in two of the startups analyzed that exited.
List of investors included: Allegro Venture Partners, Baseline Ventures, Constantin Partners, D.E. Shaw & Co., Floodgate Fund, Ron Conway, SV Angel, Tom McInerney, DFJ Gotham Ventures, Rally Ventures (formerly Icon Venture Partners), Sequoia Capital, Valhalla Partners,500 Startups, Munich Venture Partners, NVIDIA, Sequoia Capital, Sutter Hill Ventures, Khosla Ventures, Marc Benioff, Aeris Capital, Dell Ventures, Grotech Ventures, Harbert Management Corporation, New Atlantic Ventures, ORIX Venture Finance, Meakem Becker Venture Capital,Credo Ventures, Evolution Equity Partners, Cardinal Partners, Massachusetts General Physicians Organization (MGPO), Matrix Partners, New Leaf Venture Partners, Partners Innovation Fund, Ignition Partners,Tola Capital,Trilogy Equity Partners, GrowthWorks Capital,Vanedge Capital, Fonds de solidarite FTQ, Tandem Expansion Fund, Wellington Financial, AngelPad, Cota Capital, Jumpstart Ventures, Lerer Ventures, Signature Capital Securities, Streamlined Ventures, FTV Capital, Globespan Capital Partners, Imlay Investments, In-Q-Tel, Sigma Partners, Sigma Prime Ventures, Angel Investors LP, Plug and Play Ventures, Siemens Venture Capital, Toshiba America Electronic Components, Felicis Ventures, First Round Capital, SoftTech VC, Kae Capital, Alchemist Accelerator, Silicon Valley 2014, Voyager Capital, Atlas Venture, Fairhaven Capital Partners, Intel Capital, Sierra Ventures, Green Park & Golf Ventures (GPG Ventures), Medina Capital, MassChallenge, BDC Venture Capital, Innovacorp, Jevon MacDonald, Rho Canada Ventures, Connecticut Innovations, Elm Street Ventures, Adams Street Partners, Bessemer Venture Partners, Elaia Partners, IDInvest Partners, Index Ventures, Sapphire Ventures, and SoftBank Capital.
Source: Grastisography
M&A Professionals: Predictions of Next Exits
If past trends continue, common features of startups that will exit next will include:
Annual revenue $5–10 Million range
Median of total funds raised $6.5
Co-founders with engineering and computer science degrees and, at times, PhDs
1 to 2 founders
Median of 5 years old
In order to identify potential AI B2B startups exits, I captured data from Mattermark, Crunchbase, and LinkedIn and analyzed it. I used Mattermark to identify all AI B2B startups that have not exited (keywords used included: artificial intelligence, machine learning, and deep learning). Mattermark outputted nearly 200 existing AI B2B startups. I used Crunchbase to identify founders and LinkedIn for educational backgrounds.
Predictions of Next Exits
If past trends continue, the startups that have 1 to 2 co-founders with computer science and engineering degrees and have annual revenues in the range of $5–10 Million are prime for exits. Out of nearly 200 AI B2B startups, only 13 matched the latter criteria. These include:
Source: Author
VC Professionals: Keep this in Mind When Looking for High-Potential AI B2B Startups
Based on the data analysis, as with AI B2B Startups M&A targets, I’d also suggest for VCs to consider the features discussed above when assessing an AI startup investment — (1) 1 to 2 co-founders who hold (2) engineering and computer science degrees. It may be of interest to invest in one of the startups listed in the predicted exit list.
Of course, as a VC you want to get in as early as possible to reap higher rewards from the exit of an AI B2B startup investment. Consider the funding round and total amount of money the startup has raised. It may be interesting to look at their cap table, their investor and board list.
With a bit of curiosity, I re-assessed the current AI B2B startup list for potential VC investments. I found 5 startups that fit the criteria mentioned above but that have annual revenues of between $1–5M. These startups may be an even better investment for early-stage VCs (and for potential economic and control leverage).
Source: Author
Conclusion
Source: Grastisography
I come back to my original question, “what would be most valuable as a VC investing in AI?” I analyzed data (from Mattermark, Crunchbase, and LinkedIn) and identified features that make for a “winning” AI B2B Enterprise startup exit and investment. I identified “winning” features to include:
Annual revenue $5–10 Million range
Median of total funds raised $6.5
Co-founders with engineering and computer science degrees and, at times, PhDs
1 to 2 founders
Median of 5 years old
Using data was a great way of drawing patterns and taking a stab at predicting the future, however, you should take caution with the results. We don’t know if Mattermark’s, Crunchbase’s, and LinkedIn’s data is accurate or up-to-date and it’s hard to say that any quantitative deductions are 100% accurate. That’s why it’s important to use quantitative analysis as only part of an analysis procedure in investments (and in other areas in life and work). Data analysis and/or algorithms today need to be used as augmented assistance and not accepted as the complete truth.
Source: Grastisography
My recommendation is to use a combination of qualitative and quantitative analysis when making decisions. Because after all, it’s not numbers nor quantitative analysis that make for successful companies. It is great people who build great companies.
If you’re a VC fund and either don’t want to do the dirty work or want help doing it, let me do it for you.
If you liked this post, clap 👏👏👏👏👏 and share it!
Source: LinkedIn
About Author: Lolita Taub is a TEDx speaker and keynote, a World Economic Forum Global Shaper, an artificial intelligence enthusiast, and an enterprise tech professional and investor at Portfolia. She holds 9 years of enterprise B2B software-hardware-and-services sales experience at IBM, Cisco Systems, and in Silicon Valley. Lolita has been recognized for her work on Forbes, Inc.com, The Huffington Post, Entrepreneur.com, and Los Angeles Times, among other publications.
Follow Lolita on Twitter @lolitataub, visit her here, and connect with her on LinkedIn here.
",1832,Venture Capital,Lolita M Taub,https://medium.com/s/story/how-to-spot-the-next-winning-ai-b2b-startup-e579b4729dbe,"Next (and post-MBA), I want to work for a VC fund full-time and invest in AI and emerging tech.
Using reverse engineering thinking, it occurred to me that looking at AI B2B Enterprise startup exits may provide some insight.
In this post I share my analysis of the AI B2B startup exits.
I include insight on the analysis process, insight into the acquired startup, acquirer, investors, and perspective on the factors that may help you spot the next winning AI B2B startup.
Going the extra mile, I share my predictions of next exits and share my perspective on what VCs need to keep in mind when looking for a high-potential AI B2B startup.
I used Mattermark to identify all enterprise software startups that have exited in all history; keywords used included: artificial intelligence, machine learning, and deep learning.
I then used Crunchbase to find the founders of the startups and, finally, used LinkedIn to find founders’ educational backgrounds — degrees and schools they attended.
List of exited startups: Criteo, OpenSpan, Right Hemisphere, Proximal Labs, Koemei, Analyze Re, Cognitive Security, Metafor Software, Progress Software, QPID Health, Invincea, Acquisio, Cloudmeter, Wise.io, Kanjoya, Boomtrain, Algorithms.io, Nutonian, Prelert, Via Science, Brainspace, CyberFlow Analytics, SignalSense, Retail Optimization, ClearForest, Unified Inbox, Pattern, SkyData Systems, MetaMind, Cognea, and NudgeSpot (acquired by Boomtrain, which was also acquired).
A third of startups had co-founders with MBAs. The median time for an exit since startup launch was 5 years.
The median of total funds raised by the startups before exits was $6.5 Million.
Before the startups exited, the last round of funding ranged from $100 Thousand (Koemei) to $40 Million (Criteo).
The median amount of last funding raised before exits was $4 Million.
There was no pattern identified in the investor list; although, it was curious to see that Marc Benioff invested in MetaMind before Salesforce acquired the startup.
If past trends continue, common features of startups that will exit next will include:
In order to identify potential AI B2B startups exits, I captured data from Mattermark, Crunchbase, and LinkedIn and analyzed it.
I used Mattermark to identify all AI B2B startups that have not exited (keywords used included: artificial intelligence, machine learning, and deep learning).
If past trends continue, the startups that have 1 to 2 co-founders with computer science and engineering degrees and have annual revenues in the range of $5–10 Million are prime for exits.
Based on the data analysis, as with AI B2B Startups M&A targets, I’d also suggest for VCs to consider the features discussed above when assessing an AI startup investment — (1) 1 to 2 co-founders who hold (2) engineering and computer science degrees.
It may be of interest to invest in one of the startups listed in the predicted exit list.
Consider the funding round and total amount of money the startup has raised.
I come back to my original question, “what would be most valuable as a VC investing in AI?” I analyzed data (from Mattermark, Crunchbase, and LinkedIn) and identified features that make for a “winning” AI B2B Enterprise startup exit and investment."
Can Machine Learning And NLP Help Predict Suicidal Risk?,"From Depression To Suicide, How The Words We Use Can Predict The Way We Feel.","Can Machine Learning And NLP Help Predict Suicidal Risk?

From Depression To Suicide, How The Words We Use Can Predict The Way We Feel.
Unfortunately, the number of people suffering from depression is on the rise. According to this study by the NIH, nearly 7 percent of US adults, and over 10 percent of those between the ages of 18–25, have had a major depressive episode in the last year. The CDC published another surprising and frightening statistic: less than one-third of Americans taking one antidepressant medication, and less than one-half of those taking multiple antidepressants have seen a mental health professional in the past year.
Nearly 7 % of adults and over 10 % of those between the ages of 18–25 have had a major depressive episode in the last year.
Percentage of people by age and sex who have taken anti-depressants in the last year. 2005–2008. Source: CDC.
Since many lack better alternatives, the internet has become a popular venue for people to reach out for help. All this public data provides us with a tremendous opportunity to study this population and try to understand what factors may be driving this epidemic and putting people at risk.
The goal of my project was to determine if I could predict which users of a depression forum, would eventually turn suicidal.
The goal of my project was to determine if I could predict which users of a depression forum would eventually turn suicidal. I began to scrape takethislife.com, a website that contains forums that provide support to those dealing with suicidal or depressive feelings. Scraping is a data extraction method that involves writing code to access a website and automatically obtain all the information that I needed, without needlessly copying and pasting through 25,000 pages.
I scraped the depression and suicide forums for the text of the posts along with the usernames. The next step was to actually label my data, I took all of the posts from the depression forum and labeled each post with a one or a zero indicating if that user had also posted in the suicide forum. By using Natural Language Processing to analyze the text, I hoped to learn what words were indicative of the difference in the way the two groups speak.
Data Analysis
Computers can’t actually understand words in the way that humans can.
NLP
Computers can’t actually understand words in the way that humans can. Natural Language Processing involves turning text into numbers so that they can be understood by a computer and processed mathematically. There are several different ways to do this, the most common method being a count vectorizer, which counts the number of times a word appears, on the assumption that the more it’s used the more important and the higher “score” it deserves. This assumption doesn’t hold true in many cases, and in my case I decided to go with something called a TFIDF vectorizer.
A TFIDF vectorizer uses the following rule: rank of a words appearance * number of times it’s written = score. Let me give you an example from James Joyce’s Ulysses. The word is has a rank of 20 and appears 1,435 times, which gives it a score of 28,700. The word Zurich has a rank of 29,055 and only appears 1 time, which gives it a score of 29,055, making the word Zurich more important than the word is. This makes sense, because if you were listening to a conversation, you would learn more about what the person was talking about by hearing the word Zurich once than hearing is almost 1500 times.
Predictions
Now that I had a score for each word in every post, and whether or not that user had also posted in the suicide forum I was ready to start my predictions and test out my hypothesis. Were there words that the users wrote in the depression forum that could help us predict if they would also post in the suicide forum?
Were there words that the users wrote in the depression forum that could help us predict if they would also post in the suicide forum?
To do this I used something called Machine Learning, which (to gloss over full PHD programs worth of knowledge) is when we use mathematical formulas together with usually large amounts of data to see if we could spot patterns and make predictions. It usually focuses around this question: Given some input X, what is the correct prediction Y? In this case given that a user posted words A,B, or C (our input X) can we predict if they will also post in the suicide forum (our output Y). I tried out many different ML models to see which would provide the most predictive power, and eventually went with a basic one called a logistic regression. Logistic regression follows very similar rules to a linear regression which you may vaguely remember from high-school math. Instead of giving you a quantitative output, say how much will a house cost (output Y) if it has 5 bedrooms and 3 bathrooms (input X)? it gives you the probability that an observation belongs to a specific class (in this case depressive or suicidal).
After we run our model we score it against a previously held out chunk of our original data to see how well our predictions did. The logistic regression models had a recall of 73, which means that 73 percent of the times that our user was actually suicidal our model correctly predicted so.
A moral dilemma that I ran into at this stage was the issue of weighting my classes. In medical testing models will usually be weighted more heavily to guess one side over the other in order to minimize missing any of the real occurrences of the disease. This comes at a cost though, now the model, while trying to catch all real occurrences, will also tell people who don’t have the disease that they do. While this is often a big question in medical research in general, traditionally researchers will weigh their models. To try and address this I decided to also create a model with slightly weighted classes, with the understanding that healthcare professionals could account for any potential issues. With this method I was able to get my model’s recall up to 83. I could have kept going with the weighting, but at a certain point you have to differentiate between just guessing yes for everybody and actually having a model, and I felt that this was a good balance.
A look at how the two different models classified users. Notice when the number of people correctly classified as suicidal (bottom right) went up, the people classified correctly as only depressed (top left) went down.
The words themselves tell an entire story and confirmed my belief that there is a difference in the way the two groups speak.
The Words That Tell
Since I ended up going with a logistic regression model I was able to see which words the model used to determine which class a user belonged to. The words themselves tell an entire story and confirmed my belief that there is a difference in the way the two groups speak.
The phrase hate myself for was one of the strongest indicators of a person staying in the depression class, while the term hating myself was a predictor of suicide. When I first saw these two words I assumed that they were just words that got mixed in to different classes, but when I thought about it again it made sense. The term that indicated depression (in this case a good thing) was localized, I hate myself for, a particular reason which I’m aware of, and am possibly working on. While those who just hated themselves as a global, all encompassing feeling, were not able to make that distinction and lost hope much quicker.
Another pair of words that I found interesting were God, indicative of suicide, and religious which was indicative of depression, which again I found to be confusing. My theory is as follows a person who identified as religious clearly found meaning and reason for hope in it, but a person who spoke of God, a more esoteric idea, probably used it in terms of an fear of angering God and was terribly haunted by it.
Other terms that were indicative of staying in depression were fat, with other people, seeing therapist, drama, in college, in debt, lately I’ve been, who love me, long distance, on anti-depressants, cheated on me, been diagnosed, rut, and dog. Many of these words are regular everyday stresses of life that people are having a very hard time of dealing with, and some of the others allow us to see what people use to find meaning in life, such as friends, family, religion, and pets. Some terms that were indicative of suicide were idk, have to, cutting, pain, don’t deserve to, raped, sexual abuse, the hospital, hurt so much, have no control, and ashamed of.
Words indicative of their class and their feature importance.
When you see the story that the words tell and the circumstances that lead people there it is very clear that the words people use can really tell can predict their feelings.
What Next
Moving forward in this project I plan on tagging different parts of speech to see if different users form sentences differently, using industry specific stop-words to clean up the text, and hopefully one day deploying it and use it to help people.
What do you think? Have you worked on any similar projects ? Please share in the comments and responses below. Thank you!
",1557,Data Science,Yoni Levine,https://medium.com/s/story/from-depression-to-suicide-how-the-way-we-speak-can-predict-the-way-we-feel-ed359e54c81,"From Depression To Suicide, How The Words We Use Can Predict The Way We Feel.
The goal of my project was to determine if I could predict which users of a depression forum would eventually turn suicidal.
The goal of my project was to determine if I could predict which users of a depression forum would eventually turn suicidal.
The next step was to actually label my data, I took all of the posts from the depression forum and labeled each post with a one or a zero indicating if that user had also posted in the suicide forum.
By using Natural Language Processing to analyze the text, I hoped to learn what words were indicative of the difference in the way the two groups speak.
There are several different ways to do this, the most common method being a count vectorizer, which counts the number of times a word appears, on the assumption that the more it’s used the more important and the higher “score” it deserves.
A TFIDF vectorizer uses the following rule: rank of a words appearance * number of times it’s written = score.
Now that I had a score for each word in every post, and whether or not that user had also posted in the suicide forum I was ready to start my predictions and test out my hypothesis.
Were there words that the users wrote in the depression forum that could help us predict if they would also post in the suicide forum?
Were there words that the users wrote in the depression forum that could help us predict if they would also post in the suicide forum?
In this case given that a user posted words A,B, or C (our input X) can we predict if they will also post in the suicide forum (our output Y).
The logistic regression models had a recall of 73, which means that 73 percent of the times that our user was actually suicidal our model correctly predicted so.
Since I ended up going with a logistic regression model I was able to see which words the model used to determine which class a user belonged to.
The phrase hate myself for was one of the strongest indicators of a person staying in the depression class, while the term hating myself was a predictor of suicide."
My biggest learning while solving Classification problems in Machine Learning,I started my Data Science journey a few months back. I had been doing my share of learning before that. I thought I was ready as I had been…,"
My biggest learning while solving Classification problems in Machine Learning
I started my Data Science journey a few months back. I had been doing my share of learning before that. I thought I was ready as I had been participating in hackathons and competitions before. But I must admit — working on real life data science problems is a different beast altogether.
So, what was my biggest learning in my new data science role?
It was that “Defining the problem statement for Classification Problems” can be very challenging. Now, I know — it might sound simple to you. It did to me when I first started, until I realized how difficult it could be.
Let me explain this in more detail
I got a chance to explore Indian Fintech and Insurance industry. And it was very critical to assess the risk well. Or in other words — “How to define good vs bad?”

This is never an issue when we participate in Data Hackathons, where you get Ready to use data. As you get a labeled data set which makes it easy to differentiate between Good and Bad.
For example: In the image below (Image 1.1), it is easy to see that loan_status 0 refers to Good customers while loan_status 1 refers to Bad customers.
Image 1.1- Data for Loan Prediction Hack
However, in real life problems, designing a definition that will efficiently classify the data into multiple or binary classes can end up being a real challenge.
Incorrect choice will put your further analysis and model for a toss. That means designing the criteria that defines 1 and 0 is one of the critical task on any given data problem.

Example — Credit Risk Modeling
Let us start with understanding, what is Credit Risk?
Credit risk refers to the probability of loss due to a borrower’s failure to make payments on any type of debt.
Credit Risk Management, meanwhile, is the practice of mitigating losses incurred due to credit defaults by understanding the adequacy of both a bank’s capital and loan loss reserves at any given time.
Why is credit risk important for a financial institution?
Without a doubt, Credit Risk Management is a process that has been a challenge for financial institutions, however a better developed process enables a financial institute to absorb the higher capital costs for credit risk. Better credit risk management also presents an opportunity to greatly improve overall performance and secure a competitive advantage.
Data available with financial institutes
Typically, data available for a given credit risk model will comprise of data for loans given by the financial institute to the customers in past. This will include:
Demographics
Income Details
Bureau details
Customer repayment transactions.
The idea is to use this data to understand which customers are defaulting on their payments, going in partial or complete delinquencies (not paying the due payments on time), their loss bookings, debt to income ratio and much more.
Image 1.2: Glimpse of how real life data looks like
So now, to assess the risk better — we need to define who are good customers are who are bad and this is where our challenge starts.
A naive approach
A simple way might be to say Good Customers as customers who have never defaulted on any payment and Bad Customers as At-least defaulted once. (Default here refers to No of days past due the EMI/Premium payment date).
Sound good — This is setback moment.
Not every time a definition as simple as this would work. One would have to go through multiple iterations with the business to crack this definition.
A set of bad definitions can be chosen in this situation could be:
Customers Defaulted ever and also led to loss bookings
Customers Defaulted ever and had high debt to income ratio
Customer Defaulted in first 3 months of their tenure at-least twice.
Customers Defaulted in first 6 months of their tenure at-least thrice
Customers Defaulted in first 12 months of their tenure consecutively thrice.
Customers Defaulted in first 18 months of their tenure consecutively thrice.
This is not an exhaustive list. Putting all options could be taxing.
Thus, to finalize on a “Definition for Good and Bad”, generally few prescribed definitions are tested over sample of data. Typically, the real challenge is to arrive at a definition that captures “MOST- BAD” customers in shortest span, and exactly here is where Roll Rate Analysis helps.
What is Roll Rate analysis?
In this exercise, we choose a period(Months/Years) for analysis as well a static pool of customers on whom analysis is to be done. (Customers who took loan at around same time refers to static pool. Eg: All the customers who took loan in March 2018 is defined as static pool for March)
One must choose the period for Roll Rate analysis very tactfully and must note that no grave external factors like Demonetization are affecting or biasing the results. Once this period is decided we create a matrix that will analyze this static pool of customers for different default rates.
Image 1.3: Matrix highlighting the percentages of defaults in last 18 months window
The above image(Image 1.3) tells us the percentages of customers in various default buckets at the end of 18 months performance.
The first line in matrix explains us that there were 87% customers in no due bucket of which, 5% went in 30 days due and so on..
Similarly the second row in the matrix says that at the start of analysis there were 10% customers in 30 days due and in the period if 18 months of them, 77% of them jumped below 30 days due while 13% jumped ahead of 30 days due and so on…
Building such matrix one can determine the best balance between Roll back and Roll forward and bench mark as the segmenting point for good vs bad. Thus above matrix states using 120 days past due is the best number.
However the complexity does not end here. Once we decide on appropriate days past due number we must check the optimal time for which we must check 120 days past due. ie, 120 days past due in first 4 months, 120 days past due in first 6 months , 120 days past due in first 9 months, 120 days past due in first 12 months etc. The below curve shall solve the issue(Image 1.2).
Image 1.3: Counts of customers in days past due for different static pools.
Here we see that for different vintages of customers, a flattening phenomenon is observed post 9 months of loan tenure. Thus this will further strengthen our definition making it more accurate as 120 Days past Due in first 9 months.
So, to summarize, I would like to emphasize choosing the correct Bad definition is indeed a critical task and definitely requires some iterations to zero in on. The Credit Risk problem discussed in this article is also applicable for transaction centric scenarios to understand the behavior of the customers in greater depth. Such an in depth approach shall help us define “MOST-BAD” customers “a Best Criterion for a Classification problem” in shortest span.
Have you encountered a situation like this where you were caught in defining the problem in the right manner? If so, let me know, what was your experience. I would love to hear and learn from your experience.
",1190,Data Science,tanvi purohit,https://medium.com/s/story/my-biggest-learning-while-solving-classification-problems-in-machine-learning-7172bff9fad8,"However, in real life problems, designing a definition that will efficiently classify the data into multiple or binary classes can end up being a real challenge.
Credit Risk Management, meanwhile, is the practice of mitigating losses incurred due to credit defaults by understanding the adequacy of both a bank’s capital and loan loss reserves at any given time.
Typically, data available for a given credit risk model will comprise of data for loans given by the financial institute to the customers in past.
The idea is to use this data to understand which customers are defaulting on their payments, going in partial or complete delinquencies (not paying the due payments on time), their loss bookings, debt to income ratio and much more.
So now, to assess the risk better — we need to define who are good customers are who are bad and this is where our challenge starts.
Customers Defaulted in first 6 months of their tenure at-least thrice
Customers Defaulted in first 18 months of their tenure consecutively thrice.
Customers Defaulted in first 18 months of their tenure consecutively thrice.
Typically, the real challenge is to arrive at a definition that captures “MOST- BAD” customers in shortest span, and exactly here is where Roll Rate Analysis helps.
(Customers who took loan at around same time refers to static pool.
Eg: All the customers who took loan in March 2018 is defined as static pool for March)
Once this period is decided we create a matrix that will analyze this static pool of customers for different default rates.
The above image(Image 1.3) tells us the percentages of customers in various default buckets at the end of 18 months performance.
Image 1.3: Counts of customers in days past due for different static pools.
Such an in depth approach shall help us define “MOST-BAD” customers “a Best Criterion for a Classification problem” in shortest span."
Semi-supervised learning with Generative Adversarial Networks (GANs),"If you ever heard or studied about deep learning, you probably heard about MNIST, SVHN, ImageNet, PascalVoc and others. Each of these…","Semi-supervised learning with Generative Adversarial Networks (GANs)

If you ever heard or studied about deep learning, you probably heard about MNIST, SVHN, ImageNet, PascalVoc and others. Each of these datasets has one thing in common. They consist of hundreds and thousands of labeled data. In other words, these collections are composed of (x,y) pairs where (x) is the raw data, an image matrix for instance, and (y) is a description of what that data point (x) represents.
Take the MNIST dataset as an example. Each of the 60,000 data points is a (input, label) pair. input is a 28x28 grayscale image and label is a sign representing what the input is. For the MNIST dataset, an input image may be a one, two, three and so on up to nine possible categories.
MNIST pixel representation.
The most common usage of datasets like these is to develop supervised models. To train such algorithms, we usually provide a tremendous amount of data samples.
Supervised learning has been the center of most researching in deep learning. But, the necessity of creating models capable of learning from fewer data is increasing faster.
With that in mind, semi-supervised learning is a technique in which both labeled and unlabeled data are used to train a classifier.
This type of classifier takes a tiny portion of labeled data and a much larger amount of unlabeled data (from the same domain). The goal is to combine these sources of data to train a Deep Convolution Neural Networks (DCNN) to learn an inferred function capable of mapping a new datapoint to its desirable outcome.
In this frontier, we present a GAN model to classify street view house numbers using a very small labeled training set. In fact, the model uses roughly 1.3% of the original SVHN training labels i.e. 1000 (one thousand) labeled examples. We use some of the techniques described in the paper Improved Techniques for Training GANs from OpenAI.
If you are not familiar with image generation GANs, refer to A Short Introduction to Generative Adversarial Networks. This article refers to some of the contents described in that piece. The complete code notebook can found here.
Intuition
When building a GAN for generating images, we trained both — the generator and the discriminator at the same time. After training, we could discard the discriminator because we used it only for training the generator.
Semi-supervided learning GAN architecture for an 11 class classification problem.
For semi-supervised learning, we need to transform the discriminator into a multi-class classifier. This new model has to be able to generalize well on the test set, even through we do not have many labeled examples for training.
Additionally, this time, by the end of training, we can actually throw away the generator. Note that the roles changed. Now, the generator is only used for helping the discriminator during training.
Putting it differently, the generator acts as a different source of information from which the discriminator gets raw unlabeled training data. As we will see, this unlabelled data is key to improve the discriminator performance.
Also, for a regular image generation GAN, the discriminator has only one role. Compute the probability of whether its inputs are real or not — let’s call it the GAN problem.
However, to turn the discriminator into a semi-supervised classifier, besides to the GAN problem, the discriminator also has to learn the probabilities of each of the original dataset classes.
In other words, for each input image, the discriminator has to learn the probabilities of it being a one, two, three and so on.
Recall that for an image generation GAN discriminator, we have a single sigmoid unit output. This value represents the probability of an input image being real (value close to 1), or fake (value near 0 ).
Put in other words, from the discriminator point of view, values close to 1 means that the samples are likely to come from the training set. Likewise, value near 0 means a higher change that the samples come from the generator network.
By using this probability, the discriminator is able to send a signal back to the generator. This signal allows the generator to adapt its parameters during training making it possible to improve its capabilities of creating realistic images.
We have to convert the discriminator (from the previous GAN) into an 11 class classifier. To do that, we can turn its sigmoid output into a softmax with 11 class outputs. The first 10 for the individual class probabilities of the SVHN dataset (zero to nine), and the 11th class for all the fake images that come from the generator.
Note that if we set the 11th class probability to 0, then the sum of the first 10 probabilities represents the same probability computed using the sigmoid function.
Finally, we need to setup the losses in such a way that the discriminator can do both:
- (i) help the generator learning to produce realistic images. To do that, we have to instruct the discriminator to distinguish between real and fake samples.
- (ii) use the generator’s images, along with the labeled and unlabeled training data, to help classify the dataset.
To summarize, the discriminator has three different sources of training data.
Real images with labels. These are image label pairs like in any regular supervised classification problem.
Real images without labels. For those, the classifier only learns that these images are real.
Images from the generator. To these ones, the discriminator learns to classify as fake.
The combination of these different sources of data will make the classifier able to learn from a broader perspective. That in turn, allows the model to perform inference much more precisely than it would be if only using the 1000 labeled examples for training.
Generator
The Generator follows a very standard implementation described in the DCGAN paper. This approach consists of taking a random vector z as input. Reshape it to a 4D tensor and then feed it to a sequence of transpose convolutions, Batch Normalization (BN) and leaky ReLU operations.
This sequence of computations increases the spatial dimensions of the input vector while reduces its number of channels. As a result, the network outputs a 32x32x3 RGB tensor shape squashed between values of -1 and 1 through the Hyperbolic Tangent Function.
Discriminator
The discriminator, now a multi-class classifier, is the most relevant network. Here, we setup an also similar DCGAN architecture in which we use a stack of convolutions with BN and ReLU.
We use strided convolutions for reducing the dimensions of the feature-vectors. Note that not all convolutions perform this type of computation. When we want to keep the feature vector’s dimensions intact, we use strides of 1 otherwise, we use strides of 2. Finally, to stabilize learning we make extensive use of BN (except the first layer of the network).
The 2D convolution window (kernel or filter) is set to have a width and height of 3 across all the convolutions. Also, note that we have some layers with dropout. It is important to understand that our discriminator behaves (in part) like any other regular classifier. Because of that, it may suffer from the same problems any classifier would if not well designed.
When training a big classifier on a very limited dataset, one of the most likely drawbacks one might encounter, is the immense of overfitting. One thing to watch on “over trained” classifiers is that they typically show a notably difference between the training error (smaller) and the testing error (higher).
This situation shows that the model did a good job capturing the structure of the training dataset. However, because it believed too much in the training data, it fails to generalize for unseen examples.
To prevent that, we make an extensive usage of regularization through dropout. Even for the first layer of the network.
In the end, instead of applying a fully connected layer on top of the convolution stack, we perform Global Average Pooling (GAP). In GAP, we take the average over the spatial dimensions of a feature vector. This operation results in squashing the tensor dimensions to a single value.
For instance, suppose that after a series of convolutions, we get a tensor of shape [BATCH_SIZE, 8, 8, NUM_CHANNELS]. To apply GAP, we take the average value over the [8x8] tensor slice. This results in a tensor of shape [BATCH_SIZE, 1, 1, NUM_CHANNELS] that can be reshaped to [BATCH_SIZE, NUM_CHANNELS].
In Network in Network, the authors describe some advantages of GAP over traditional fully connected layers. These include: higher robustness for spatial translation and less overfitting concerns. After GAP, we apply a fully connected layer to output the final logits. These have shape [BATCH_SIZE, NUM_CLASSES] and corresponds to the unscaled final class values.
To get the classification probabilities, we feed the logits through the softmax function. However, we still need a way to represent the probability of an input image being real rather than fake. That is, we still need to account for the binary classification problem of regular a GAN.
We know that the logits are in terms of softmax probability values. Yet, we need a way to represent them as sigmoid logits as well. We know that the probability of an input being real corresponds to the sum over all real class logits. With that mind, we can feed these values to a LogSumExp function that will model the binary classification value. After that, we feed the result from the LogSumExp to a sigmoid function.
We can use the Tensorflow’s LogSumExp built-in function to avoid numerical problems. This routine prevents over/under flow issues that may occur when LogSumExp encounters very extreme, either positive or negative values.
Model Loss
As we mentioned, we can divide the discriminator loss in two parts. One that represents the GAN problem, the unsupervised loss. And the other that computes the individual real class probabilities, the supervised loss.
For the unsupervised loss, the discriminator has to differentiate between real training images and fake images from the generator.
As for a regular GAN, half of the time the discriminator receives unlabeled images from the training set and the other half, imaginary unlabeled images from the generator.
In both cases we are dealing with a binary classification problem. Since we want a probability value near 1 for real images and close to 0 for unreal ones, we can use the sigmoid cross entropy function to compute the loss.
For images coming from the training set, we maximize their probabilities of being real by assigning labels of 1s. For fabricated images coming from the generator, we maximize their probabilities to be fake by giving them labels of 0s.
For the supervised loss, we need to use the logits from the discriminator. Since this is a multi-class classification problem, we can use the softmax cross entropy function with the real labels we have available.
Note that this part is similar to any other classification model. In the end, the discriminator loss is the sum of both the supervised loss and the unsupervised loss. Also, because we are pretending we do not have most of the labels, we need to ignore them in the supervised loss. To do that, we multiply the loss by the masks variable that indicates which set of labels are available for usage.
As described in the Improved Techniques for Training GANs paper, we use feature matching for the generator loss.
As the authors describe:
Feature matching is the concept of penalizing the mean absolute error between the average value of some set of features on the training data and the average values of that set of features on the generated samples.
To do that, we take some set of statistics (the moments) from two different sources and force them to be similar.
First, we take the average of the features extracted from the discriminator when a real training minibatch is being processed.
Second, we compute the moments in the same way, but now for when a minibatch composed of fake images that come from the generator was being analyzed by the discriminator.
Finally, with these two sets of moments, the generator loss is the mean absolute difference between them. In other words, as the paper emphasizes:
We train the generator to match the expected values of the features on an intermediate layer of the discriminator.
Although feature-matching loss performs well on the task of semi-supervised learning, the images produced by generator are not as good as the ones created in the last post.
Sample images created by the generator network using the feature matching loss.
In the Improved Techniques for Training GANs paper, OpenAI reports state-of-the-art results for semi-supervised classification learning on MNIST, CIFAR-10 and SVHN.
Our implementation reaches train and test accuracies of nearly 93% and 68% respectively. These results are better than the ones reported in this NIPS 2014 paper which got something around 64%.
This notebook is not intended for demonstrating best practices such cross-validation techniques. It only uses some of the techniques described in the original OpenAI paper.
The notebook was based on the Udacity Deep learning Fundamentals nanodegree program in which I graduated from.
Concluding
Many researches considers unsupervised learning as the missing link to general AI systems.
To break these obstacles, attempts to solve already established problems using less labeled data are key. In this scenario, GANs pose a real alternative for learning complicated tasks with less labeled samples.
Yet, the performance gap between supervised and semi-supervised learning is still far from being equal. But we certainly can expect this gap to become shorter as new approaches come in to play.
If you are curious to dig deeper in these subjects, take a look at:
Dive head first into advanced GANs: exploring self-attention and spectral norm
Lately, Generative Models are drawing a lot of attention. Much of that comes from Generative Adversarial Networks…medium.freecodecamp.org
An intuitive introduction to Generative Adversarial Networks (GANs)
In a GAN, two differentiable functions are locked in a game. The two players, the generator and the discriminator have…medium.freecodecamp.org
And if you need more, that is my deep learning blog.
Thanks for reading!

Credits to Sam Williams for this awesome “clap” gif! Check it out in his post.
",3008,Machine Learning,Thalles Silva,https://towardsdatascience.com/semi-supervised-learning-with-gans-9f3cb128c5e,"Semi-supervised learning with Generative Adversarial Networks (GANs)
With that in mind, semi-supervised learning is a technique in which both labeled and unlabeled data are used to train a classifier.
The goal is to combine these sources of data to train a Deep Convolution Neural Networks (DCNN) to learn an inferred function capable of mapping a new datapoint to its desirable outcome.
In this frontier, we present a GAN model to classify street view house numbers using a very small labeled training set.
Semi-supervided learning GAN architecture for an 11 class classification problem.
For semi-supervised learning, we need to transform the discriminator into a multi-class classifier.
This new model has to be able to generalize well on the test set, even through we do not have many labeled examples for training.
Putting it differently, the generator acts as a different source of information from which the discriminator gets raw unlabeled training data.
Also, for a regular image generation GAN, the discriminator has only one role.
However, to turn the discriminator into a semi-supervised classifier, besides to the GAN problem, the discriminator also has to learn the probabilities of each of the original dataset classes.
In other words, for each input image, the discriminator has to learn the probabilities of it being a one, two, three and so on.
Recall that for an image generation GAN discriminator, we have a single sigmoid unit output.
Put in other words, from the discriminator point of view, values close to 1 means that the samples are likely to come from the training set.
The first 10 for the individual class probabilities of the SVHN dataset (zero to nine), and the 11th class for all the fake images that come from the generator.
- (ii) use the generator’s images, along with the labeled and unlabeled training data, to help classify the dataset.
These are image label pairs like in any regular supervised classification problem.
However, we still need a way to represent the probability of an input image being real rather than fake.
For the unsupervised loss, the discriminator has to differentiate between real training images and fake images from the generator.
As for a regular GAN, half of the time the discriminator receives unlabeled images from the training set and the other half, imaginary unlabeled images from the generator.
Since we want a probability value near 1 for real images and close to 0 for unreal ones, we can use the sigmoid cross entropy function to compute the loss.
For images coming from the training set, we maximize their probabilities of being real by assigning labels of 1s.
For fabricated images coming from the generator, we maximize their probabilities to be fake by giving them labels of 0s.
For the supervised loss, we need to use the logits from the discriminator.
Since this is a multi-class classification problem, we can use the softmax cross entropy function with the real labels we have available.
As described in the Improved Techniques for Training GANs paper, we use feature matching for the generator loss.
Second, we compute the moments in the same way, but now for when a minibatch composed of fake images that come from the generator was being analyzed by the discriminator.
We train the generator to match the expected values of the features on an intermediate layer of the discriminator.
Although feature-matching loss performs well on the task of semi-supervised learning, the images produced by generator are not as good as the ones created in the last post.
Sample images created by the generator network using the feature matching loss.
In the Improved Techniques for Training GANs paper, OpenAI reports state-of-the-art results for semi-supervised classification learning on MNIST, CIFAR-10 and SVHN."
3 challenges for resort revenue managers (and a possible solution),"In the nineties I was head rep for an Italian TO in Rhodes. Our main product was the Blue Sea Hotel >> in Faliraki, where we would have…","3 challenges for resort revenue managers (and a possible solution)

In the nineties I was head rep for an Italian TO in Rhodes. Our main product was the Blue Sea Hotel >> in Faliraki, where we would have about a hundred weekly arrivals, up to 300 hundred pax in August. There was a guaranteed allotment contract in place, nevertheless during high season we would have a few overbooked rooms every week! NO kidding. I guess it was a smart strategy for Mr Costas Nikitaras, the General Manager, as he certainly knew his trade. But in the long run, I think it was way too costly for the hotel; I know for a fact that it was for the Italian operator.
Over twenty years later, I am amazed to find out this kind of procedure is still used in certain resorts, in spite of the massive industry-wide changes in terms of distribution and technology since Mr Nikitaras’ tactics. The two main reasons, obviously, being a lack of proper revenue management, and an unhealthy dependence on wholesaler business. Which, of course, stem huge problems in inventory management and price dynamicity. Owners and GMs of small chains or independent properties are realizing something should be done, so they hire anybody able to use Excel to revenue manage the hell out of that mess, only to dismiss recommendations. Meanwhile, large leisure chains are giving their RM and everybody in the distribution environment a hard time, with a thousand ridiculously named room types, sold by the dozen and stop-sold every other minute. Have you ever worked with/at Sandals?
 I really think that leisure revenue managers are the unsung heroes of the accommodation industry; although I expect this text to be of little help, at least may it serve as a tribute to their effort.
An incomparable setting
As opposed as their urban colleagues, resort revenue managers cannot count that much (or at all) on the standard RMS. Package rates, complex seasonality, virtual room types, inflexible wholesale contracts, honeymooners, and the list goes on. To further complicate things, many properties have such a unique setup and exceptional circumstances that standard industry tools are impossible to adopt. Should those hotels keep wrangling their collection of spreadsheets, or dismiss revenue management altogether? Absolutely not: the good news is that now business intelligence tools are cheaper and easier to use than ever, so many data sources can be combined and analyzed in order to obtain advanced forecasts, dynamic pricing recommendations, inventory optimization and much more.
 What I mean is:
It is possible to create a comprehensive, tailor-made and cheap RMS, by processing your own data on a BI solution.
Actually, a single BI tool can benefit all areas of a property, but today I’m addressing my revenue manager friends. These are the three hardest challenges they face, and my proposed data-driven solutions…
1 — Contracting nightmares
In most popular holiday locations around the world, long gone is the Golden Era of DMCs and resort hotels working in harmony, reliant on each other’s business. A global online distribution paradigm is the main culprit, although not the only one. We all know that OTAs take most of the available rooms on independent medium-to-small sized properties in popular destinations, while the largest TO’s prepay a great deal of rooms (up to 75–80% in certain cases) at well positioned branded hotels. The second situation may make sense for both parties (not ideal for the hotel, though), while the first one is hurting small players, no doubt.
It is a well-known fact that all kind of properties would profit much more from a balanced distribution, leaning on direct sales as much as possible (as long as the cost of acquisition is kept at bay). But that’s easier said than done, more so on the leisure world. I’ve been advocating for a long time the obvious solution, for both branded and independent properties: to move towards a dynamic price scenario. No proper hotel can (will) survive without a channel manager these days, so if a wholesaler wants to buy standard product, it should be able to connect to online distribution systems. As of last year, some hoteliers (even more wholesalers) would laugh at my proposition… Well, guess what: tour operators finally started embracing dynamic rates ( see an example here >> )
Perhaps fixed rates contracts are not dead yet, but hotels can certainly conduct a thorough evaluation via data analysis, in order to establish if/how each contract is profitable, which intermediaries can get lost, and so on. Moreover, hoteliers would be able to regain control over inventory and play with rate elasticity, to some extent. Only adequate analytics can help with that.
 However, direct integration with online distribution systems is the best bet for all parties. No more clunky stop-sales reports, no more boring negotiations, great opportunities to be discovered through data analysis. Hotels that are way too dependable on TO contracts, in many cases will finally realize that full occupancy does not necessarily mean positive bottom line; those critically depending on OTAs could find a way to generate higher and higher direct sales.
2 — Hello dynamic pricing!
Even if wholesalers are starting to accept dynamic contracts, many hoteliers I know are still against rate oscillations. It’s the usual case of “why fix what’s not broken?”, since their bottom line looks good so far. I get it: why bother if numbers are not red? They’ll be doing something only when the alarm rings, but by then it may be too late. The level of shortsightedness is higher when the hotelier is also the property’s owner… It’s the same type of hotelier that in the nineties told me “Why would I want a website with a booking system?”. None of those hotels exist today, but that’s not the point. The point is, why would you sell a room at 100 when you could’ve sell it at 200? Plain and simple math. Of course, it’s not easy to establish what’s the right price for the right segment every day, especially if you have many markets and even more room types to play with… but it’s way easier -and cheaper! — today than it was a year ago. There are no excuses: the market offers instruments >> that allow even tiny properties to establish the perfect pricing strategy, no large investments or specialized personnel required. I’ll go further on, stating that with a reasonable investment, that tiny property could even afford automated pricing strategies, just like the big chains!

3 — Inventory overriding
You know how it goes, especially in the luxury segment: the property has 40 rooms, but it might have 54 room types. Add to the mix the several extra options like sea view, butler service, honeymoon package and so on… Got the picture? It certainly makes sense from the commercial standpoint, but this “sukk” policy originated when there was no revenue management or instant booking confirmation. It’s a pain in the neck to manage, distribute and price that spiderweb of rooms and amenities, but it shouldn’t be. Suppose a property decides to allot 50% of its rooms to online distribution and the rest between TOs and direct: analytics will come handy not only for pricing options but it will be a must when it comes to inventory management. Instead of a convoluted collection of spreadsheets, how about a single interactive dashboard that would allow for simulations and disparate forecasting? And I mean artificial intelligence-powered forecasting, not the age-old pick-up based method, or similar crap based on historical data only. The kind of forecasting that would be able to foretell when/what and how much will spend every clustered segment… which in turn was clustered by an A.I. algorithm that’s included in your cheap, homemade RMS. So much so that with a system like that, the property with 40 real rooms could have 80 room types, each personalized by cluster, segment or even particular client. How about having a room called “Triple Coraline Seaview Exclusively for Mermaid Princesses”? I know my daughter will love it so much that I’ll be forced to return next year.
 Allow me to state it again: all this is possible with a reasonably priced BI system >> now. No need for the ultra-expensive RMS, that can’t go anywhere near something like that, anyway.
Conclusion
Everybody is talking about chatbots and bitching about OTAs, everybody is obsessed by RevPar and pissed by wholesalers’ attitudes. But the real problem is internal. Many hoteliers worry about not catching up with technology but are not willing to spend a dime on it, unless the competitor next door does it. Few of them realize that they already have the main ingredient to bake the profitable cake: data. Moreover, these days one does not have to be a Data Scientist or Mathematician to perform revenue management, nor possess an “analytical mind” as per job ads. And it makes no sense to spend hours and hours compiling spreadsheets and reports that are outdated (useless) the moment they’re finished. All that is needed is the oven (a BI system) and the recipe to convert data into a fragrant and profitable cake. What’s that recipe? The data-driven mindset that will turn you into a capable cook. Where are the ingredients? All the data collected by your PMS, plus any competitive intelligence you got yourself (from Expedia, Booking, webscrapping, etc.) or bought (from STR >>, for instance) and voila! Combined with a BI system, you’ve got yourself the basics for a tailor-made revenue management system.
If you are an experienced RM, and your tools are basically Excel and little more, please have a look at the alternative >> . If you’re a newbie, you might want to try the old methods first, to learn and realize later on the importance of using modern tools… But if you are the property owner, you’d better let your ready-made, A.I. powered, customized RMS do the revenue management for you, while you dedicate your time to the holidaymakers enjoying the premises. After all, human touch beats number crunching anytime.
Thanks for reading!
Marcello Bresin
Originally published at revva.pro on July 9, 2018.
",1703,Revenue Management,Marcello Bresin,https://medium.com/s/story/3-challenges-for-resort-revenue-managers-and-a-possible-solution-bc429c3d783c,"Which, of course, stem huge problems in inventory management and price dynamicity.
Owners and GMs of small chains or independent properties are realizing something should be done, so they hire anybody able to use Excel to revenue manage the hell out of that mess, only to dismiss recommendations.
Package rates, complex seasonality, virtual room types, inflexible wholesale contracts, honeymooners, and the list goes on.
Should those hotels keep wrangling their collection of spreadsheets, or dismiss revenue management altogether?
Absolutely not: the good news is that now business intelligence tools are cheaper and easier to use than ever, so many data sources can be combined and analyzed in order to obtain advanced forecasts, dynamic pricing recommendations, inventory optimization and much more.
Actually, a single BI tool can benefit all areas of a property, but today I’m addressing my revenue manager friends.
We all know that OTAs take most of the available rooms on independent medium-to-small sized properties in popular destinations, while the largest TO’s prepay a great deal of rooms (up to 75–80% in certain cases) at well positioned branded hotels.
It is a well-known fact that all kind of properties would profit much more from a balanced distribution, leaning on direct sales as much as possible (as long as the cost of acquisition is kept at bay).
I’ve been advocating for a long time the obvious solution, for both branded and independent properties: to move towards a dynamic price scenario.
No proper hotel can (will) survive without a channel manager these days, so if a wholesaler wants to buy standard product, it should be able to connect to online distribution systems.
Perhaps fixed rates contracts are not dead yet, but hotels can certainly conduct a thorough evaluation via data analysis, in order to establish if/how each contract is profitable, which intermediaries can get lost, and so on.
Hotels that are way too dependable on TO contracts, in many cases will finally realize that full occupancy does not necessarily mean positive bottom line; those critically depending on OTAs could find a way to generate higher and higher direct sales.
Even if wholesalers are starting to accept dynamic contracts, many hoteliers I know are still against rate oscillations.
Of course, it’s not easy to establish what’s the right price for the right segment every day, especially if you have many markets and even more room types to play with… but it’s way easier -and cheaper!
I’ll go further on, stating that with a reasonable investment, that tiny property could even afford automated pricing strategies, just like the big chains!
It’s a pain in the neck to manage, distribute and price that spiderweb of rooms and amenities, but it shouldn’t be.
Suppose a property decides to allot 50% of its rooms to online distribution and the rest between TOs and direct: analytics will come handy not only for pricing options but it will be a must when it comes to inventory management."
Deep study of a not very deep neural network. Part 3b: Choosing an optimizer,In this part we continue discussing optimizers. In the previous article I have shown on a very simple example how optimizers work and how…,"Deep study of a not very deep neural network. Part 3b: Choosing an optimizer
In this part we continue discussing optimizers. In the previous article I have shown on a very simple example how optimizers work and how the choice of parameters affect the way they walk towards the minima. Now it is time to test them on the MNIST dataset, and see which one is the best.
Ways to evaluate optimizer’s performance
The obvious way to name the best optimizer is by ranking them by the resulting validation accuracy by the end of the training. However, this is tricky. As you have seen in the previous part, some optimizers may reach the minima very quickly, whereas others will be moving very slowly. If you train for 100 epochs, you may not be sure that the value you see at the 100th epoch may not improve at the 101th epoch.
Moreover, the initialization of the weights of our neural network (which we will discuss in the future parts) implies randomness, therefore each time you train your networks, the final results may differ significantly for the same set of parameters. In order to make the results comparable it seems reasonable to average the results of each optimizer across some number of experiments.
Another point of randomness for many types of optimizers is computing and applying gradients. Remember SGD, which takes a random mini-batch to compute the error and decide, which direction to move in the next step. This causes situations, when in the middle of the training you see that the optimizer got lucky and received a very representative mini-batch which allowed him to greatly improve the accuracy across the entire validation set. But in the next step it receives a not very balanced mini-batch, the accuracy drops and never returns back to the maximum. There are ways to deal with such situations, which will be covered later in the series, but for now let’s assume they do not exist. In this case it is worth exploring, how far the maximum accuracy value observed over the entire training process is from the accuracy value obtained on the last epoch. If the difference is large, we may say that the training was unstable, and the maximum accuracy we observe for a particular configuration is just a one-time event, which we cannot rely on. On the other hand, if the observed maximum accuracy is close to the value on the last epoch, we may be pretty sure that the results of this particular configuration are reproducible.
To sum up, we will be testing out optimizers from two perspectives: what is the maximum accuracy averaged across five experiments, and how far this maximum is from the average accuracy value on the last epoch.
Experiment results — Quick facts
For your convenience, here’s the link to the entire rankings table. (I’ve separated the configurations, which have not converged at all or which have the accuracy values lower than 90, into separate sheet.)
In part 2, when testing different activation functions with RMSProp optimizer the maximum observed accuracy was 98.2 for SELU activation, and the maximum averaged accuracy was 98.02 for ELU activation, both on normalized data. This time by changing the optimizer and fine-tuning the parameters we’ve been able to significantly improve these results. The averaged maximum is now 98.17 for ELU activation with customized Adamax optimizer on normalized data, and the overall maximum is 98.37 for SELU activation with customized Adam on standardized data.
Now let’s examine the top-20 configurations, sorted by averaged max:

First thing that you may spot is that there are only two activations are present in this top-20: ELU and SELU. In fact, they fully occupy the top-100, with the only exception of SoftPlus occuring 3 times in the entire top-100.
Next, there is no single optimizer which can be called “the best”. The difference between the first and the 20th place is just 0.08%. Then, the majority of top-performing optimizers have non-default parameters. There are a few cases when optimizers trained with default parameters performed slightly better than the customized ones, but in general, we see that fine-tuning optimizer’s parameters leads to higher accuracy.
Also, there’s no clear preference in terms of data transformation type: both normalized and standardized types are equally present.
What is notable here is that the difference between the maximum achieved value and the value at the last epoch may differ very much, as well as the difference between the same averaged maximum and the maximum across five experiments. At the top there are both very stable configurations (e.g. #1, 3) and those for which these differences are relatively large (#12, 16).
Here’s how the general picture looks like:
Fig.1 Average maximum accuracy for various optimizers
We see that Adam-based optimizers perform higher, with Adamax showing the best results. I understand that averaging them is not very scientific. It is better to compare specific configurations of these optimizers. Nevertheless this picture is a good illustration of the general trends in the results of my experiment. Just to confirm this trend let’s look at the same averaged values, but only for ELU activation:
Fig.2 Average maximum accuracy for various optimizers with ELU activation
Adam and Adamax again are the best, followed by Adadelta and RMSProp.
Now let’s have a closer look at each optimizer and discuss what worked well for each of them.
SGD
Using the right momentum value significantly improves the accuracy: in our case the value of 0.9 demonstrated the best results both on normalized and standardized data. Nesterov momentum did not affect the accuracy very much, but in other tasks the difference may be larger. So the recommendation here is to always use Nesterov — it won’t make it worse.
This diagram shows the max accuracy averaged across all activation functions. This way you can get the idea of the general performance of various configurations of the optimizer and not be overwhelmed by the amount of columns.
Fig.3 Average maximum accuracy for SGD optimizer with various parameters
In regards of the data transformation types and activations, SGD performs better on standardized data with ELU and SELU activations.
Adagrad
This optimizer does not have any tunable parameters, so here we are able to dig one level deeper and show the performance for each activation:
Fig.4 Average maximum accuracy for AdaGrad optimizer
What is evident here is the optimizer’s strong preference of standardized data. And as before, ELU activation was the best, with ReLU seem to be coming second.
RMSProp
Here the default configuration was not the best:
Fig.5 Average maximum accuracy for RMSProp optimizer with various parameters
Higher RHO values lead to higher accuracy for both standardized and normalized data. However, when RHO is too high (0.999), the performance drops significantly. So the recommendation is to test the values in the range 0.9–0.99 when using this optimizer.
Adadelta
Very similar to RMSProp, RHO values in the range 0.9–0.99 are good:
Fig.6 Average maximum accuracy for AdaDelta optimizer with various parameters
Adam
This optimizer demonstrated very close results for almost all configurations. The differences between their performance becomes evident when we set the x-scale to a very short range:
Fig.7 Average maximum accuracy for Adam optimizer with various parameters
It is notable that the default configuration has shown the best results, and the optimizer performs better on standardized data. Also, setting AMSGrad to true generally improves the results.
Adamax
Fig.8 Average maximum accuracy for Adamax optimizer with various parameters
Here the default configuration was again the best, and the same story with the standardized data. The good choice from beta_1 is 0.9–0.95 and for beta_2 is 0.999.
Nadam
Fig.9 Average maximum accuracy for Nadam optimizer with various parameters
Here we don’t see that standardized data is prefered over the normalized one, and custom configurations work better than the default one. The optimal beta_1 values are the same as for Adamax, but for beta_2 the value of 0.99 is nearly as good as 0.999.
Which optimizer is the best?
In short, there is no one definite answer to this question. Our experiment demonstrated that it depends on how you adjust the parameters of the optimizer, and on how you configure your neural network. In our case Adam and Adamax optimizers with default configurations were better than the others, but this may not be the case in other tasks and with another data. As always, you should check several options and play with the optimizers’ parameters to see which ones work better for your case.
What is also worth noting the important thing to consider when choosing an optimizer may be its stability. The notebook on my github visualizes the process of training of each configuration, and from it you can see, for example, that SGD, despite having slightly lower accuracy, leads to more consistent results, than other optimizers. I have found a very good discussion of this issue:
https://www.quora.com/Why-do-the-state-of-the-art-deep-learning-models-like-ResNet-and-DenseNet-use-SGD-with-momentum-over-Adam-for-training
Probably, if we have given SGD some more training time and fine-tuned our network a bit more, eventually it would have reached much higher accuracy values.
It is time to wrap up our discussion of the optimizers, and move on to the next part, where we will talk about Learning rate hyperparameter, and how changing it may push the model’s accuracy even higher. See you in the next part!
I’m always happy to meet new people and share ideas, so if you liked the article, cosider adding me on LinkedIn.
Deep study of a not very deep neural network series:
Part 1: What’s in our data
Part 2: Activation functions
Part 3a: Optimizers overview
Part 3b: Choosing an optimizer
Part 4: How to find the right learning rate
Part 5: Dropout and Noise
Part 6: Weights initialization
Part 7: Regularization
Part 8: Batch normalization
Part 9: Size matters
Part 10: Merging it all together
",1589,Machine Learning,Rinat Maksutov,https://medium.com/s/story/deep-study-of-a-not-very-deep-neural-network-part-3b-choosing-an-optimizer-de8965aaf1ff,"The obvious way to name the best optimizer is by ranking them by the resulting validation accuracy by the end of the training.
Moreover, the initialization of the weights of our neural network (which we will discuss in the future parts) implies randomness, therefore each time you train your networks, the final results may differ significantly for the same set of parameters.
If the difference is large, we may say that the training was unstable, and the maximum accuracy we observe for a particular configuration is just a one-time event, which we cannot rely on.
On the other hand, if the observed maximum accuracy is close to the value on the last epoch, we may be pretty sure that the results of this particular configuration are reproducible.
In part 2, when testing different activation functions with RMSProp optimizer the maximum observed accuracy was 98.2 for SELU activation, and the maximum averaged accuracy was 98.02 for ELU activation, both on normalized data.
The averaged maximum is now 98.17 for ELU activation with customized Adamax optimizer on normalized data, and the overall maximum is 98.37 for SELU activation with customized Adam on standardized data.
There are a few cases when optimizers trained with default parameters performed slightly better than the customized ones, but in general, we see that fine-tuning optimizer’s parameters leads to higher accuracy.
Fig.1 Average maximum accuracy for various optimizers
We see that Adam-based optimizers perform higher, with Adamax showing the best results.
Fig.2 Average maximum accuracy for various optimizers with ELU activation
Using the right momentum value significantly improves the accuracy: in our case the value of 0.9 demonstrated the best results both on normalized and standardized data.
Fig.3 Average maximum accuracy for SGD optimizer with various parameters
Fig.4 Average maximum accuracy for AdaGrad optimizer
Fig.5 Average maximum accuracy for RMSProp optimizer with various parameters
Fig.6 Average maximum accuracy for AdaDelta optimizer with various parameters
Fig.7 Average maximum accuracy for Adam optimizer with various parameters
It is notable that the default configuration has shown the best results, and the optimizer performs better on standardized data.
Fig.8 Average maximum accuracy for Adamax optimizer with various parameters
Fig.9 Average maximum accuracy for Nadam optimizer with various parameters
Here we don’t see that standardized data is prefered over the normalized one, and custom configurations work better than the default one.
In our case Adam and Adamax optimizers with default configurations were better than the others, but this may not be the case in other tasks and with another data."
"VR, AR, AI, LBVR Summer Sizzles",There was so much compelling news in the world of VR and AR this July I couldn’t possibly cover it all with the usual depth and relish to…,"VR, AR, AI, LBVR Summer Sizzles
There was so much compelling news in the world of VR and AR this July I couldn’t possibly cover it all with the usual depth and relish to which readers have become accustomed. This is one of those cases where by the time I returned from vacation most of the July recap below wasn’t really news anymore but this news, taken as a group, is a testament to the continued entrepreneurial energy being lavished on immersive tech and illustrates how things come over the transom to me in no particular order.
From upper left, “Jurassic World” at Dave & Busters, the new AR Looking Glass, “The Art of Burning Man”, and the Lenovo Smart Display.
This story contains news and commentary about new tricks from old dogs, like Dave & Busters, and new companies we’ve never heard of, like Looking Glass, who are developing Digital Assistants, Artificial Intelligence, and new Motion Capture techniques.
Jurassic World LBVR at Dave & Busters
Dave & Busters launched Jurassic World VR motion platform experience at all 111 Dave & Buster’s domestic locations. I spent an hour in Dave & Buster’s in Times Square last Wednesday and went through the simulation twice. It’s terrific. Best 5 minutes of VR $5 will ever buy you. D&B is not terribly crowded, as you would expect, at 9pm on a weeknight, but there was a crowd of thirty people waiting to board the bonafide theme park quality ride in the midst of the massive arcade. People loved it. Grandmothers and six-year-olds. Loved. It. Screaming their heads off and very, very present in Jurassic World. And they didn’t shoot anything but pictures.
Kudos to Dave & Buster’s for cooking the whole deal up with Universal Studios, The Virtual Reality Company, which did the content, and motion platform integration, and to VRStudios which provided the VRcade Attraction Management Platform.

I spent some time wandering around D&B and playing games after experiencing Jurrasic World, courtesy of a very generous D&B manager. None of these amusements compared to the experience just had. The VRCompany informs me they have already had 0ver 600 thousand patrons. Doing some quick math, that’s a gross of three million dollars and counting. No wonder people are excited about Location Based VR.
Meet Looking Glass — AR on Your Desktop Without Glasses
I had a chance to meet with Looking Glass Factory co-founder and CEO Shawn Frayne who gave me a demo of Looking Glass’ Holographic desktop Display for volumetric content. The device, targeting 3D content creators, is available for pre-order in limited quantities on Kickstarter until August 24th. I usually don’t write about Kickstarter campaigns, having been burned more than once, but in this case, I tried the product and recommend it without hesitation. It’s compatible with 3D creation programs like Maya, Unity, and Blender.
Look ma! No glasses! It’s a real Hologram on my desk.
The AR display allows users to preview 3D objects, characters, and models and show them to clients, coworkers, or audiences without a headset. Most importantly, with its 50-degree field of view, a number of people can view the 3D simultaneously. Frayne told me the image is created with light field and volumetric display technology which creates 45 views of the object. Coupled with a Leap Motion Controller, the 3D objects displayed can be manipulated with the users’ hands.
Looking Glass passed its Kickstarter goal of $50,000 in a couple of hours. $509,969 has been pledged so far. Models will start shipping in September and continue as inventory is replenished from China through December. After the pre-order window is over, the Standard (8.9 inch) model is $600 and the Large (15.9 inch) goes for $3,000.
The Brooklyn, NY startup has raised $13 million in funding from investors like Brad Feld’s Foundry Group, Lux Capital, SOSV, and Uncork Capital.
Sanar — Intel — Smithsonian Create Virtual Burning Man
Intel, The Smithsonian and Sansar have teamed up to bring the art of Burning Man into VR.
Jason Gholston and Smithsonian American Art Museum (SAAM) Curator Nora Atkinson met me in a virtual replica in Sansar of SAAM’s Renwick Gallery in Washington, DC, to share their current exhibit, “No Spectators: The Art of Burning Man”. Unlike the exhibit in IRL this show will never close. Intel provided a valuable assist by processing the photogrammetric capture of the pieces in the exhibit, while massive virtual world Sansar provided the VR platform for the virtual gallery. I would note you can enter Sansar on your laptop. You do not need a VR rig to see this, although that is the best experience.
Burning Man is an annual event in the western United States at Black Rock City — a temporary city erected in the Black Rock Desert of northwest Nevada, approximately 100 miles north-northeast of Reno. I’ve always wanted to go. The exclusive annual event is a veritable nerd Woodstock, where art and technology meet in the desert for exhibitions, events, and community. One of the striking qualities of the exhibition is the scale of the art there. People cavort in colorful costumes around the large-scale art and working conceptual contraptions. In addition, many art pieces are mobile and/or interactive and some even travel around to many of the hundreds of campsites formed by the convergence of thousands of RVs and campers. Tickets to Burning Man are sold by lottery on their website. Many are re-sold on StubHub, Craigslist, and eBay for hundreds of dollars over face value. If you’re inclined to pay a premium, you can still go later this month: Burning Man takes place on the 26th of August through the 4th of September.
In VR, the art doesn’t have to be trapped in gallery but can reside instead in its native habitat in the Nevada desert.
With support from Intel and Sansar (which is part of Linden Lab, which created and operates Second Life), SAAM was able to virtually recreate pieces from the exhibition in striking detail, at realistic scale, heretofore unachievable without being present at Burning Man.
Photogrammetry was used to capture the museum pieces, which include a sixty-foot tall metal statue of a woman, and a fully operational antique bus converted into a cinema. Photogrammetry is a process of capturing spaces and things with thousands of still photos taken from numerous angles. The images are crunched by a program that stitches the photos together into a detailed 3D space or object, which can be edited and enhanced.
Behind-the-scenes videos from the making of this experience, developed by Intel’s in-house creative team, Agency Inside, are available on YouTube:




Lenovo’s New Smart Home Device Delivers Google’s Suite Of Digital Assistants
Lenovo opened a new front in the virtual assistant war between the Google Assistant and Amazon Alexa. Enter the new Lenovo Smart Display with Google Assistant, which hit the shelves of major retailers (but not Amazon) last week. Like the Amazon Echo Show, the Smart Display addresses the”pain point” of speaker-only assistants. “We think the Smart Display can be the home’s IoT control center,” Lenovo’s Carly Okerfelt, Senior Product Manager, told me during a demo July 19th.
With Google Assistant as the interface you can control your nest with Nest.
The Smart Display is sleek. There’s no battery. No keyboard. It’s not a pad. It’s a smart speaker with a screen. You use your voice for just about everything from, setting calendar reminders, making to-do lists, pulling up old Google Photos, and listening to music or podcasts. If you’ve got a Nest enabled home, you’ll be able to access the remote cameras, shut off the lights, and set the thermostat.
Lenovo extols use cases like finding recipes for cooking, listening to the news, pulling up directions, and easily connecting to Duo, Google’s answer to Facetime. As a kitchen assistant, the Lenovo Smart Display utilizes Google apps you may never have heard of, like Google Shopping Lists, and Google Expenses. Reviewers from The Verge, Cnet, Digital Trends, and TechRadar singled out the kitchen assistant abilities of the Smart Display. After all, how can you scroll to a Youtube video or a recipe with your hands covered in cake batter?
Let your voice do the scrolling while your hands are covered with flour.
The screen can provide a richer experience around using a voice assistant. Ask for nearby restaurants and the Lenovo Smart Display shows you a list of options while the embedded Google Assistant reads them off. You can use your finger or your voice to get more specific venue information, such as directions and pictures. You can also use Lenovo’s display to make video calls or watch videos on YouTube.

The stylish Smart Display comes in two flavors, a $249 10"" screen or an 8"" priced at $199. The 7"" Echo show lists for $229, but Amazon is the only place selling it at that price. It has recently been discounted at Best Buy and Macy’s by $100 to $129.
At the same time, Mayfield Robotics announced this week the premature passing of its uber disappointing Kuri, the doe-eyed $800 home robot, basically Smart Display on wheels. I feel bad for the first adopters who took a chance on Kuri, believing the company’s claim it had developed a robust ecosystem for new apps.
Happier times. Two months ago. 2 May 2018; Kaijen Hsiao, CTO, Mayfield Robotics with Kuri the robot on the AutoTech & Talk Robot stage during day two of Collision 2018 in New Orleans. (Photo By Seb Daly/Sportsfile via Getty Images)
Human Interact Lab Is Building The AI Platform Of The Future
Yao Huang, CEO of Venture Capital Group and Incubator The Hatchery, is an International Business Consultant with strong ties to China. She has an impressive record of brokering deals between large Western companies and the Chinese government.
Yao and I were recently introduced by a mutual friend, and she was kind enough to give me a crash course in AI two weeks ago over soup dumplings in Manhattan. Yao is personally leading The Hatchery’s investment in Chicago-based, Human Interact, and acting as its CEO.
Yao Huang, CEO of Hatchery VC and Human Interact, which is creating AI applications for numerous business vertical.
Human Interact is building an AI platform with tech that originated at Microsoft. This short video explains it better than I can (link). And this short video (link) shows off Human Interact’s amazing AI-enabled VR experience, Starship Commander, which Human Interact developed to demonstrate the power of the platform they are building. It’s the dialogue, the ability to talk to the character that’s so interesting and potentially disruptive to so many verticals.
Holosuit Offers The First Full Body Motion Capture Suit & Gloves
Finally, I just gotta give a shout out to Kaaya Tech, led by CEO Harsha Kikkeri, who has finally made a full body mo-cap suit, including jacket or jersey, pants, gloves and haptic feedback, for cheap. The whole suit is $1,399, and the gloves alone $99 and will interface via Bluetooth or WiFi with any computer, tablet, smartphone or XR device. Their successful Kickstarter just ended, but the gear is for sale at their website https://www.holosuit.com/h_s_shop/.
With CEO of Kaayatech Harsha Kikkeri checking out the data glove and mo-cap suit.
Kikkeri, a former Microsoft executive who worked in the robotics division, has bootstrapped the entire project. In addition to the traditional B-to-B market for prosumer mo-cap solutions, the company envisions diverse use cases for training, medicine, and leisure. Kaaya Tech has partnerships in place for factory training, military simulation training and disaster response, surgery and paramedic training, a cricket academy and game development.
“The controller remains the Achilles heel of mixed reality, with cumbersome and unnatural handheld devices only tracking your hand movement in a limited range,” said Kikkeri. “We designed HoloSuit from the ground up to provide an unparalleled immersive, full-body and bidirectional AR/VR/MR experience.” Rocking my High Fidelity avatar with a full body mo-cap suit is something I can’t wait to try.
This post was originally featured on Forbes.com on August 1, 2018
",1857,Virtual Reality,Charlie Fink,https://virtualrealitypop.com/vr-ar-ai-lbvr-summer-sizzles-4c289829011f,"This is one of those cases where by the time I returned from vacation most of the July recap below wasn’t really news anymore but this news, taken as a group, is a testament to the continued entrepreneurial energy being lavished on immersive tech and illustrates how things come over the transom to me in no particular order.
From upper left, “Jurassic World” at Dave & Busters, the new AR Looking Glass, “The Art of Burning Man”, and the Lenovo Smart Display.
This story contains news and commentary about new tricks from old dogs, like Dave & Busters, and new companies we’ve never heard of, like Looking Glass, who are developing Digital Assistants, Artificial Intelligence, and new Motion Capture techniques.
Dave & Busters launched Jurassic World VR motion platform experience at all 111 Dave & Buster’s domestic locations.
Kudos to Dave & Buster’s for cooking the whole deal up with Universal Studios, The Virtual Reality Company, which did the content, and motion platform integration, and to VRStudios which provided the VRcade Attraction Management Platform.
I had a chance to meet with Looking Glass Factory co-founder and CEO Shawn Frayne who gave me a demo of Looking Glass’ Holographic desktop Display for volumetric content.
Coupled with a Leap Motion Controller, the 3D objects displayed can be manipulated with the users’ hands.
Intel, The Smithsonian and Sansar have teamed up to bring the art of Burning Man into VR.
Intel provided a valuable assist by processing the photogrammetric capture of the pieces in the exhibit, while massive virtual world Sansar provided the VR platform for the virtual gallery.
With support from Intel and Sansar (which is part of Linden Lab, which created and operates Second Life), SAAM was able to virtually recreate pieces from the exhibition in striking detail, at realistic scale, heretofore unachievable without being present at Burning Man. Photogrammetry was used to capture the museum pieces, which include a sixty-foot tall metal statue of a woman, and a fully operational antique bus converted into a cinema.
Lenovo’s New Smart Home Device Delivers Google’s Suite Of Digital Assistants
Enter the new Lenovo Smart Display with Google Assistant, which hit the shelves of major retailers (but not Amazon) last week.
Like the Amazon Echo Show, the Smart Display addresses the”pain point” of speaker-only assistants.
“We think the Smart Display can be the home’s IoT control center,” Lenovo’s Carly Okerfelt, Senior Product Manager, told me during a demo July 19th.
You use your voice for just about everything from, setting calendar reminders, making to-do lists, pulling up old Google Photos, and listening to music or podcasts.
Lenovo extols use cases like finding recipes for cooking, listening to the news, pulling up directions, and easily connecting to Duo, Google’s answer to Facetime.
As a kitchen assistant, the Lenovo Smart Display utilizes Google apps you may never have heard of, like Google Shopping Lists, and Google Expenses.
Reviewers from The Verge, Cnet, Digital Trends, and TechRadar singled out the kitchen assistant abilities of the Smart Display.
Ask for nearby restaurants and the Lenovo Smart Display shows you a list of options while the embedded Google Assistant reads them off.
At the same time, Mayfield Robotics announced this week the premature passing of its uber disappointing Kuri, the doe-eyed $800 home robot, basically Smart Display on wheels.
Yao Huang, CEO of Hatchery VC and Human Interact, which is creating AI applications for numerous business vertical.
And this short video (link) shows off Human Interact’s amazing AI-enabled VR experience, Starship Commander, which Human Interact developed to demonstrate the power of the platform they are building."
Stylometric Analysis: Satoshi Nakamoto,Abstract:,"Stylometric Analysis: Satoshi Nakamoto

Abstract:
Natural Language Processing tools were applied to the Satoshi Nakamoto’s Bitcoin paper to compare it to numerous cryptocurrency-related papers in an attempt to identify the true identity of the unknown Satoshi Nakamoto. There are two parts to the paper; the first part is stylometric analysis on the linguistic features generated and n-grams of each document in the corpus consisting of the relevant literature listed on Satoshi Nakamoto Institute and using machine learning models of the linguistic features to predict an author/authors on the Satoshi Nakamoto’s Bitcoin paper and his personal email texts. The second part is semantic similarity analysis where the content of each document in the corpus is compared in terms of semantic similarity number using the built-in functions in spaCy and gensim. The results from the two parts suggested which author/authors in the corpus are linguistically and semantically similar to Satoshi Nakamoto.
1 Problem Statement
Bitcoin has been a long-lasting peer-to-peer digital cryptocurrency for people who are skeptical of a current monetary system that is heavily controlled by third parties such as central and commercial banks. Bitcoin has come to the world’s attention not just because of the cryptocurrency itself, but also because of an algorithm behind Bitcoin, which is called blockchain. The real identity of Satoshi Nakamoto, who is known as the creator of Bitcoin and blockchain, has been an intensely debated topic among the members of the Bitcoin community. Since Satoshi Nakamoto and people involved in the early stage of this Bitcoin project only interacted via email, nobody has seen and interacted with him in real person; therefore, his identity is still unknown. Satoshi Nakamoto, who had refused to reveal himself to the public due to his privacy concern, left a few write-ups; one of the write-ups is the paper called “Bitcoin: A Peer-to-Peer Electronic Cash System” describing how Bitcoin works using blockchain and another is a few email exchanges between Satoshi and the people who were involved in the early stage of Bitcoin.
The paper examines to answer one question regarding Satoshi Nakamoto, “Who is/are linguistically and semantically similar to Satoshi Nakamoto?” The paper applies stylometric and semantic similarity analyses on the relevant literature listed on Satoshi Nakamoto Institute, the Bitcoin paper, and Satoshi’s email exchanges to find out who is/are linguistically and textually similar to Satoshi Nakamoto. Stylometric analysis is an analysis of linguistic style used to suggest whether a text belongs to a certain author based on linguistic features. Semantic similarity analysis is an analysis used to indicate whether the content or meaning of a text is similar to the content of another or not.
The true identity of Satoshi Nakamoto is important to the Bitcoin community. Satoshi has been known to have approximately 1 million Bitcoins or 7 percent of the total Bitcoin supply. He has the strongest influence on the economy of Bitcoin; if he decides to sell some of his Bitcoins into the market, the market will respond to the change by possibly devaluing all the existing Bitcoins. Furthermore, manifesting the true identity of Satoshi Nakamoto can bring many upgrades to blockchain and new applications of blockchain in fields other than finance can be introduced.
2 Data
The data were gathered using a python module called Article. The literature listed on Satoshi Nakamoto Institute only in a format of HTML were collected using the module. The authors of the collected literature have been known as possible candidates for Satoshi Nakamoto in the Bitcoin community such as Hal Finney, Ian Grigg, Nick Szabo, Timothy C. May, and Wei Dai. A total of 29 documents was collected, including 6 Hal Finney texts, 2 Ian Grigg texts, 16 Nick Szabo texts, 2 Timothy C. May texts, 1 Wei Dai text, and 2 Satoshi Nakamoto texts, where one of them is the Bitcoin paper, and another is his email exchange texts with others. The texts of every author in the data except Satoshi Nakamoto were combined into one single text file. The training corpus contains a single combined text of every author except Satoshi Nakamoto and the test corpus contains two individual Satoshi Nakamoto’s texts.
3 Methodology
The stylometric analysis has three components, linguistic features, classification algorithms, and n-grams. A total of 10 linguistic features was generated and used to compare from author to author in the corpuses. These features were generated using sent_tokenize function and stopwords built in the nltk module. The descriptions of the features are provided in Table 1. Classification algorithms such as Support Vector Machine, Random Forest, and Gaussian Naive Bayes, were used to classify the Satoshi Nakamoto’s Bitcoin paper and his email exchanges as one of the authors of the training corpus. The algorithms were trained with the features of the authors except those of Satoshi Nakamoto and were all implemented in python using the scikit-learn module.
In addition, n-grams of each document in the corpuses, where n is from 1 to 4, were produced using the nltk module. The tokens in each document were lemmatized using nltk.WordNetLemmatizer to prevent the same word from being counted as another word due to plurality. First, 1-gram, called uni-gram, was generated with and without stopwords. Afterwards, bigram, trigram, and quadgram were created, compared, and analyzed to see if Satoshi Nakamoto repeats a certain pattern of words in order and other authors use the same pattern in their writings.
The semantic similarity analysis was done using the built-in functions that compute semantic similarity in spaCy and gensim implemented in python. In spaCy, .similarity() method was used to compare the content of one document to that of another and determine the similarity using a number between 0 and 1, where 0 means the two documents are not related to each other and 1 means that the contents of the two documents are identical. spacy.load(‘en_core_web_lg’), which consists of 300-dimensional word vectors trained on Common Crawl with GloVe and 1.1m keys and 1.1m unique vectors (300 dimensions), was used. In gensim, similarities.MatrixSimilarity() was used in computing the cosine similarity using a number between -1 and 1, where the closer to 1 the number is, the more similar two documents are to each other in terms of content.
4 Results
According to the classification algorithms in Table 3, they all predicted that Nick Szabo is linguistically similar to Satoshi who had written the Bitcoin paper and Ian Grigg is linguistically similar to Satoshi who had exchanged the emails. In Table 4, there are two unigrams, (‘would’, 31) and (‘one’, 29) in Satoshi’s email exchanges. The word ‘would’ is used by Hal Finney 28 times and the word ‘one’ is used by Nick Szabo 199 times. There is one unigram, the word ‘contract’, commonly used by Ian Grigg and Nick Szabo.
From spaCy (Table 5), Wei Dai has the highest similarity score to the Bitcoin paper and Hal Finney has the highest similarity score to Satoshi’s email exchanges. From gensim (Table 6), Timothy C. May has the highest similarity score to the Bitcoin paper and Ian Grigg has the highest similarity score to Satoshi’s email exchanges. An unusual result is that Ian Grigg has a similarity score of .99996 to Satoshi’s email exchanges (rounded up to 1.0 in the table).
5 Conclusion
Based on the results, Satoshi who had written the Bitcoin paper may not be the same Satoshi who had exchanged emails. Satoshi Nakamoto may possibly be more than one person; Satoshi Nakamoto is a pseudonym for a team of computer scientists and cryptographers who were involved in creating Bitcoin and blockchain. Nick Szabo and Ian Grigg are the two authors who are linguistically similar to Satoshi Nakamoto in the Bitcoin paper and his email texts, respectively. In addition, Wei Dai and Timothy C. May are two potential candidates for the Bitcoin paper in terms of semantic similarity. Hal Finney and Ian Grigg are two possible candidates for Satoshi’s email exchanges. Since it is a known fact that Hal Finney had interacted with Satoshi Nakamoto via email, Hal Finney should not be included in the list of possible candidates for Satoshi who exchanged emails; Ian Grigg is linguistically and semantically similar to Satoshi Nakamoto. Therefore, the possible candidates for Satoshi Nakamoto are Nick Szabo, Ian Grigg, Wei Dai, and Timothy C. May.
6 Discussion
Satoshi Nakamoto used the phrase “proof-of-work” repeatedly throughout the Bitcoin paper and Nick Szabo is the only author of the training corpus who used the same exact phrase in his blog post called Bit gold. It supports a theory that Nick Szabo is very close to Satoshi in terms of linguistic style. The document distances of every author of the corpus in 2-dimensional spaces using multidimensional scaling, MDS on sklearn were visualized. In Pic 1, the distance between Ian Grigg and Nick Szabo is the shortest, suggesting that Ian Grigg and Nick Szabo are closely related to each other, which might not be a coincidence. Wei Dai and Timothy C. May are far away from each other and Nick Szabo and Ian Grigg, possibly suggesting that Wei Dai and Timothy C. May are not strong candidates for Satoshi Nakamoto compared to Nick Szabo and Ian Grigg.
7 Future Work
The classification algorithms trained with only 5 data points consisting of 10 features are worrisome. In an ideal data science world, machine learning models need to be trained with a bigger training sample with cross-validation. Along with the absolute frequency of n-grams, the relative frequency of n-grams can be added to the study. In addition, Craig Steven Wright, who claimed himself as Satoshi Nakamoto, can be added, if possible, as one of the authors of the training corpus because the algorithms and semantic similarity allow comparing him to the authors of the current training corpus. It would be interesting to see if he would have outperformed Nick Szabo and Ian Grigg, who are the two strongest candidates for Satoshi Nakamoto.
Revised_ANLY580_ FINAL PAPER.pdf
Edit descriptionhttps://drive.google.com/file/d/1T3ITw4KL-QKy1zY3fpM1yTZ799HZLoPS/view?usp=sharing
p.s: I just realized that I cannot insert tables here so I shared the paper on Google Drive instead.
",1689,Bitcoin,Michael Chon,https://towardsdatascience.com/stylometric-analysis-satoshi-nakamoto-294926cdf995,"There are two parts to the paper; the first part is stylometric analysis on the linguistic features generated and n-grams of each document in the corpus consisting of the relevant literature listed on Satoshi Nakamoto Institute and using machine learning models of the linguistic features to predict an author/authors on the Satoshi Nakamoto’s Bitcoin paper and his personal email texts.
Satoshi Nakamoto, who had refused to reveal himself to the public due to his privacy concern, left a few write-ups; one of the write-ups is the paper called “Bitcoin: A Peer-to-Peer Electronic Cash System” describing how Bitcoin works using blockchain and another is a few email exchanges between Satoshi and the people who were involved in the early stage of Bitcoin.
The authors of the collected literature have been known as possible candidates for Satoshi Nakamoto in the Bitcoin community such as Hal Finney, Ian Grigg, Nick Szabo, Timothy C.
May, and Wei Dai. A total of 29 documents was collected, including 6 Hal Finney texts, 2 Ian Grigg texts, 16 Nick Szabo texts, 2 Timothy C.
May texts, 1 Wei Dai text, and 2 Satoshi Nakamoto texts, where one of them is the Bitcoin paper, and another is his email exchange texts with others.
Classification algorithms such as Support Vector Machine, Random Forest, and Gaussian Naive Bayes, were used to classify the Satoshi Nakamoto’s Bitcoin paper and his email exchanges as one of the authors of the training corpus.
According to the classification algorithms in Table 3, they all predicted that Nick Szabo is linguistically similar to Satoshi who had written the Bitcoin paper and Ian Grigg is linguistically similar to Satoshi who had exchanged the emails.
From spaCy (Table 5), Wei Dai has the highest similarity score to the Bitcoin paper and Hal Finney has the highest similarity score to Satoshi’s email exchanges.
May has the highest similarity score to the Bitcoin paper and Ian Grigg has the highest similarity score to Satoshi’s email exchanges.
Nick Szabo and Ian Grigg are the two authors who are linguistically similar to Satoshi Nakamoto in the Bitcoin paper and his email texts, respectively.
Hal Finney and Ian Grigg are two possible candidates for Satoshi’s email exchanges.
Therefore, the possible candidates for Satoshi Nakamoto are Nick Szabo, Ian Grigg, Wei Dai, and Timothy C.
Satoshi Nakamoto used the phrase “proof-of-work” repeatedly throughout the Bitcoin paper and Nick Szabo is the only author of the training corpus who used the same exact phrase in his blog post called Bit gold.
May are far away from each other and Nick Szabo and Ian Grigg, possibly suggesting that Wei Dai and Timothy C.
May are not strong candidates for Satoshi Nakamoto compared to Nick Szabo and Ian Grigg."
Jobs Every Company Will Be Hiring for By 2020,The Hottest Jobs for Year 2020,"Jobs Every Company Will Be Hiring for By 2020
The Hottest Jobs for Year 2020
Source: https://pixabay.com/en/users/kellepics-4893063/
The job market is changing and changing quickly, according to a report by the World Economic Forum, technological advances and demographic changes may lead to the loss of about 5 million jobs by 2020. The report calls this the fourth industrial revolution, and states that administrative and white-collar office jobs ( work that’s performed in an office or other administrative setting) will be gone. Some jobs will be gone completely, others will blossom and jobs that don’t exist today will become the norm.
In the future current jobs today will change. We are already seeing some of the changes today with self driving cars possibly replacing taxi and Uber drivers, self driving trucks like Tesla trucks, possibly replacing truck drivers , and Amazon using robots in it’s warehouses and delivering packages with drones.
There are many people and sources out there trying to predict what the job market will look like in the future. In this article I will talk about the jobs in my opinion that will be needed in the future based off of articles I’ve read, the changes I see in my company, and other companies. Spoiler alert, jobs that fall under the computer and mathematical occupations will grow and will be needed. Let’s get started.
Data Analyst / Scientist
The world is producing more and more data every year. Scientist estimate the total data stored as of 2011 was 295 Exabyte’s of data. David Reinsel estimates the amount of data being produced to reach 163 Zettabytes by 2025, and as of 2016 IBM reported about 90% of the worlds data had been created within the past 2 years.
IBM Predicts Demand For Data Scientists Will Soar 28% By 2020
Data Analytics and Data Science are interdisciplinary fields of scientific methods, processes, algorithms, and systems to extract knowledge or insights from data in various forms, either structured or unstructured. Both fields are very similar and overlap sometimes but not exactly the same for example according to GlassDoor.com, the average salary for Data Analyst is $65,470 while the average salary for a Data Scientist is $118,709, this isn’t the only difference of course. Data analyst are excellent at Structured Query Language (SQL) and can use it and regular expressions to slice up data. Data scientists on the other hand can do everything a data analyst can, but they usually have a strong foundation in modeling analytics, math , statistics and computer science. With all of this data companies will need someone to make sense of it all.

Machine Learning Engineer
Machine learning is a field in computer science and a particular application of data science, that gives computers the ability to “learn” from data and make predictions. It is a subset of artificial intelligence (AI). A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at task T, as measured by P, improves with experience E.
Already we are seeing computers outperform their human counter parts from AI beating the best players in games like GO, Chess, and Poker, to them outperforming lawyers in finding issues with Non-disclosure agreements (NDA’s), and identifying cancerous cells with more accuracy than epidemiologists. You can read a summary about AI’s improvements here.
Artificial Intelligence (AI) Break Throughs
AI is here and improvingmedium.com
So, is machine learning the job of the future, well according to Jeff Nelson the answer is yes and no. He believes right now and possibly the next 20 years, we will see a major need for machine learning and data science specialists to show the applications of machine learning to businesses and help them increase profits, but eventually like any technology it will almost certainly become just another core technology in the business. I would say then by 2020 it is probably a good chance this job will be in high demand.
Software Developer
This occupation should be no surprise that it will be one of the hottest jobs in year 2020. Software developer includes programmers, computer scientists, computer engineers, augmented reality developers and Information Technology just to name a few. Software developers are the people who create the software people use every day, this includes the research, design, programming, and testing of the computer software.

As Steve Tickle on Quora.com put it, “software cannot become standardized as software solves problems, unless all problems become standardized. This will not happen.” So there will always be a need for software developers. Software Developers are among one of the fastest growing jobs out there. The projected future growth for this occupation according to the Bureau of Labor Statistics is 24% through 2022 , which is much faster growth than average.
Database Administrator
Database administrators (DBA) will probably become more in-demand by 2020, since companies are moving towards software that include Artificial Intelligence. “Having well-maintained databases is really the secret to allowing those products to work effectively” according to Nate Meneer who is a Forrester researcher. The database administrator maintains the integrity, performance, and security of the database. The DBA also does troubleshooting of any issues on behalf of the users. According to glassdoor.com, the average base pay is $77,428/year as of 2018.
Cloud Engineer
Many companies are moving to the “cloud” and most of those companies are choosing multiple vendors like Salesforce.com, Oracle, Amazon , and Microsoft etc. Cloud engineering is an IT professionals responsibility for any technological duties associated with cloud computing. Cloud computing is definitely a buzz a word which doesn’t mean your data is in the fluffy white things in the sky, but that your data is stored, and maintained somewhere else (not on premise) and the networks of servers find the data that you need and sends it to you. Cloud engineer salaries according to glassdoor.com is $95000 /year as of 2018.

Specialized Sales People
Specialized sales people will be needed to explain a company’s product or service to many different clients. This includes current businesses and new leads that the company has not worked with before.
Senior Managers
These managers will be needed to help lead the changes within their organization. A manager is a person responsible for controlling or administering all or part of a company. There are already changes within companies that are happening from reskilling employees, to moving all data to the cloud within the organization.
Product Designers (Creativity Jobs)
Many jobs will be automated, but creative jobs will still require a human being. This includes car designers, app designers, and manufacturing goods.

Human Resources and Organizational Development
Human Resources (HR), will be needed to retrain employees for other occupations within their organization, seeing as how some current occupations will be gone all together. Human resources will also be needed to hire people for the new high in demand skills within the market.
The future looks bright for the jobs listed in this article, and I expect to see them on the rise in the next few years. I hope you all enjoyed this article, thanks for reading. If you truly enjoyed please leave a few claps to show it !
Let me know what job you think will be in high demand in the future.
If you enjoyed this article, please leave many claps on here and share it, thanks for taking the time out of your busy day to read this article !
Check Out the following for content / videos on Computer Science, Algorithm Analysis, Programming and Logic:
YouTube Channel:
randerson112358: https://www.youtube.com/channel/UCaV_0qp2NZd319K4_K8Z5SQ
compsci112358:
https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA
Website:
http://everythingcomputerscience.com/
Video Tutorials on Recurrence Relation:
https://www.udemy.com/recurrence-relation-made-easy/
Video Tutorial on Algorithm Analysis:
https://www.udemy.com/algorithm-analysis/
Twitter:
https://twitter.com/CsEverything
Resources:
Difference between Data Analyst and Data Scientist
Data Analyst vs. Data Scientist — Take a closer look at what’s the difference between the two hot jobs of global…www.dezyre.com
IBM Predicts Demand For Data Scientists Will Soar 28% By 2020
Jobs requiring machine learning skills are paying an average of $114,000. Data scientist jobs pay an average of…www.forbes.com
Software Developers : Occupational Outlook Handbook: : U.S. Bureau of Labor Statistics
Software developers are the creative minds behind computer programs. Some develop the applications that allow people to…www.bls.gov
Here Are the 7 Fastest-Growing and Best Tech Jobs of the Future
Find out which tech jobs are hot right now and have the staying power to grow in the future. Here’s a list of tech…www.thebalance.com
The 10 IT jobs that will be most in-demand in 2020 | ZDNet
As more companies undergo digital transformation, in-demand IT roles will shift by 2020 to include positions focused on…www.zdnet.com
",1445,Data Science,randerson112358,https://medium.com/s/story/jobs-every-company-will-be-hiring-for-by-2020-f77a000a9cb5,"There are many people and sources out there trying to predict what the job market will look like in the future.
In this article I will talk about the jobs in my opinion that will be needed in the future based off of articles I’ve read, the changes I see in my company, and other companies.
Both fields are very similar and overlap sometimes but not exactly the same for example according to GlassDoor.com, the average salary for Data Analyst is $65,470 while the average salary for a Data Scientist is $118,709, this isn’t the only difference of course.
So, is machine learning the job of the future, well according to Jeff Nelson the answer is yes and no.
He believes right now and possibly the next 20 years, we will see a major need for machine learning and data science specialists to show the applications of machine learning to businesses and help them increase profits, but eventually like any technology it will almost certainly become just another core technology in the business.
Software Developers are among one of the fastest growing jobs out there.
Database administrators (DBA) will probably become more in-demand by 2020, since companies are moving towards software that include Artificial Intelligence.
According to glassdoor.com, the average base pay is $77,428/year as of 2018.
Cloud engineer salaries according to glassdoor.com is $95000 /year as of 2018.
There are already changes within companies that are happening from reskilling employees, to moving all data to the cloud within the organization.
Human Resources (HR), will be needed to retrain employees for other occupations within their organization, seeing as how some current occupations will be gone all together.
Human resources will also be needed to hire people for the new high in demand skills within the market.
The future looks bright for the jobs listed in this article, and I expect to see them on the rise in the next few years.
randerson112358: https://www.youtube.com/channel/UCaV_0qp2NZd319K4_K8Z5SQ
https://www.youtube.com/channel/UCbmb5IoBtHZTpYZCDBOC1CA
https://www.udemy.com/algorithm-analysis/
Data Scientist — Take a closer look at what’s the difference between the two hot jobs of global…www.dezyre.com
Jobs requiring machine learning skills are paying an average of $114,000.
Data scientist jobs pay an average of…www.forbes.com"
Top 20 Web Crawler Tools to Scrape the Websites,Web crawling (also known as web scraping) is widely applied in many areas today. It targets at fetching new or updated data from any…,"Top 20 Web Crawler Tools to Scrape the Websites
Web crawling (also known as web scraping) is widely applied in many areas today. It targets at fetching new or updated data from any websites and store the data for an easy access. Web crawler tools are getting well known to the common, since the web crawler has simplified and automated the entire crawling process to make web data resource become easily accessible to everyone. Using a web crawler tool will set free people from repetitive typing or copy-pasting, and we could expect a well-structured and all-inclusive data collection. Additionally, these web crawler tools enable users to crawl the world wide web in a methodical and fast manner without coding and transform the data into various formats conforming to their needs.
In this post, I’d propose top 20 popular web crawlers around the web for your reference. You may find the most suited web crawler that’s tailored to your needs.
(- See more at: Octoparse Blog)
1. Octoparse

Octoparse is a free and powerful website crawler used for extracting almost all kind of data you need from the website. You can use Octoparse to rip a website with its extensive functionalities and capabilities. There are two kinds of learning mode — Wizard Mode and Advanced Mode — for non-programmers to quickly get used to Octoparse. After downloading the freeware, its point-and-click UI allows you to grab all the text from the website and thus you can download almost all the website content and save it as a structured format like EXCEL, TXT, HTML or your databases.
More advanced, it has provided Scheduled Cloud Extraction which enables you to extract data from dynamic websites in real time and keep a tracking record on the website updates.
And you could extract many tough websites with difficult data block layout using its built-in Regex tool, and locate web elements precisely using the XPath configuration tool. You will not be bothered by IP blocking any more, since Octoparse offers IP Proxy Servers that will automates IP’s leaving without being detected by aggressive websites.
To conclude, Octoparse should be able to satisfy users’ most crawling needs, both basic or high-end, without any coding skills.
2. Cyotek WebCopy

WebCopy is a free website crawler that allows you to copy partial or full websites locally in to your harddisk for offline reading.
It will scan the specified website before downloading the website content onto your hardisk and auto-remap the links to resources like images and other web pages in the site to match its local path, excluding a section of the website. Additional options are also available such as downloading a URL to include in the copy, but not crawling it.
There are many settings you can make to configure how your website will be crawled, in addition to rules and forms mentioned above, you can also configure domain aliases, user agent strings, default documents and more.
However, WebCopy does not include a virtual DOM or any form of JavaScript parsing. If a website makes heavy use of JavaScript to operate, it is unlikely WebCopy will be able to make a true copy if it is unable to discover all of the website due to JavaScript being used to dynamically generate links.
3. HTTrack

As a website crawler freeware, HTTrack provides functions well suited for downloading an entire website from the Internet to your PC. It has provided versions available for Windows, Linux, Sun Solaris, and other Unix systems. It can mirror one site, or more than one site together (with shared links). You can decide the number of connections to opened concurrently while downloading web pages under “Set options”. You can get the photos, files, HTML code from the entire directories, update current mirrored website and resume interrupted downloads.
Plus, Proxy support is available with HTTTrack to maximize speed, with optional authentication.
HTTrack Works as a command-line program, or through a shell for both private (capture) or professionnal (on-line web mirror) use. With that saying, HTTrack should be preferred and used more by people with advanced programming skills.
4. Getleft

Getleft is a free and easy-to-use website grabber that can be used to rip a website. It downloads an entire website with its easy-to-use interface and multiple options. After you launch the Getleft, you can enter a URL and choose the files that should be downloaded before begin downloading the website. While it goes, it changes the original pages, all the links get changed to relative links, for local browsing.Additionally, it offers multilingual support, at present Getleft supports 14 languages.However, it only provides limited Ftp supports, it will download the files but not recursively.
On the whole, Getleft should satisfy users’ basic crawling needs without more complex tactical skills.
5. Scraper

Scraper is a Chrome extension with limited data extraction features but it’s helpful for making online research, and exporting data to Google Spreadsheets. This tool is intended for beginners as well as experts who can easily copy data to the clipboard or store to the spreadsheets using OAuth. Scraper is a free web crawler tool, which works right in your browser and auto-generates smaller XPaths for defining URLs to crawl. It may not offer all-inclusive crawling services, but novices also needn’t tackle messy configurations.
6. OutWit Hub

OutWit Hub is a Firefox add-on with dozens of data extraction features to simplify your web searches. This web crawler tool can browse through pages and store the extracted information in a proper format.
OutWit Hub offers a single interface for scraping tiny or huge amounts of data per needs. OutWit Hub lets you scrape any web page from the browser itself and even create automatic agents to extract data and format it per settings.
It is one of the simplest web scraping tools, which is free to use and offers you the convenience to extract web data without writing a single line of code.
7. ParseHub

Parsehub is a great web crawler that supports collecting data from websites that use AJAX technologies, JavaScript, cookies and etc. Its machine learning technology can read, analyze and then transform web documents into relevant data.
The desktop application of Parsehub supports systems such as windows, Mac OS X and Linux, or you can use the web app that is built within the browser.
As a freeware, you can set up no more than five publice projects in Parsehub. The paid subscription plans allows you to create at least 20 private projects for scraping websites.
8. Visual Scraper

VisualScraper is another great free and non-coding web scraper with simple point-and-click interface and could be used to collect data from the web. You can get real-time data from several web pages and export the extracted data as CSV, XML, JSON or SQL files. Besides the SaaS, VisualScraper offer web scraping service such as data delivery services and createing software extractors services.
Visual Scraper enables users to schedule their projects to be run on specific time or repeat the sequence every minutes, days, week, month, year. Uers could use it to extract news, updates, forum frequently.
9. Scrapinghub

Scrapinghub is a cloud-based data extraction tool that helps thousands of developers to fetch valuable data. Its open source visual scraping tool, allows users to scrape websites without any programming knowledge.
Scrapinghub uses Crawlera, a smart proxy rotator that supports bypassing bot counter-measures to crawl huge or bot-protected sites easily. It enables users to crawl from multiple IPs and locations without the pain of proxy management through a simple HTTP API.
Scrapinghub converts the entire web page into organized content. Its team of experts are available for help in case its crawl builder can’t work your requirements. .
10. Dexi.io

As a browser-based web crawler, Dexi.io allows you to scrape data based on your browser from any website and provide three types of robot for you to create a scraping task — Extractor, Crawler and Pipes. The freeware provides anonymous web proxy servers for your web scraping and your extracted data will be hosted on Dexi.io’s servers for two weeks before the data is archived, or you can directly export the extracted data to JSON or CSV files. It offers paid services to meet your needs for getting real-time data.
11. Webhose.io

Webhose.io enables users to get real-time data from crawling online sources from all over the world into various, clean formats. This web crawler enables you to crawl data and further extract keywords in many different languages using multiple filters covering a wide array of sources.
And you can save the scraped data in XML, JSON and RSS formats. And users are allowed to access the history data from its Archive. Plus, webhose.io supports at most 80 languages with its crawling data results. And users can easily index and search the structured data crawled by Webhose.io.
On the whole, Webhose.io could satisfy users’ elementary crawling requirements.
12. Import. io

Users are able to form their own datasets by simply importing the data from a particular web page and exporting the data to CSV.
You can easily scrape thousands of web pages in minutes without writing a single line of code and build 1000+ APIs based on your requirements. Public APIs has provided powerful and flexible capabilities to control Import.io programmatically and gain automated access to the data, Import.io has made crawling easier by integrating web data into your own app or web site with just a few clicks.
To better serve users’ crawling requirements, it also offers a free app for Windows, Mac OS X and Linux to build data extractors and crawlers, download data and sync with the online account. Plus, users are able to schedule crawling tasks weekly, daily or hourly.
13. 80legs

80legs is a powerful web crawling tool that can be configured based on customized requirements. It supports fetching huge amounts of data along with the option to download the extracted data instantly. 80legs provides high-performance web crawling that works rapidly and fetches required data in mere seconds
14. Spinn3r

Spinn3r allows you to fetch entire data from blogs, news & social media sites and RSS & ATOM feeds. Spinn3r is distributed with a firehouse API that manages 95% of the indexing work. It offers an advanced spam protection, which removes spam and inappropriate language uses, thus improving data safety.
Spinn3r indexes content similar to Google and saves the extracted data in JSON files. The web scraper constantly scans the web and finds updates from multiple sources to get you real-time publications. Its admin console lets you control crawls and full-text search allows making complex queries on raw data.
15. Content Grabber

Content Graber is a web crawling software targeted at enterprises. It allows you to create a stand-alone web crawling agents. It can extract content from almost any website and save it as structured data in a format of your choice, including Excel reports, XML, CSV and most databases.
It is more suitable for people with advanced programming skills, since it offers many powerful scripting editing, debugging interfaces for people in need. Users are allowed to use C# or VB.NET to debug or write script to control the crawling process programmingly. For example, Content Grabber can integrate with Visual Studio 2013 for the most powerful script editing, debugging and unit test for a advanced and tactful customized crawler based on users’ particular needs.
16. Helium Scraper

Helium Scraper is a visual web data crawling software that works pretty well when the association between elements is small. It’s non coding, non configuration. And users can get access to the online templates based for various crawling needs.
Basically, it could satisfy users’ crawling needs within an elementary level.
17. UiPath

UiPath is a robotic process automation software for free web scraping. It automates web and desktop data crawling out of most third-party Apps. You can install the robotic process automation software if you run Windows system. Uipath is able to extract tabular and pattern-based data across multiple web pages.
Uipath has provided the built-in tools for further crawling. This method is very effective when dealing complex UIs. The Screen Scraping Tool can handle both individual text elements, groups of text and blocks of text, such as data extraction in table format.
Plus, no programming is needed to create intelligent web agents, but the .NET hacker inside you will have complete control over the data.
18. Scrape. it

Scrape.it is a node.js web scraping software for humans. It’s a cloud-base web data extraction tool. It’s designed towards those with advanced programming skills, since it has offered both public and private packages to discover, reuse, update, and share code with millions of developers worldwide. Its powerful integration will help you build a customized crawler based on your needs.
19. WebHarvy

WebHarvy is a point-and-click web scraping software. It’s designed for non-programmers. WebHarvy can automatically scrape Text, Images, URLs & Emails from websites, and save the scraped content in various formats. It also provides built-in scheduler and proxy support which enables anonymously crawling and prevents the web scraping software from being blocked by web servers, you have the option to access target websites via proxy servers or VPN.
Users can save the data extracted from web pages in a variety of formats. The current version of WebHarvy Web Scraper allows you to export the scraped data as an XML, CSV, JSON or TSV file. User can also export the scraped data to an SQL database.
20. Connotate

Connotate is an automated web crawler designed for Enterprise-scale web content extraction which needs an enterprise-scale solution. Business users can easily create extraction agents in as little as minutes — without any programming. User can easily create extraction agents simply by point-and-click.
It is able to automatically extract over 95% of sites without programming, including complex JavaScript-based dynamic site technologies, such as Ajax. And Connotate supports any language for data crawling from most sites.
Additionally, Connotate also offers the function to integrate webpage and database content, including content from SQL databases and MongoDB for database extraction.
To conclude, the crawlers I mentioned above can satisfy the basic crawling needs for most users, while there are still many variance about their respective functionalities among these tools, since many of these crawler tools have provided more advanced and built-in configuration tools for users. Thus, be sure you have fully understand what characters an crawler has provided before you subscribe it.
To learn more detailed knowledge about how to scrape data from websites using a web crawler, check out the posts or tutorials below:
Web Scraping Hotel Information from Google Maps
- See more at: Octoparse Blog
",2480,Web Scraping,Octoparse,https://medium.com/s/story/top-20-web-crawler-tools-to-scrape-the-websites-9088a4b6618d,"Web crawler tools are getting well known to the common, since the web crawler has simplified and automated the entire crawling process to make web data resource become easily accessible to everyone.
Using a web crawler tool will set free people from repetitive typing or copy-pasting, and we could expect a well-structured and all-inclusive data collection.
Additionally, these web crawler tools enable users to crawl the world wide web in a methodical and fast manner without coding and transform the data into various formats conforming to their needs.
Octoparse is a free and powerful website crawler used for extracting almost all kind of data you need from the website.
To conclude, Octoparse should be able to satisfy users’ most crawling needs, both basic or high-end, without any coding skills.
Scraper is a free web crawler tool, which works right in your browser and auto-generates smaller XPaths for defining URLs to crawl.
This web crawler tool can browse through pages and store the extracted information in a proper format.
OutWit Hub lets you scrape any web page from the browser itself and even create automatic agents to extract data and format it per settings.
It is one of the simplest web scraping tools, which is free to use and offers you the convenience to extract web data without writing a single line of code.
Parsehub is a great web crawler that supports collecting data from websites that use AJAX technologies, JavaScript, cookies and etc.
You can get real-time data from several web pages and export the extracted data as CSV, XML, JSON or SQL files.
As a browser-based web crawler, Dexi.io allows you to scrape data based on your browser from any website and provide three types of robot for you to create a scraping task — Extractor, Crawler and Pipes.
Webhose.io enables users to get real-time data from crawling online sources from all over the world into various, clean formats.
This web crawler enables you to crawl data and further extract keywords in many different languages using multiple filters covering a wide array of sources.
Users are able to form their own datasets by simply importing the data from a particular web page and exporting the data to CSV.
You can easily scrape thousands of web pages in minutes without writing a single line of code and build 1000+ APIs based on your requirements.
Public APIs has provided powerful and flexible capabilities to control Import.io programmatically and gain automated access to the data, Import.io has made crawling easier by integrating web data into your own app or web site with just a few clicks.
To better serve users’ crawling requirements, it also offers a free app for Windows, Mac OS X and Linux to build data extractors and crawlers, download data and sync with the online account.
80legs is a powerful web crawling tool that can be configured based on customized requirements.
80legs provides high-performance web crawling that works rapidly and fetches required data in mere seconds
It can extract content from almost any website and save it as structured data in a format of your choice, including Excel reports, XML, CSV and most databases.
Helium Scraper is a visual web data crawling software that works pretty well when the association between elements is small.
Uipath is able to extract tabular and pattern-based data across multiple web pages.
It’s a cloud-base web data extraction tool.
It also provides built-in scheduler and proxy support which enables anonymously crawling and prevents the web scraping software from being blocked by web servers, you have the option to access target websites via proxy servers or VPN.
Users can save the data extracted from web pages in a variety of formats.
The current version of WebHarvy Web Scraper allows you to export the scraped data as an XML, CSV, JSON or TSV file.
To learn more detailed knowledge about how to scrape data from websites using a web crawler, check out the posts or tutorials below:"
3 Ways Artificial Intelligence Can Complement Qualitative Research,Back in 1950 Alan Turing’s famous question “Can machines think?” opened his paper on ‘Computing Machinery and Intelligence’. Jumping ahead…,"
3 Ways Artificial Intelligence Can Complement Qualitative Research
Back in 1950 Alan Turing’s famous question “Can machines think?” opened his paper on ‘Computing Machinery and Intelligence’. Jumping ahead in time somewhat, the advancements around artificial intelligence (AI) are manifold and span pretty much all aspects of our lives.
AI is serving millions daily — be it via your smartphone, your car, your bank or your house. Sometimes you actively use a service such as Siri or Cortana, other times it’s less obvious and very often it even goes unnoticed. The AI wave is having an impact on industries across the board including market research.
AI is not a Man versus Machine saga; it’s in fact. Man with Machine synergy — Sudipto Ghosh
AI in Qualitative Research
AI is inherently based on mathematical models and driven by numerical data. You might assume that this clashes somewhat with the descriptive nature of qualitative research. After all, qualitative research is concerned with a set of methodologies designed to explore; to delve into the depths of human behaviour and its reasoning.
In order to achieve this qualitative research questions tend to be contextual, broad and non-static. Moderation is often described as intuitive and the data output — text, audio, visual and audio-visual — ‘messy’ and immeasurable. Today however, AI products have advanced beyond mere numbers. They are now equipped to deal with this ‘messy’ immeasurable content, or at least assist in the process.
We have written both blogs and whitepapers about the direct role of chatbots in qualitative moderation as well as the part wider AI techniques can and will play in qual and quant data collection. analysis and interpretation. But there are even more potential complementary applications of AI for qual in particular. Below I share my top three.
1. Insight Community Engagement
MROCs are a great example of a market research approach which provides vast amounts of qualitative data output. The sheer volume of community member content produced over weeks, months or years is a challenge for any community manager. Text and sentiment analysis are already in full swing in qualitative data analysis, but there is another exciting application of these tools in that of insight community engagement.
By applying deep learning to a combination of text and sentiment analysis, login, activity and profile data, market research can leverage AI to anticipate member disengagement before it happens. This enables the (human) researcher to manage member engagement proactively, to reduce churn without firefighting.
And there is nothing to stop this clever AI technique from being extended into engagement optimisation, for deep learning to ‘learn’ what engages who, when and where. All leading to higher quality feedback for both researchers and clients.
2. Conversational Insight Activation
We have already seen various market research dashboard applications enter the arena, all designed to provide internal stakeholder access to real-time data in a visualising appealing manner, i.e. to encourage insight based action. But what if we took it one step further… beyond the dashboard to an insight activation chatbot.
For an entry level insight activation chatbot, i.e. one that deals with quant data only, the AI required is still twofold:
The analytical ability to identify correlation within a vast number of differing quant data sets
The natural language ability to articulate this correlation, with supporting visuals
Working off the back of a dashboard style database, an entry level bot of this nature would be able analyse both historic and current data, and summarise only query relevant insights in bite-size easily digestible qualitative chunks. Individual customer persona chatbots could even be programmed to emulate Q&A style conversations with business members. Now we are using qual to embed insight within organisations, that’s pretty clever.
If we were to add deep learning into the mix we could take our chatbot even further. This would allow for more creative proactive use. Your friendly insight activation chatbot no longer has to wait to be called upon; rather it can push insights out to company members. Recipients feedback on relevance when they are able and the bot learns what is beneficial to their job role, at what time, and even the language to use to resonate. It’s got insight culture written all over it… with one small caveat.
Each bit-sized insight would have to come with full study links. Even when qual research data is added to the quant database, a chatbot identifying causation? There are a few too many holes in text and sentiment analysis for that from my perspective, at least as it stands today.
Community engagement. insight activation. participant service — 3 innovative AI market research applications
3. Researcher’s Assistant
Advances in both natural language and voice processing have seen the use of virtual assistants, both commercial and personal, taken to a whole new level. Commercially this form of AI can be applied to customer, information and entertainment services as well as purchasing functions and has become increasingly popular with the big players such as Amazon, Facebook and Google.
In our blog, 5 Big Qual Market Research Chatbots You Can Build Today we proposed a specific use of a chatbot for moderation assistance in big qual. But there is also the potential to create a generic research assistant. As in the commercial scenarios referenced above, the ‘Researcher’s Assistant’ would provide participant information services either full-time, or out of hours. It would effectively ‘staff’ the research project.
In the current AI circumstances this staffing would be limited to natural language processing and generation based on a knowledge bank but nevertheless, the RA would provide a degree of interactive support to bolster participation where it was previously a challenge to do so, i.e. evenings, nights, research projects spanning different times zones, and / or free the human researcher to focus on the more complex cognitive tasks.
Conclusion
Market research organisations and professionals are embracing AI with open arms, trying out new approaches and research set-ups. It is envisioned that researchers and AI will be working alongside each other to create (qualitative) insights that previously were beyond the realms of possibility. AI is here to stay, let’s get involved proactively, let’s lead the way.
",1028,Artificial Intelligence,FlexMR,https://medium.com/s/story/3-ways-artificial-intelligence-can-complement-qualitative-research-d3bb1723a260,"We have written both blogs and whitepapers about the direct role of chatbots in qualitative moderation as well as the part wider AI techniques can and will play in qual and quant data collection.
MROCs are a great example of a market research approach which provides vast amounts of qualitative data output.
Text and sentiment analysis are already in full swing in qualitative data analysis, but there is another exciting application of these tools in that of insight community engagement.
By applying deep learning to a combination of text and sentiment analysis, login, activity and profile data, market research can leverage AI to anticipate member disengagement before it happens.
We have already seen various market research dashboard applications enter the arena, all designed to provide internal stakeholder access to real-time data in a visualising appealing manner, i.e. to encourage insight based action.
For an entry level insight activation chatbot, i.e. one that deals with quant data only, the AI required is still twofold:
Working off the back of a dashboard style database, an entry level bot of this nature would be able analyse both historic and current data, and summarise only query relevant insights in bite-size easily digestible qualitative chunks.
Even when qual research data is added to the quant database, a chatbot identifying causation?
participant service — 3 innovative AI market research applications
Advances in both natural language and voice processing have seen the use of virtual assistants, both commercial and personal, taken to a whole new level.
In our blog, 5 Big Qual Market Research Chatbots You Can Build Today we proposed a specific use of a chatbot for moderation assistance in big qual.
It is envisioned that researchers and AI will be working alongside each other to create (qualitative) insights that previously were beyond the realms of possibility."
"Artificial Intelligence: The past, the present and the future","Humans have always striven for greater things. The evolution of man has seen inventions and discoveries, the rise of civilizations, massive…","Artificial Intelligence: The past, the present and the future
Humans have always striven for greater things. The evolution of man has seen inventions and discoveries, the rise of civilizations, massive feats of engineering and in the recent past exponential advances in technology. The inception of all this was in the human mind. So there is no surprise that for thousands of years, people have tried to understand how the human mind works. But it does not stop there. Attempts are now being made to not only understand, but also to build intelligent entities that replicate this intelligence. This has given rise to the field of Artificial Intelligence.
Although the phrase Artificial Intelligence seem modern and recent, the concept can be traced back to classical philosophy where attempts have been made to link the human mind to a symbolic system. Since then many different attempts and advances have been made in laying a slow but solid foundation that would eventually provide a strong base for the field of Artificial Intelligence. A very recent example is the work of the mathematician Alan Turing. Widely thought to be a man ahead of his time, his ideas proved to be of profound impact in the field of AI. Even at an age where computing was at its dawn, he was pursuing the question, “Can machines think?” With no clear visibility as to where computing was heading at this time, he argued that conjectures are of great importance to new lines of research. He then published a set of criteria that he believed was necessary to determine if a machine was genuinely intelligent. This is now known as the Turing Test which essentially states that a machine could be judged intelligent if it can fool a human examiner into thinking the machine is human. The Turing Test is still considered the holy grail of AI researches and remains a useful method to evaluate the progress of AI.
However, the field of AI was not formally founded until 1956, at a conference at Dartmouth College where the term Artificial Intelligence was first used. With lots of optimism it was stated at this conference that, “every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it’ . This principle kick started an AI revolution that many believed would lead to fully fledged thinking machines by the turn of the millennium.
The early years of AI was highly successful. Lots of enthusiasm surrounded the community and even minor advancements by AI researches in early computing was seen as breakthroughs. The primitive nature of computers and the rapid advancements in technology only made them more frequent. To everyone in the intellectual community this was rapid progress which lead to bold (and on hindsight overconfident) statements such as this;
“It is not my aim to surprise or shock you but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create. Moreover, their ability to do these things is going to increase rapidly until — in a visible future — the range of problems they can handle will be coextensive with the range to which the human mind has been applied.”
But the bubble would soon burst as the reality of the ever growing engineering requirements would add to the complexity of the problems to be solved. The earliest strategies of AI problem solving was exhaustive; which basically meant that possible sequences of solutions were tried out until a solution was found. This worked at that time due to the simplicity of the then existing problems. It was also wrongly assumed that scaling up a problem was only a matter of faster hardware and more memory. But with time as theorems and ‘fact’ failed to hold their own, the realization set in within the community that all ‘progress’ made was just the tip of the iceberg. True Artificial Intelligence was still beyond reach.
The first decade of AI research problem solving did not scale up with problem complexity as expected. Clearly, new ideas were required going forward. One approach was to use more powerful, domain-specific knowledge that allowed larger reasoning steps during problem solving. The DENDRAL program was an influential pioneer project in AI that followed this approach. The software program is considered the first expert system because it automated the decision-making process and problem solving behavior of organic chemists. The Dendral project lead to the derivation of many other systems that had widespread application in real-world problems. This lead to a higher demand for workable knowledge representation schemes and as a result a large number of different representation and reasoning languages were developed. AI soon became an industry. Commercial expert systems were helping companies save millions every year, competition was brewing between countries and within a decade, a few million dollar industry was transformed into a billion dollar industry.
Modern Artificial Intelligence has seen significant advancements in the past decade due to greater use of the scientific method in experimenting with and comparing approaches. Many parallels have been drawn and sub fields of AI have found common ground with other disciplines as well. Neural Networks and Genetic Algorithms are two such areas that has had important theoretical advancements recently. Artificial Neural Networks (ANN) attempt to simulate the conditions that exist within the human brain, which is without doubt the ultimate artificial intelligence machine. The concept behind the natural brain is the idea of connection strength between individual synapses and dendrites. In ANN, A similar principle is followed where artificial neurons used most are assigned a higher weight, indicating a stronger connection. However, no artificial neural network exists as of now that can match the complexity of the human brain. Genetic algorithms on the other hand, mimics the process of Darwin’s natural selection where computers/algorithms are allowed to evolve through multiple iterations, and the result is a population of fit or highly effective computers/algorithms that are chosen to be the best fit for a particular problem. Likewise, there are many other algorithms such as Bayesian networks, Support Vector Machines, and most importantly, Natural Language Processing (NLP). An NLP module is able to process fuzzy human language that would otherwise make no sense to a machine. It could analyze a paragraph consisting of spoken language, analyze it and produce required results, even predict the sentiment behind the language, something an ordinary machine is not able to do. The above mentioned techniques and algorithms, also brings cognitive psychology, neuroscience, evolution, sociology, etc. into the picture. What this means is the AI venture cannot be successful without a thorough grounding on the aforementioned areas because they dive into the study of the human mind on an individual and collective level.
So the question is, where are we currently in AI development? Or how far are we from a truly intelligent machine? To answer this we need to understand the three calibers of AI development that most experts agree on.
Artificial Narrow Intelligence (ANI): AI’s that have a specialization such as gaming, stock market prediction, healthcare, etc. They have no purpose beyond the highly specialized task for which they were created.
Artificial General Intelligence (AGI): An AI that passes the singularity of human intelligence and is able to think, reason, think on an abstract level (a feature that separates humans from chimps), solve complex problems, and even to learn by experience.
Artificial Super Intelligence (ASI): An AI smarter than anything humanity has ever known, even smarter than the collective intelligence of humanity. What we would otherwise call a god. Having mental capacity and power trillions of times that of a human.
Artificial Narrow Intelligence is basically a machine that surpasses human intelligence at a specific thing. The world currently has successfully conquered ANI. Artificial Narrow Intelligence is everywhere; from Google’s search, through self-driving cars, to chess agents able to beat the best human chess player. ANI systems are increasingly being used at commercial level by internet giants such as Google, Facebook and Youtube. The advancement through these gauntlets of AI research is increasing exponentially in speed and knowledge, simply because knowledge and our technological capacity is increasing exponentially according to Moore’s Law. This leads us to the million dollar question, does each new innovation in ANI pave the way albeit slowly towards AGI and ASI?
Man has walked on the moon. Science and engineering together have gone well beyond what was previously thought possible. All of this at its infancy, an idea within the human brain. Yet no brain on earth is even remotely close to replicating an entity that can mimic the functionality of the human brain. AGI is what everyone imagine AI to be. AGI is the Artificial Intelligence of science fiction. It is the idea of Artificial Intelligence that is as intelligent as the human brain. But ironically the difficulties lie in what is actually easy or second nature for us human beings.
“Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are. Build a computer that can multiply two ten-digit numbers in a split second — incredibly easy. Build one that can look at a dog and answer whether it’s a dog or a cat — spectacularly difficult. Make AI that can beat any human in chess? Done. Make one that can read a paragraph from a six-year-old’s picture book and not just recognize the words but understand the meaning of them? Google is currently spending billions of dollars trying to do it.”
It was suggested that a successful AI is not one that tries to behave like just another human being. It is one that behaves like a child. Rather than being preprogrammed with instructions, the machine is allowed to evolve and learn through experience much a like a child does. The things that seem so simple to humans; vision, motion, perception, prediction, abstraction, are that way because humanity has collectively evolved to be good at those. But areas such as highly complicated mathematics are not a natural feature of collective humanity. It has to be learnt by every human being if at all. But that very thing is what a computer can be so good at. So maybe in order for a machine to be good at what it isn’t, and at the same time be intelligent, the best way is to make this whole thing the computer’s problem. What this means is to program the computer to be student and teacher at the same time. Let it be an AI researcher allowing itself not only to learn, but also to improve and figure out how to make itself smarter. Let it evolve.
The final two calibers would be massive milestones for AI scientists. As of now they are highly theoretical and pose both solutions and problems. A super intelligent machine could do things we couldn’t even dream of doing, with intelligence a trillion times greater than ours, it could do science that is way beyond human capability. On the other hand, such an intelligence could also come to think of humanity as an unevolved population of primates. The consequences are unpredictable and scary in that case. But the trajectory of AI is such. These are leaps and bounds ahead from where we are now; the incipient stages of AI application. But based on the rate of technological advancement, they aren’t too out of reach. If they come to pass, the criteria for ‘intelligence proper’ would not only be met, it would far surpass it to the point of redefining intelligence for machines whose thinking would be on a different class altogether.
REFERENCES
1. Artificial Intelligence: A Modern Approach by Stuart Russell and Peter Norvig.
2. http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html
3. http://www.bbc.com/news/technology-18475646
",1984,Artificial Intelligence,Priyath Gregory,https://medium.com/s/story/artificial-intelligence-the-past-the-present-and-the-future-5feeaf08c6cc,"The evolution of man has seen inventions and discoveries, the rise of civilizations, massive feats of engineering and in the recent past exponential advances in technology.
So there is no surprise that for thousands of years, people have tried to understand how the human mind works.
Although the phrase Artificial Intelligence seem modern and recent, the concept can be traced back to classical philosophy where attempts have been made to link the human mind to a symbolic system.
Since then many different attempts and advances have been made in laying a slow but solid foundation that would eventually provide a strong base for the field of Artificial Intelligence.
Even at an age where computing was at its dawn, he was pursuing the question, “Can machines think?” With no clear visibility as to where computing was heading at this time, he argued that conjectures are of great importance to new lines of research.
Moreover, their ability to do these things is going to increase rapidly until — in a visible future — the range of problems they can handle will be coextensive with the range to which the human mind has been applied.”
Modern Artificial Intelligence has seen significant advancements in the past decade due to greater use of the scientific method in experimenting with and comparing approaches.
Artificial Neural Networks (ANN) attempt to simulate the conditions that exist within the human brain, which is without doubt the ultimate artificial intelligence machine.
However, no artificial neural network exists as of now that can match the complexity of the human brain.
Genetic algorithms on the other hand, mimics the process of Darwin’s natural selection where computers/algorithms are allowed to evolve through multiple iterations, and the result is a population of fit or highly effective computers/algorithms that are chosen to be the best fit for a particular problem.
What this means is the AI venture cannot be successful without a thorough grounding on the aforementioned areas because they dive into the study of the human mind on an individual and collective level.
Artificial General Intelligence (AGI): An AI that passes the singularity of human intelligence and is able to think, reason, think on an abstract level (a feature that separates humans from chimps), solve complex problems, and even to learn by experience.
Artificial Narrow Intelligence is basically a machine that surpasses human intelligence at a specific thing.
Artificial Narrow Intelligence is everywhere; from Google’s search, through self-driving cars, to chess agents able to beat the best human chess player.
The advancement through these gauntlets of AI research is increasing exponentially in speed and knowledge, simply because knowledge and our technological capacity is increasing exponentially according to Moore’s Law. This leads us to the million dollar question, does each new innovation in ANI pave the way albeit slowly towards AGI and ASI?
AGI is the Artificial Intelligence of science fiction.
It is the idea of Artificial Intelligence that is as intelligent as the human brain.
“Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are.
But areas such as highly complicated mathematics are not a natural feature of collective humanity.
So maybe in order for a machine to be good at what it isn’t, and at the same time be intelligent, the best way is to make this whole thing the computer’s problem.
A super intelligent machine could do things we couldn’t even dream of doing, with intelligence a trillion times greater than ours, it could do science that is way beyond human capability."
Rossmann Drug Store Sales Prediction:,Abstract -­­ In this paper we tried to apply machine learning algorithm into a real world problem — drug store sales forecasting. Given…,"Rossmann Drug Store Sales Prediction:
Abstract -­­ In this paper we tried to apply machine learning algorithm into a real world problem — drug store sales forecasting. Given store information, and sales record we applied Linear Regression, Support Vector Regression (SVR) with Gaussian and Polynomial Kernels and Random Forest algorithm, and tried to predict sales for [1]-­­3 weeks Root Mean Square Percentage Error (RMSPE) is used to measure the accuracy. As it turned out, Random Forest outshined all other models and reached RMSPE of 12.3%, which is a reliable forecast that enables store managers allocate staff and stock up effectively.
1. INTRODUCTION
This problem is one of several Machine Learning problems on Kaggle1 . The aim of this problem is to forecast future sales of 1,115 Rossman drug stores located across Germany based on their historical sales data. The practical meaning of solving this problem lies in that reliable sales forecasts enables store managers to create effective staff schedules that increase productivity and motivation. What’s more, for the purpose of practicing what we learnt from the Machine Learning class, this problem saves us the trouble of collecting data, and in the meanwhile provides a perfect real case to apply supervised learning algorithms.
3. DATASET AND FEATURES
The dataset of this problem can be found online and the link is provided below. The data comes in two sets
Sales Dataset -­­ Historical sales data for 1,115 Rossman stores from 2013/1/1 to 2015/7/31. Features include store number, date, day of week, whether there’s a promotion, whether it’s a school or state
holiday and sales on that day.
[1] The link to this problem: https://www.kaggle.com/c/rossmann-­­store-­­sales
[2] The link to dataset: https://www.kaggle.com/c/rossmann-­­store-­­sales/data
1. Store Dataset -­­ Stores’ individual characteristics. Features include store type, assortment level, nearest competitor’s distance and when the competitor was opened, and whether there’s a consecutive promotion.
Throughout our trial, we’ve tried to take advantage of different subset of features. However, reducing number of features didn’t increase accuracy for this problem. So all features are used for building models
70%/30% and k-­­fold cross validations are used in this problem for training and testing. Root Mean Square Percentage Error (RMSPE) is used to measure accuracy, which is defined as

4. Modeling METHODS
There are two methods to train the data. One is to train each store separately, which means forecasting sales of a single store based on its own sales record, regardless of store attributes. The other one is to train all stores together, considering store attributes as parameters.
To train each store separately, one straightforward idea is to apply linear regression. According to the normal equation,

we can easily predict sales by

Further more, we figured that this problem can actually be kernelized. Here consider that case of applying MAP estimate for
to avoid overfitting, which results in the following primal problem

We can easily predict the sales


Gaussian Kernel And Polynomial kernel
we can see that this problem can actually be kernalized, thus we can apply the kernel trick. We tried Gaussian Kernel and Polynomial Kernel in this case, which is illustrated as following.
(a) Gaussian Kernel
(b) Polynomial Kernel (Polynomials of degree up to d)
Our next model for this project is Random Forest Regression. We tried this model because it’s fast and can accommodate categorical data. RF first picked a certain amount of data from the dataset randomly (ie. bootstrap) and then picked a certain amount of features out of the total features randomly to build decision trees. The final result for each test data is average of results obtained by all these decision trees. Decision trees usually overfit the data; however randomness will average out the high variance
5. EXPERIMENTS AND RESULTS
Linear Regression
Linear regression is used as our baseline model. 70%/30% cross validation is used here to divide the data set into
training set and test set. As it turns out, linear regression gives us a RMSPE of 52.8%.
Support Vector Regression
One thing special on the implementation of SVR is that, it need to build an m*m matrix, where m indicates the number
of training samples. Since the size of our training set is ~700,000 , it’s unrealistic to operate on the whole dataset. To take use of the abundant dataset practically, we build a SVR model for each store, and compute the mean of each store’s RMSPE as our final error rate.
Firstly, we applied Polynomial Kernel and Gaussian Kernel for a single store, Store 1. By trying different pairs of
for Gaussian Kernel, and different pairs of for Polynomial Kernel, we found that when
lambda = 140,
Sigma = 45,
Gaussian Kernel gives the best RMSPE of 13.6%, when Polynomial Kernel gives the best RMSPE of 12.8%. Two kernels are comparable in this scenario.
Gaussian Kernel
Polynomial Kernel
Secondly, using the method of finding optimal pairs of parameters discussed above, we applied Gaussian Kernel and Polynomial Kernel to all stores. As we dig deeper into the dataset, we found that accuracies vary on different time period of prediction. Below are figures of how RMSPE varies with different time period for prediction. In terms of average RMSPE, Polynomial Kernel(16.3%) beats Gaussian Kernel (26.8%) significantly. In the meantime Polynomial Kernel is also more robust than Gaussian Kernel, given that there are fewer outliers and no extreme
outliers(RMSPE>1) in the figure of Polynomial Kernel. So overall, Polynomial Kernel suits the dataset better and provides more reliable results.
Random Forest
We applied Random Forest after merging all data including all the categorical data. We used scikit-­­learn package of python for implementing the algorithm[7].
The two main parameters we tuned for RF is the number of trees and the size of the random subsets of features to consider when splitting a node. We used 5 fold cross validation to get RMSPE while varying these parameters. Two plots are shown below. From these plots, we could see that RMSPE doesn’t change too much after tree number reaches 30 and after feature number reaches 20.
RMSPE with feature number
RMSPE with tree numbers
Conclusion:

As is shown in the result, among all models, Random Forest works the best, and provides a reliable prediction of the sales. Linear regression, SVR with Gaussian/Polynomial Kernels and RF all have their own strengths and limitations. By implementing these algorithms, we’ve studies the properties of the dataset and made reasonable predictions. In the future, we wish to use the fact that sales records are consecutive in time, and see how time series affect prediction result. Also there are still many effective machine learning algorithms worth trying, so we would like to try more algorithms in the future, such as Gradient Boosting and k-­­Nearest Neighbors algorithm.
",1107,Machine Learning,Vikas Mishra,https://medium.com/s/story/rossmann-drug-store-sales-prediction-9dd962c2d02e,"Abstract -­­ In this paper we tried to apply machine learning algorithm into a real world problem — drug store sales forecasting.
Given store information, and sales record we applied Linear Regression, Support Vector Regression (SVR) with Gaussian and Polynomial Kernels and Random Forest algorithm, and tried to predict sales for [1]-­­3 weeks Root Mean Square Percentage Error (RMSPE) is used to measure the accuracy.
As it turned out, Random Forest outshined all other models and reached RMSPE of 12.3%, which is a reliable forecast that enables store managers allocate staff and stock up effectively.
The aim of this problem is to forecast future sales of 1,115 Rossman drug stores located across Germany based on their historical sales data.
The practical meaning of solving this problem lies in that reliable sales forecasts enables store managers to create effective staff schedules that increase productivity and motivation.
What’s more, for the purpose of practicing what we learnt from the Machine Learning class, this problem saves us the trouble of collecting data, and in the meanwhile provides a perfect real case to apply supervised learning algorithms.
Sales Dataset -­­ Historical sales data for 1,115 Rossman stores from 2013/1/1 to 2015/7/31.
To train each store separately, one straightforward idea is to apply linear regression.
We tried Gaussian Kernel and Polynomial Kernel in this case, which is illustrated as following.
To take use of the abundant dataset practically, we build a SVR model for each store, and compute the mean of each store’s RMSPE as our final error rate.
Firstly, we applied Polynomial Kernel and Gaussian Kernel for a single store, Store 1.
Secondly, using the method of finding optimal pairs of parameters discussed above, we applied Gaussian Kernel and Polynomial Kernel to all stores.
In terms of average RMSPE, Polynomial Kernel(16.3%) beats Gaussian Kernel (26.8%) significantly.
outliers(RMSPE>1) in the figure of Polynomial Kernel.
So overall, Polynomial Kernel suits the dataset better and provides more reliable results.
As is shown in the result, among all models, Random Forest works the best, and provides a reliable prediction of the sales.
Linear regression, SVR with Gaussian/Polynomial Kernels and RF all have their own strengths and limitations."
Creation of the Mythical AI — Bringing Artificial Intelligence Reporting Back to Earth,Artificial intelligence is a key enabling technology for connected autonomous vehicles (self driving cars in particular) and a host of…,"Creation of the Mythical AI — Bringing Artificial Intelligence Reporting Back to Earth
Photo by Kamil Pietrzak on Unsplash
Artificial intelligence is a key enabling technology for connected autonomous vehicles (self driving cars in particular) and a host of other important technological advances. But are we becoming too tied up in the reporting hype?
Since 2015, the wave of sensational artificial intelligence stories is in full swing. AI is a regular political debating topic. Exciting announcements bombard us and professional forecasters portray a future sometimes utopian and most often dystopian, focusing on extremes like a well-written science fiction story.
Photo by Lukas on Unsplash
The media routinely disguises and exaggerates reality. Abilities are bestowed on AI it which it does not possess and is not ready to have. We are building a mythical vision of AI, based on propaganda and early innovation. The marketing communications of suppliers, such as IBM and Google, amplify and embellish progress through a strategy of amazement. Checking the intricate detail of reports is beyond the casual observer and hampered by the widespread absence of technical knowledge on AI, even among of digital specialists.
This term AI was first used in 1955 by John MacCarthy when he proposed holding of a summer camp at Dartmouth College in New Hampshire. It was a marketing phrase created to talk about a dispersed collection of technologies rather than an emerging field of study. Today the debates are endless as scientists, philosophers and the media struggle to define the nature of human intelligence and how much of it machines can gain. Sometimes it is easier to refer to AI in terms of what it cannot yet do. For many revealing the magic of today’s AI destroys their established definition of AI.
Photo by Kevin on Unsplash
According to current articles… IBM Watson is the swiss army knife AI who knows how to do everything, DeepMind AlphaGo is more intelligent than the best strategy game players, the latest Intel processor Loihi mimics the brain, autonomous cars are happening now and miracle marketing solutions predict in advance what you want. The AI peddled by the media embodies a future which will remove all chance and free will in our little monotonous lives.
Jobs will disappear, a valid concern for many. Forecasts are predicting up to 50% by 2025. However, reports fail to mention that much of elimination of trades does not come from AI but general automation and transferring work to customers, for example, automatic cash registers, or by distributing work through the emerging collaborative economy.
Photo by Igor Ovsyannykov on Unsplash
Journalists effective in their communication are rarely specialists in AI in the scientific sense of the term. And when they are, subtle exaggerations accumulate. An example is that of singularity advocate Ray Kurzweil who offers up quotes regularly. From what you read you would think he is directing all of Google’s AI research. In reality, he is only a small part of Google’s AI effort, the two main divisions are Google Brain and Deep Mind.
By thinking only of the future, we forget the present. Obsession with AI creates a delirious atmosphere that becomes disruptive to capitalism. Talk of AI will interest companies, and scare them. But chiefly, AI wastes management time as they wonder how to integrate the shiny new thing into their strategy. Many conventional businesses need to return to Earth, to deliver practical and workable things.
To help you navigate the sensationalism that accompanies AI, here are a selection of myths surrounding AI that grate with me.
- AI algorithms have not improved in the last 20 years
Many reports suggest AI algorithms have not evolved. Better hardware and the abundance of exploitable data are the reason for recent advances. This is not true.
“Equations written in chalk on a worn-out blackboard” by Roman Mager on Unsplash
Algorithmic progress is slow in machine learning because it relies on classification, prediction and segmentation methods whos roots are one to two decades old. It is different in deep learning. Deep learning dates back to 2006 and its techniques have been evolving since. In 2016 alone, the convolutional neural networks used for image recognition have undergone many iterations, also the generative networks capable of transforming and generating images. Only since 2015 have these systems had the ability to tag multiple objects in an picture or video.
The family of neural network types is complex. Deep learning requires assembly of these bricks that constantly evolve. The general aim of AI development comprises two complementary objectives: to improve the qualitative performance of neural networks and to speed up training. The latter is very expensive in terms of machine resources, even with GPUs and neuromorphic processors such as Googles TPU.
Photo by Alexandre Debiève on Unsplash
Memory neural networks used for language processing (recognition, translation, generation) have many variations, including: Stacked RNN (RNN stacked), MARNN (Memory-Augmented Recurrent Neural Networks), LSTM (Long Short-Term Memory), GRU (Gated Recurrent Units) BLSTM (Bidirectional Long Short-Term Memory), BPTT (BackProp Through Time) and RTRL (Real Time Recurrent Learning). They take into account the context in which the objects like words are detected to analyze the meaning of a sentence. One of the key points of these networks is their ability to memorize contexts.
The pleasant quirk of AI research is that many advances are exploitable rather quickly because they are public and reproducible with the open source tools of the market such as TensorFlow or Torch.
- AI only comprises machine learning and deep learning
Deep learning has come to the fore as the miracle solution to address all problems. Deep learning is so called because its neural networks are a multilayer. This does not make them intelligent. Many papers have shown that human intelligence does not work like the current deep learning neural networks. These only a statistical and numerical view of the objects they analyze. It is neither semantic nor analytic. Deep learning is not the ultimate outcome of AI, it only serves to solve certain classes of problems.
The broad field of AI includes other techniques, including symbolic AI, logic programming, and rule engines. News has muted them because of the glamour of deep learning. The best AI solutions often integrate and assemble several approaches.
- Useful data is the main component of AI
Data is one of the three key components of AI solutions, together with the computational grunt and algorithms. Without all these elements, you don’t have AI.
Photo by imgix on Unsplash
Effective algorithms play a key role in the quality of the results in deep learning, and in their performance, particularly the speed of the training phase of the models. Sometimes it may also be necessary to combine many methods to produce useful output.
To determine performance in image recognition, training time is compared to the percentage of successful identification. The progress of the algorithms aims to improve one or the other. The size of the datasets is however critical to train a model. If the algorithm used is better than the training speed, which is often the case in convolutional neural network variants, then the recognition performance will not change when the trained model is run… with more training data, it will be longer. You need both better data sets and better algorithms so that the training is as fast as possible and the results generate minimal errors.
- Training an AI happens quickly
The training time of a single-function deep learning system takes hours or days, or even weeks for image recognition for medical imaging interpretation. What’s often overlooked is that the AI does not train alone. It requires a lot of iterations with human intervention.
“A drone shot of a running track in a stadium in Berlin” by Marvin Ronsdorf on Unsplash
The ideal of self-learning systems has not yet been achieved. Data scientists must do many tests and pay close supervision to select the right statistical models of segmentation, classification or prediction. For deep learning neural networks, researchers must perform tests to determine the major characteristics and evaluate their effectiveness with the available datasets. Iterations can be long and laborious. They can last several days, which is much longer than the compilation of code or the immediate nature of the interpreted software written today.
Once a machine learning model has been successfully trained, its operational execution is then quick. With deep learning, this is simple to understand: a convolutional image recognition neural network can include millions of parameters that must be adjusted by testing for each object in the test database. This generates a huge computational overhead. Once the model is trained, it suffices to load the static variables determined in the training phase and to pass the object to be recognized through the network of neurons. And since it is software, the cost of replication and execution of the solution is low.
In simple terms, AI gives us laborious training and an inexpensive operation. The neuromorphic cores in the latest mobile processors (Kirin 970 for Huawei and A11 Bionic Neural Engine for the iPhone 8 and X) are not powerful (1 to 2 TFlops/s) but they are only used for execution of neural networks already trained on servers with much more power. On the device, pre-built models are used for the recognition of images or speech.
The media also overlook that most AI solutions, such as those of medical imaging, rely on knowledge that is the result of lengthy scientific research carried out by thousands of specialists. When AlphaGo wins against the world champion, it does so by analyzing 150,000 games played by the best players in the world. This human expertise took a long time to build. The AI is a “free rider” which exploits this long accumulation of data.
Maybe one-day RNNs will interact with the physical world and accumulate high-level knowledge based on experimentation. Today, this principle is being explored using physical robots that learn to move by trial and error. The intellectual equivalent is not there yet, but researchers are working on it.
- AI is ‘free’
AI only appears free in consumer online services funded by advertising. Our perception of ‘free’ is always the case in business models where the user is not the buyer. This illusion is not specific to AI.
“Three businesswomen talking at a white table” by Tim Gouw on Unsplash
The emergence of AI is big business for the cloud and the data centers it requires for its operation. As an example, a single Nvidia DGX1 server costs an eye-watering $129K. The hardware costs will fall helped by the effect of Moores Law. However, as the hardware cost fall, the sophistication of AI applications will increase demanding more power. For the next few years at least the pace of hardware improvement will not catch up with the computing power demanded by AI.
In situations where software or an AI can replace human activity, there are cost savings, particularly in developed countries where wages are high. AI does not have to be free to compete.
- AI will exceed exponential growth leading to a ‘Singularity’ event
Progress AI performance is significant, however, the rate of improvements are unlikely to be sustained in the long term. Between the last two generations of Nvidia GPU (Pascal V100 2016 and Volta GV100 2017), the gross performance gain in the calculations related to deep learning range from a factor 1.8 to 9.3. This is related to the use of matrix multipliers in this new generation of GPUs, a technique that was established recently and incorporated into the newest neuromorphic processors.
Photo by Marc Sendra martorell on Unsplash
In practice, the training of a ResNet convolutional neural network for image recognition is 2.4 times faster and its execution in run-time is 3.7 times faster between these two generations of GPUs. This is great but we are far from exponential. It is, however, possible to make use of parallelization… if you line up 40 processors the training will be 100 times faster than with an older generation GPU since the deep learning frameworks know how to parallelize the training on several GPUs.
Exponential growth could, however, be achieved using as yet unproven forms of computational devices. Future optical and quantum processors will each be able to boost AI progress significantly in the execution of AI models and especially training.
-Artificial General Intelligence (AGI) will emerge from deep learning techniques
One flaw of this theorem is that it relies on a false premise that intelligence will emerge from increased computing power alone. The computational capabilities of the human brain (which calculates little and is massively parallel) are that not that similar to current AI machines (which do binary operations on matrices).
“A person in a glossy mask crouching on the reflective floor in a neon-soaked building” by Drew Graham on Unsplash
AGI is the mechanical outcome of the blind application of Moore’s Law. As more discoveries are made on the complexity of the brain, then the predicted near future emergence of AGI appears less and less likely. We still do not know how to describe how the brain works in terms of memory or computation. Neurons are much more nuanced than estimated 10 or 15 years ago.
Human intelligence is complex. It integrates the sophistication of the brain to our senses, our experiences and our perception of the world. Biological senses are even more difficult to imitate by machines than intellectual abilities. Reports always talk about the cortex that manages the intelligence but neglect the cerebellum, this part of the ultra-dense brain that includes four times more neurons than the cortex and manages automated learned tasks (walking, eye movements, driving, cycling, walking, dance, recognition, …).
AI futurists generally adopt a too linear view of the history of technology. In reality, progress follows several paths in parallel. Experience shows that machines paired with humans are superior to machines alone. Humans and machines do not have the same capabilities and complement each other. The machines already far exceed us in a lot of applications without it needing superhuman intelligence.
There is no doubt that AI will drive a major evolution of digital tools. But we must understand the issues and opportunities. These are not sensational. The potential of AI is huge for companies in all sectors of activity but still, you need to keep your head on your shoulders to apprehend it calmly. Today’s technologies are already rich enough to be creative and innovative. Over-automation has limitations that need to be tested. Companies must keep humans in the loop and not spend their time hiding behind software.
",2334,Artificial Intelligence,Adam Mackay,https://medium.com/s/story/creation-of-the-mythical-ai-bringing-artificial-intelligence-reporting-back-to-earth-c9629aab2d64,"Today the debates are endless as scientists, philosophers and the media struggle to define the nature of human intelligence and how much of it machines can gain.
According to current articles… IBM Watson is the swiss army knife AI who knows how to do everything, DeepMind AlphaGo is more intelligent than the best strategy game players, the latest Intel processor Loihi mimics the brain, autonomous cars are happening now and miracle marketing solutions predict in advance what you want.
Algorithmic progress is slow in machine learning because it relies on classification, prediction and segmentation methods whos roots are one to two decades old.
Deep learning requires assembly of these bricks that constantly evolve.
The general aim of AI development comprises two complementary objectives: to improve the qualitative performance of neural networks and to speed up training.
Deep learning has come to the fore as the miracle solution to address all problems.
Deep learning is so called because its neural networks are a multilayer.
Many papers have shown that human intelligence does not work like the current deep learning neural networks.
Data is one of the three key components of AI solutions, together with the computational grunt and algorithms.
Effective algorithms play a key role in the quality of the results in deep learning, and in their performance, particularly the speed of the training phase of the models.
To determine performance in image recognition, training time is compared to the percentage of successful identification.
If the algorithm used is better than the training speed, which is often the case in convolutional neural network variants, then the recognition performance will not change when the trained model is run… with more training data, it will be longer.
You need both better data sets and better algorithms so that the training is as fast as possible and the results generate minimal errors.
The training time of a single-function deep learning system takes hours or days, or even weeks for image recognition for medical imaging interpretation.
For deep learning neural networks, researchers must perform tests to determine the major characteristics and evaluate their effectiveness with the available datasets.
Once a machine learning model has been successfully trained, its operational execution is then quick.
With deep learning, this is simple to understand: a convolutional image recognition neural network can include millions of parameters that must be adjusted by testing for each object in the test database.
The media also overlook that most AI solutions, such as those of medical imaging, rely on knowledge that is the result of lengthy scientific research carried out by thousands of specialists.
Between the last two generations of Nvidia GPU (Pascal V100 2016 and Volta GV100 2017), the gross performance gain in the calculations related to deep learning range from a factor 1.8 to 9.3.
This is related to the use of matrix multipliers in this new generation of GPUs, a technique that was established recently and incorporated into the newest neuromorphic processors.
In practice, the training of a ResNet convolutional neural network for image recognition is 2.4 times faster and its execution in run-time is 3.7 times faster between these two generations of GPUs. This is great but we are far from exponential.
It is, however, possible to make use of parallelization… if you line up 40 processors the training will be 100 times faster than with an older generation GPU since the deep learning frameworks know how to parallelize the training on several GPUs. Exponential growth could, however, be achieved using as yet unproven forms of computational devices.
Future optical and quantum processors will each be able to boost AI progress significantly in the execution of AI models and especially training.
-Artificial General Intelligence (AGI) will emerge from deep learning techniques
The computational capabilities of the human brain (which calculates little and is massively parallel) are that not that similar to current AI machines (which do binary operations on matrices).
AGI is the mechanical outcome of the blind application of Moore’s Law. As more discoveries are made on the complexity of the brain, then the predicted near future emergence of AGI appears less and less likely.
Reports always talk about the cortex that manages the intelligence but neglect the cerebellum, this part of the ultra-dense brain that includes four times more neurons than the cortex and manages automated learned tasks (walking, eye movements, driving, cycling, walking, dance, recognition, …).
The machines already far exceed us in a lot of applications without it needing superhuman intelligence."
Implement Reinforcement learning using Markov Decision Process [Tutorial],"The Markov decision process, better known as MDP, is an approach in reinforcement learning to take decisions in a gridworld environment. A…","Implement Reinforcement learning using Markov Decision Process [Tutorial]
The Markov decision process, better known as MDP, is an approach in reinforcement learning to take decisions in a gridworld environment. A gridworld environment consists of states in the form of grids.
The MDP tries to capture a world in the form of a grid by dividing it into states, actions, models/transition models, and rewards. The solution to an MDP is called a policy and the objective is to find the optimal policy for that MDP task.
Thus, any reinforcement learning task composed of a set of states, actions, and rewards that follows the Markov property would be considered an MDP.
In this tutorial, we will dig deep into MDPs, states, actions, rewards, policies, and how to solve them using Bellman equations.
This article is a reinforcement learning tutorial taken from the book, Reinforcement learning with TensorFlow.
Markov decision processes
MDP is defined as the collection of the following:
States: S
Actions: A(s), A
Transition model: T(s,a,s’) ~ P(s’|s,a)
Rewards: R(s), R(s,a), R(s,a,s’)
Policy:


is the optimal policy
In the case of an MDP, the environment is fully observable, that is, whatever observation the agent makes at any point in time is enough to make an optimal decision. In case of a partially observable environment, the agent needs a memory to store the past observations to make the best possible decisions.
Let’s try to break this into different lego blocks to understand what this overall process means.
The Markov property
In short, as per the Markov property, in order to know the information of near future (say, at time t+1) the present information at time t matters.
Given a sequence,

, the first order of Markov says,

, that is,

depends only on

. Therefore,

will depend only on

. The second order of Markov says,

, that is,

depends only on

and

In our context, we will follow the first order of the Markov property from now on. Therefore, we can convert any process to a Markov property if the probability of the new state, say

, depends only on the current state,

, such that the current state captures and remembers the property and knowledge from the past. Thus, as per the Markov property, the world (that is, the environment) is considered to be stationary, that is, the rules in the world are fixed.
The S state set
The S state set is a set of different states, represented as s, which constitute the environment. States are the feature representation of the data obtained from the environment. Thus, any input from the agent’s sensors can play an important role in state formation. State spaces can be either discrete or continuous. The starts from start state and has to reach the goal state in the most optimized path without ending up in bad states (like the red colored state shown in the diagram below).
Consider the following gridworld as having 12 discrete states, where the green-colored grid is the goal state, red is the state to avoid, and black is a wall that you’ll bounce back from if you hit it head on:

The states can be represented as 1, 2,….., 12 or by coordinates, (1,1),(1,2),…..(3,4).
Actions
The actions are the things an agent can perform or execute in a particular state. In other words, actions are sets of things an agent is allowed to do in the given environment. Like states, actions can also be either discrete or continuous.
Consider the following gridworld example having 12 discrete states and 4 discrete actions (UP, DOWN, RIGHT, and LEFT):

The preceding example shows the action space to be a discrete set space, that is, a

A where, A = {UP, DOWN, RIGHT, and LEFT}. It can also be treated as a function of state, that is, a = A(s), where depending on the state function, it decides which action is possible.
Transition model
The transition model T(s, a, s’) is a function of three variables, which are the current state (s), action (a), and the new state (s’), and defines the rules to play the game in the environment. It gives probability P(s’|s, a), that is, the probability of landing up in the new s’ state given that the agent takes an action, a, in given state, s.
The transition model plays the crucial role in a stochastic world, unlike the case of a deterministic world where the probability for any landing state other than the determined one will have zero probability.
Let’s consider the following environment (world) and consider different cases, determined and stochastic:

Since the actions a

A where, A = {UP, DOWN, RIGHT, and LEFT}.
The behavior of these two cases depends on certain factors:
Determined environment: In a determined environment, if you take a certain action, say UP, you will certainly perform that action with probability 1.
Stochastic environment: In a stochastic environment, if you take the same action, say UP, there will certain probability say 0.8 to actually perform the given action and there is 0.1 probability it can perform an action (either LEFT or RIGHT) perpendicular to the given action, UP. Here, for the s state and the UP action transition model, T(s’,UP, s) = P(s’| s,UP) = 0.8.
Since T(s,a,s’) ~ P(s’|s,a), where the probability of new state depends on the current state and action only, and none of the past states. Thus, the transition model follows the first order Markov property.
We can also say that our universe is also a stochastic environment, since the universe is composed of atoms that are in different states defined by position and velocity. Actions performed by each atom change their states and cause changes in the universe.
Rewards
The reward of the state quantifies the usefulness of entering into a state. There are three different forms to represent the reward namely, R(s), R(s, a) and R(s, a, s’), but they are all equivalent.
For a particular environment, the domain knowledge plays an important role in the assignment of rewards for different states as minor changes in the reward do matter for finding the optimal solution to an MDP problem.
There are two approaches we reward our agent for when taking a certain action. They are:
Credit assignment problem: We look at the past and check which actions led to the present reward, that is, which action gets the credit
Delayed rewards: In contrast, in the present state, we check which action to take that will lead us to potential rewards
Delayed rewards form the idea of foresight planning. Therefore, this concept is being used to calculate the expected reward for different states. We will discuss this in the later sections.
Policy
Until now, we have covered the blocks that create an MDP problem, that is, states, actions, transition models, and rewards, now comes the solution. The policy is the solution to an MDP problem.

The policy is a function that takes the state as an input and outputs the action to be taken. Therefore, the policy is a command that the agent has to obey.

is called the optimal policy, which maximizes the expected reward. Among all the policies taken, the optimal policy is the one that optimizes to maximize the amount of reward received or expected to receive over a lifetime. For an MDP, there’s no end of the lifetime and you have to decide the end time.
Thus, the policy is nothing but a guide telling which action to take for a given state. It is not a plan but uncovers the underlying plan of the environment by returning the actions to take for each state.
The Bellman equations
Since the optimal

policy is the policy that maximizes the expected rewards, therefore,

,
where

means the expected value of the rewards obtained from the sequence of states agent observes if it follows the

policy. Thus,

outputs the

policy that has the highest expected reward.
Similarly, we can also calculate the utility of the policy of a state, that is, if we are at the s state, given a

policy, then, the utility of the

policy for the s state, that is,

would be the expected rewards from that state onward:

The immediate reward of the state, that is,

is different than the utility of the

state (that is, the utility of the optimal policy of the

state) because of the concept of delayed rewards. From now onward, the utility of the

state will refer to the utility of the optimal policy of the state, that is, the

state.
Moreover, the optimal policy can also be regarded as the policy that maximizes the expected utility. Therefore,

where, T(s,a,s’) is the transition probability, that is, P(s’|s,a) and U(s’) is the utility of the new landing state after the a action is taken on the s state.

refers to the summation of all possible new state outcomes for a particular action taken, then whichever action gives the maximum value of

that is considered to be the part of the optimal policy and thereby, the utility of the ‘s’ state is given by the following Bellman equation,

where,

is the immediate reward and

is the reward from future, that is, the discounted utilities of the ‘s’ state where the agent can reach from the given s state if the action, a, is taken.
Solving the Bellman equation to find policies
Say we have some n states in the given environment and if we see the Bellman equation,

we find out that n states are given; therefore, we will have n equations and n unknown but the

function makes it non-linear. Thus, we cannot solve them as linear equations.
Therefore, in order to solve:
Start with an arbitrary utility
Update the utilities based on the neighborhood until convergence, that is, update the utility of the state using the Bellman equation based on the utilities of the landing states from the given state
Iterate this multiple times to lead to the true value of the states. This process of iterating to convergence towards the true value of the state is called value iteration.
For the terminal states where the game ends, the utility of those terminal state equals the immediate reward the agent receives while entering the terminal state.
Let’s try to understand this by implementing an example.
An example of value iteration using the Bellman equation
Consider the following environment and the given information:

Given information:
A, C, and X are the names of some states.
The green-colored state is the goal state, G, with a reward of +1.
The red-colored state is the bad state, B, with a reward of -1, try to prevent your agent from entering this state
Thus, the green and red states are the terminal states, enter either and the game is over. If the agent encounters the green state, that is, the goal state, the agent wins, while if they enter the red state, then the agent loses the game.

,

(that is, reward for all states except the G and B states is -0.04),

(that is, the utility at the first time step is 0, except the G and B states).
Transition probability T(s,a,s’) equals 0.8 if going in the desired direction; otherwise, 0.1 each if going perpendicular to the desired direction. For example, if the action is UP then with 0.8 probability, the agent goes UP but with 0.1 probability it goes RIGHT and 0.1 to the LEFT.
Questions:
Find

, the utility of the X state at time step 1, that is, the agent will go through one iteration
Similarly, find

Solution:


R(X) = -0.04
Action as’



RIGHT
G
0.8+10.8 x 1 = 0.8RIGHTC0.100.1 x 0 = 0RIGHTX0.100.1 x 0 = 0
Thus, for action a = RIGHT,

Action as’



DOWN
C
0.800.8 x 0 = 0DOWNG0.1+10.1 x 1 = 0.1DOWNA0.100.1 x 0 = 0
Thus, for action a = DOWN,

Action as’



UP
X
0.800.8 x 0 = 0UPG0.1+10.1 x 1 = 0.1UPA0.100.1 x 0 = 0
Thus, for action a = UP,

Action as’



LEFT
A
0.800.8 x 0 = 0LEFTX0.100.1 x 0 = 0LEFTC0.100.1 x 0 = 0
Thus, for action a = LEFT,

Therefore, among all actions,

Therefore,

, where

and

Similarly, calculate

and

and we get

and

Since,

, and,

R(X) = -0.04
Action as’



RIGHT
G
0.8+10.8 x 1 = 0.8RIGHTC0.1–0.040.1 x -0.04 = -0.004RIGHTX0.10.360.1 x 0.36 = 0.036
Thus, for action a = RIGHT,

Action as’



DOWN
C
0.8–0.040.8 x -0.04 = -0.032DOWNG0.1+10.1 x 1 = 0.1DOWNA0.1–0.040.1 x -0.04 = -0.004
Thus, for action a = DOWN,

Action as’



UP
X
0.80.360.8 x 0.36 = 0.288UPG0.1+10.1 x 1 = 0.1UPA0.1–0.040.1 x -0.04 = -0.004
Thus, for action a = UP,

Action as’



LEFT
A
0.8–0.040.8 x -0.04 = -0.032LEFTX0.10.360.1 x 0.36 = 0.036LEFTC0.1–0.040.1 x -0.04 = -0.004
Thus, for action a = LEFT,

Therefore, among all actions,

Therefore,

, where

and

Therefore, the answers to the preceding questions are:


Policy iteration
The process of obtaining optimal utility by iterating over the policy and updating the policy itself instead of value until the policy converges to the optimum is called policy iteration. The process of policy iteration is as follows:
Start with a random policy,

For the given

policy at iteration step t, calculate

by using the following formula:

Improve the

policy by

This ends an interesting reinforcement learning tutorial. Want to implement state-of-the-art Reinforcement Learning algorithms from scratch? Get this best-selling title, Reinforcement Learning with TensorFlow.
Read Next:
How Reinforcement Learning works
Convolutional Neural Networks with Reinforcement Learning
Getting started with Q-learning using TensorFlow
",2269,Machine Learning,Fatema Patrawala,https://medium.com/s/story/implement-reinforcement-learning-using-markov-decision-process-tutorial-272012fdae51,"Implement Reinforcement learning using Markov Decision Process [Tutorial]
The Markov decision process, better known as MDP, is an approach in reinforcement learning to take decisions in a gridworld environment.
The MDP tries to capture a world in the form of a grid by dividing it into states, actions, models/transition models, and rewards.
Thus, any reinforcement learning task composed of a set of states, actions, and rewards that follows the Markov property would be considered an MDP.
In this tutorial, we will dig deep into MDPs, states, actions, rewards, policies, and how to solve them using Bellman equations.
In the case of an MDP, the environment is fully observable, that is, whatever observation the agent makes at any point in time is enough to make an optimal decision.
Therefore, we can convert any process to a Markov property if the probability of the new state, say
The actions are the things an agent can perform or execute in a particular state.
In other words, actions are sets of things an agent is allowed to do in the given environment.
Consider the following gridworld example having 12 discrete states and 4 discrete actions (UP, DOWN, RIGHT, and LEFT):
The transition model T(s, a, s’) is a function of three variables, which are the current state (s), action (a), and the new state (s’), and defines the rules to play the game in the environment.
It gives probability P(s’|s, a), that is, the probability of landing up in the new s’ state given that the agent takes an action, a, in given state, s.
Here, for the s state and the UP action transition model, T(s’,UP, s) = P(s’| s,UP) = 0.8.
Since T(s,a,s’) ~ P(s’|s,a), where the probability of new state depends on the current state and action only, and none of the past states.
Thus, the transition model follows the first order Markov property.
For a particular environment, the domain knowledge plays an important role in the assignment of rewards for different states as minor changes in the reward do matter for finding the optimal solution to an MDP problem.
There are two approaches we reward our agent for when taking a certain action.
Therefore, this concept is being used to calculate the expected reward for different states.
Until now, we have covered the blocks that create an MDP problem, that is, states, actions, transition models, and rewards, now comes the solution.
The policy is a function that takes the state as an input and outputs the action to be taken.
is called the optimal policy, which maximizes the expected reward.
Thus, the policy is nothing but a guide telling which action to take for a given state.
means the expected value of the rewards obtained from the sequence of states agent observes if it follows the
Similarly, we can also calculate the utility of the policy of a state, that is, if we are at the s state, given a
would be the expected rewards from that state onward:
state (that is, the utility of the optimal policy of the
state will refer to the utility of the optimal policy of the state, that is, the
where, T(s,a,s’) is the transition probability, that is, P(s’|s,a) and U(s’) is the utility of the new landing state after the a action is taken on the s state.
that is considered to be the part of the optimal policy and thereby, the utility of the ‘s’ state is given by the following Bellman equation,
is the reward from future, that is, the discounted utilities of the ‘s’ state where the agent can reach from the given s state if the action, a, is taken.
Say we have some n states in the given environment and if we see the Bellman equation,
For the terminal states where the game ends, the utility of those terminal state equals the immediate reward the agent receives while entering the terminal state.
Consider the following environment and the given information:
For example, if the action is UP then with 0.8 probability, the agent goes UP but with 0.1 probability it goes RIGHT and 0.1 to the LEFT.
, the utility of the X state at time step 1, that is, the agent will go through one iteration
The process of policy iteration is as follows:"
Clustering the Top 1%: Asset Analysis in R,The recent tax reform bill passed in the US has raised a lot of questions about wealth distribution in the country. While there’s been a…,"Cluster Dendrogram of Affluent US Households
Clustering the Top 1%: Asset Analysis in R
The recent tax reform bill passed in the US has raised a lot of questions about wealth distribution in the country. While there’s been a lot of focus on how the tax plan will impact income, there’s been less attention focused on how this plan impacts the assets of wealthy households.
The goal of this post is to show how the R programming language can be used to data mine publicly available sources to better understand the net worth of affluent households in the US. Using data from the 2016 Federal Reserve Survey of Consumer Finances, we investigate the following questions:
How rich are the top 1% and top 0.1% of households?
Are there different types of millionaires in the US?
How do asset allocations differ across different net worth segments?
To answer these questions, we present descriptive statistics of this survey data and perform cluster analysis on affluent households, which we identify as households with a net worth of more than $1,000,000 USD.
Based on the survey data, our analysis shows that the net worth of the top 1% of households in the US is $10.4M and the net worth of the top 0.1% of households is $43.2M. This post presents an analysis of the different asset compositions of millionaires, and shows how asset allocations differ between the top 10%, 1%, and 0.1% of households in the US. The R source code used to produce all results and figures presented in this post is available as a Jupyter Notebook.
Setting Up the Environment 
To perform cluster analysis on the affluent households in the US, we use several packages available from the CRAN library for R. For exploratory data analysis, we like to use the R kernel for the Jupyter notebook, since it enables data scientists to easily store notebooks on GitHub and share findings with other teams.
Setting up this environment is outside the scope of this post, but I’ve previously detailed our motivation for this setup in this post, and additional details for setting up Jupyter with R support are available here.
We’re now ready to start digging into the survey data to better understand the assets of affluent households in the US. To begin, we’ll load several libraries that will help us analyze the survey data and perform clustering.
The code block below shows the libraries that need to be loaded for executing this notebook. The readxl library is needed to read the source data and convert it into a data frame, the reldist and ENmisc libraries are used for computing distributions with weighted data sets, and the remaining libraries are used for cluster analysis.
Getting the Data
The next step is to download the data from the Federal Reserve website. The survey data is available as a zipped xlsx file. To download the data and load it into R as a data frame for analysis, we use the code block below. Since this is a large file, we make sure that we do not download it multiple times. The resulting file that we unzip is about 40MB and takes some time to load into a data frame.
Summary Statistics 
Now that we have the survey data loaded into R, we can start to analyze the asset allocation of the richest households in the US. We’ll start with some summary stats: how many survey participants and how many households are represented by this survey? Counting the number of rows in the data frame answers the first questions (31.2k), and adding up the weights of all of the survey respondents answers the second question (126M).
To answer questions about averages, such as what is the mean household net worth, we need to use weighted statistics (since the weight of one survey respondent can be much larger than others). To compute the mean net worth, we can use built-in R functions, which returns a value of $690k. However, since net worth is much closer to a log normal than normal distribution, we should use other approaches.
To compute the median value with weighted responses, we use the reldist library, which assigns more support to respondents with larger weights and less support to respondents with lower weights. When using this approach to compute the weighted median, we find that the 50th percentile for household net worth in the US is $97k.
The net worth of the top 1% is $10.4M and the net worth of the top 0.1% is $43.2M. We use the wtd.quantile function to compute these descriptive statistics, and the code sample above uses the fully quantified function names, due to collisions with the ENmisc package.
Demographic Data 
The survey data provides a number of different demographic variables that can be used to analyze net worth by different factors. These variables include race, marital status, education level, employment status, and others. The goal of this post is to show how asset allocation varies by net worth segment, and analysis of how these demographic factors impact net worth is left as an exercise for the reader.
One demographic variable that we did explore is the impact of age on household net worth. As expected, the median net worth does increase as the head of household becomes older, with net worth plateauing around 60 years old.
The code above shows how to display a box plot of the survey data by age, using the ENmisc package to compute weighted distributions. The results of this plot show that the median net worth of households in the US are $114k at 40, $163k at 50, and $243k at 60.

Asset Allocation 
The next step in the notebook is to evaluate the asset allocation of different net worth segments. For this analysis, we define a segment based on the log10 value of the household net worth. That means all 5-figure households get grouped together, all 6-figure households get grouped together, as so on.
The code above computes the asset allocation by household, by dividing the amount of assets for a household, such as business equity (BUS) over the total number of financial and non-financial assets, excluding debt. The “…” pattern is used to indicate that multiple rows have been excluded from the code snippet that are listed in the full notebook. The second code block groups the households into different net worth segments, and the third block plots the results as shown below.
The results show that the wealthy, ultra wealthy, and billionaires have large amounts of assets in business equity (stock and options). The wealthy have only a small percentage of assets in retirement funds, and instead have assets in stock, mutual funds, and residential and commercial real estate.

Clustering Millionaires
So far, we’ve looked at aggregate statistics of wealthy households, but that doesn’t tell us about the different types of affluent households. To understand how assets vary across affluent homes, we can use cluster analysis. One of the most useful ways to visualize the difference between instances in a sample population is to use factor maps to visualize the variance in the population.
The code above first filters survey respondents to affluent households, with a net worth of more than $1M USD, and then plots a factor map using principal component analysis (PCA). The figure below shows how the different assets impact the trajectory of plotting a household across the two principal components discovered via PCA.
The results plotted below show that there are a few different assets groups that vary across affluent net worth. The most significant factor is business equity. Some other groupings of factors include investment assets (STOCKS, BONDS) and real estate assets/retirement funds.

How many clusters to use?
We’ve now shown signs that there are different types of millionaires, and that assets vary based on net worth segments. To understand how asset allocation differs by net worth segment, we can use cluster analysis. We first identify clusters in the affluent survey respondents, and then apply these labels to the overall population of survey respondents.
To determine how many clusters to use, we created a cluster dendrogram using the code snippet above, shown as the header image of this post. We also varied the number of clusters, k, until we had the largest number of distinctly identifiable clusters.
If you’d prefer to take a quantitative approach, you can use the fviz_nbclust function, which computes the optimal number of clusters using a silhouette metric. For our analysis, we decided to use 7 clusters.
To cluster the affluent households into unique groupings, we used the CLARA algorithm. A visualization of the different clusters is shown below. The results are similar to PCA and the factor map approach discussed above.

Cluster Descriptions 
Now that we’ve determined how many clusters to use, it’s useful to inspect the clusters and assign qualitative labels based on the feature sets. The code snippet below shows how to compute the average feature values for the 7 different clusters.
The results of this code block are shown below. Based on these results, we came up with the following cluster descriptions:
V1: Stocks/Bonds — 31% of assets, followed by home and mutual funds
V2: Diversified — 53% busequity, 10% home and 9% in other real estate
V3: Residential Real Estate — 48% of assets
V4: Mutual Funds — 50% of assets
V5: Retirement — 48% of assets
V6: Business Equity — 85% of assets
V7: Commercial Real Estate — 59% of assets
With the exception of cluster V7, containing only 3% of the population, most of the clusters are relatively even in size. The second smallest cluster represents 12% of the population while the largest cluster represents 20%. You can use table(groups) to show the unweighted cluster population sizes.

Cluster Populations by Net Worth Segments
The last step in this analysis is to apply the different cluster assignments to the overall population, and to group the populations by net worth segments. Since we trained the clusters on only affluent households, we need to use a classification algorithm to label the non-affluent households in the population. The code snippet below uses knn to accomplish this task.
The remaining code blocks compute the number of households that are classified as each cluster, for each of the net worth segments.
The results of this process are shown in the figure below. The chart shows some obvious and some novel results: home ownership and retirement funds make up the majority of assets for non-affluent households, there is a relatively even mix of clusters around $2M (excluding commercial real estate and business equity), and business equity dominates net worth for the ultra-wealthy households, followed by other investment assets.

Summary
In this post we used R to download and analyze data from the 2016 Federal Reserve survey of consumer finances to understand how wealthy the top households are in the US, and to cluster affluent households by asset allocation. We identified 7 distinct groups of millionaires, and showed how the distribution of clusters varies based on net worth segment. Please keep in mind that the results presented are from weighted survey data, and may not be representative of the overall US population.
Ben Weber is a principal data scientist at Zynga. We are hiring!
",2436,Data Science,Ben Weber,https://medium.freecodecamp.org/clustering-the-top-1-asset-analysis-in-r-6c529b382b42,"The goal of this post is to show how the R programming language can be used to data mine publicly available sources to better understand the net worth of affluent households in the US.
To answer these questions, we present descriptive statistics of this survey data and perform cluster analysis on affluent households, which we identify as households with a net worth of more than $1,000,000 USD.
Based on the survey data, our analysis shows that the net worth of the top 1% of households in the US is $10.4M and the net worth of the top 0.1% of households is $43.2M.
To perform cluster analysis on the affluent households in the US, we use several packages available from the CRAN library for R.
We’re now ready to start digging into the survey data to better understand the assets of affluent households in the US.
Now that we have the survey data loaded into R, we can start to analyze the asset allocation of the richest households in the US.
To answer questions about averages, such as what is the mean household net worth, we need to use weighted statistics (since the weight of one survey respondent can be much larger than others).
The survey data provides a number of different demographic variables that can be used to analyze net worth by different factors.
The goal of this post is to show how asset allocation varies by net worth segment, and analysis of how these demographic factors impact net worth is left as an exercise for the reader.
The next step in the notebook is to evaluate the asset allocation of different net worth segments.
The second code block groups the households into different net worth segments, and the third block plots the results as shown below.
To understand how assets vary across affluent homes, we can use cluster analysis.
The code above first filters survey respondents to affluent households, with a net worth of more than $1M USD, and then plots a factor map using principal component analysis (PCA).
The results plotted below show that there are a few different assets groups that vary across affluent net worth.
We’ve now shown signs that there are different types of millionaires, and that assets vary based on net worth segments.
To understand how asset allocation differs by net worth segment, we can use cluster analysis.
The last step in this analysis is to apply the different cluster assignments to the overall population, and to group the populations by net worth segments.
The remaining code blocks compute the number of households that are classified as each cluster, for each of the net worth segments.
In this post we used R to download and analyze data from the 2016 Federal Reserve survey of consumer finances to understand how wealthy the top households are in the US, and to cluster affluent households by asset allocation.
We identified 7 distinct groups of millionaires, and showed how the distribution of clusters varies based on net worth segment."
Pursue to Experience the Touching in Jay-Chou's World,"In this article, I’ll illustrate the work what I done. First, the motivation will be discussed. Second, I will introduce the fundamental…","Pursue to Experience the Touching in Jay-Chou's World
Figure 1. The sport field watching scenario in different image space
In this article, I’ll illustrate the work what I done. First, the motivation will be discussed. Second, I will introduce the fundamental theory of my work. At last, I will show the revised version of my idea, and the result of my work. The copyright of the video and the related image is own by JVR Music International Ltd. and Jay Chou. The original video site is here. If there’s any issue about the copyright, please inform me and I will remove these images.
Motivation
Figure 2. The shot of waiting-for-you MV
In 18, January 2018, the famous singer Jay Chou released his first single video in the latest year. The name of this song is called waiting for you(等你下課). For this precious day, he gave the warming blessing to the fans:
這首歌是送給自己、也是送給你們的生日禮物
Maybe everyone has this touching in your life. In our young, we might do some crazy but memorable things with the girl we like. However, the girl might not like you very much. After few years. You might lay on the sport field and watch the sky. Also, we not only remember the letter which write to the girl we miss, and the things we done, but also wish you can realize: there’s a person which very care about you in these years.
I was touching by the song, the lyrics and the screen. Each frame shows the unique feeling toward my heart. The first idea is that I want to live in Jay Chou’s world, and try to change the ending of this period of emotion. However, we cannot live in the film. How to try to live in the film is the topic issue in this work.
Fundamental Idea about Style Transfer
At first, I try to gain the touching in Chou’s video. Next, we can transfer the question into the style transfer problem. In recent years, the scientist try to use deep learning technique to solve this problem. The pioneer about these approach is in this article[1].
To give a brief classification of these methods, there’re two big types of method to solve the problem. The first one is pure convolutional neural network (pure-CNN) based. There’re two components in this approach. The one component is render network which is just as the usual hour-glass shape (If you don’t know what I mean, please refer to my previous article). The another one is feature extraction network, and the VGG-16 is commonly used in this part. The real-world image and style image will extract the symbol of feature by the feature extraction network, and the loss can be computed.
The other method is generative adversarial network (GAN) based. In this approach, there’re two components in the structure: generator and discriminator. The generator will try to render the image, and the discriminator will judge if it’s in the real image space or in style image space.
Classical Model: CycleGAN
Figure 3. The idea of CycleGAN
In the end of March 2017, there’s a classical model purposed which called Cycle-Consistent Adversarial Networks (CycleGAN)[2]. This project got thousands of stars on GitHub pages in the short time. The most creative idea is that it can reach unpaired training image. For example, you cannot find two horse and zebra doing the same action in the same environment and background. However, we can reach horse-zebra transformation by CycleGAN. This model can also achieve some others tasks, including image to photo and summer to winter which is shown in Figure 3.
Figure 4. The loss computation topology of CycleGAN
The training path of CycleGAN is shown in Figure 4. In our application scenario, we can split the image as two image space: real-image space (real space) and waiting-for-you space (wait space). In the whole structure, there’re four components: wait-to-real generator, real discriminator, real-to-wait generator and wait discriminator. The corresponding generator will transfer the input image to the target image domain.
There’re two paths in this topology. In the first path, the waiting image will be transferred by wait-to-real generator firstly. Then the real discriminator will judge if the transfer result is at real image domain or not. Next, this image will be transferred back into waiting space by real-to-wait generator, and the result is called reconstructed image. At last, we can compute the lasso loss (l1 loss) between original image and reconstructed image.
The other path in this topology is the reversed direction which opposed to the first path. The real image will be transferred by real-to-wait generator, then judge if it’s at wait image domain or not by wait discriminator. Next, the generated image will be transferred back by wait-to-real generator, and compare the l1 loss at the end.
Figure 5. The identity mapping loss in CycleGAN
There’s another key issue. To make the generated image keep the original color tone, the author also included the identity mapping loss. The idea of identity mapping loss is that the image will keep the same structure and intensity by generator whose target image space is as the same as the input’s. The idea of identity mapping loss is shown in Figure 5. By adding this l1 loss, the generator can ensure the identity mapping between the same image domain.
Mask-CycleGAN
Figure 6. The shot of screen and mask
However, after I observe the image in wait space, the obstacle I imagine is that the cover text might influence the transfer result. In the most frames of the video, the text just give the hint about the lyrics. However, during the loss computing, we will still compute the difference between these text region, and it’s not necessary for the image domain transfer problem! Thus, I select the pixels which is too white, and do the dilation for one times. The result is called mask by myself, and the mapping between waiting image and mask is illustrated in Figure 6.
Figure 7. The loss computation topology of Mask-CycleGAN
The revised version of loss computation topology is shown in Figure 7. As we can see, before the computation of l1 loss in wait space, the both image will multiply the mask to remove the text region. However, this change will make the variance of those text region too high. In order to control the variance. We set the probability parameter α to control this revision. In our experiments, α is 0.5.
Result
In our experiments, the hardware we use is GTX 1080-ti. There’re two models will be trained in this works: usual CycleGAN and Mask-CycleGAN. Each model train for 10 epochs, and it should spend around 1 day to complete the training. The training data we use in wait space is the screen in the waiting-for-you video. The image is captured for each 10 frame, and the number of image in waiting space is 749.
On the other hands, we use MS-COCO dataset to become the training data in real space. In the original paper, the natural image was used to do the photo-image transfer training. However, there’s some unnatural object which occur in our waiting space, such as building and school. In order to increase the shared semantic between the two image space, we drop the original Flickr dataset and adopt MS-COCO which capture more wild semantic in the real world.
Figure 8. The render result 1
The Figure 8 shows the render results. The 1st and 2nd rows show the first path (waiting →real →waiting)we mentioned in our previous section. On the other hand, the 3rd and 4th rows shows the second path(real →waiting → real). The left part is the render result of usual CycleGAN, and the right side is the render result of Mask-CycleGAN. The Figure 9 shows another training result.
Figure 9. The render result 2
As you can see, the color tone in real world is very strong. However, since there’re a lot of night star images in waiting space, the color tone in waiting world is rather dark. For the texture field, the image in real world space is more detailed. Conversely, the image in waiting space is more smooth. The feeling of waiting space is just like using pastel painting on the screen with soft impression.
Figure 10. The loss curve of two models
We can compare the result from the loss curve. The Figure 10 shows the loss curve of both strategies. As you can see, the loss of both models have convergence trend as the time extending. It proves that our idea can work in real case!
Figure 11. The render result toward NCTU sport field
At last, I show the render result toward our school’s sport field in Figure 11. The left one is the original image, and the right side is the render result. Unfortunately, we can just learn the blur and dark painting toward the waiting space. It still has some distance to reach the style in Jay Chou’s world.
Increase Shared Semantic
However, as you can see in the Figure 8 and Figure 9, the result of render isn’t perfect. There two reason I guess. First, the MS-COCO dataset doesn’t have enough shared semantic that the generator can learn. In original CycleGAN paper, the author also discussed that the transformation between cat and dog will be fail since they didn’t have enough shared information.
Second, the adversarial loss I adopt is least square loss. According to the original paper of LSGAN[3], the least square loss will give more large penalty to the image if it’s fake, and the trend is shown in Figure 12. As the result, the image might become timid to generate the image with specific pastel style.
Figure 12. The penalty circustance between usual GAN and LSGAN
In order to overcome these two disadvantages, I try to use original CycleGAN structure with ResNet-6 generator and DCGAN discriminator. Next, I try to use original binary cross entropy loss as the adversarial loss. At last, I use another dataset which is collected by ourselves. This dataset contains the image which comes from the cover of original MV. Moreover, we adopt the latest waiting-for-you MV which Jay Chou released in 21 February, 2018 that can increase the diversity and shared message.
In this dataset, there are 1782 images in real world modality and 2039 images in waiting-for-you image modality. Moreover, there are some fade out images which is much dark than the other one. In order to prevent the render result becoming dark, we ignore these images with low brightness.
Figure 13. The image of different dataset
The circumstance between three modalities is shown in Figure 13. The left most image comes from MS-COCO dataset, and the middle image comes from our collected dataset. As you can see, the middle image has more share information toward the right most image rather than the left one.
Figure 14. The Mask-CycleGAN training result after changing dataset
The training result after adopting our collected dataset as training data is shown in Figure 14. The left column is the original image which locates in real world modality. Additionally, the middle column is the transfer results and the right column is the reconstruction results. As you can see, the generator can have much better performance than the usual one.
So what’s the problem that causes the worse performance in the initial implementation? At last, we try to use least square error again, and the result is shown in Figure 15. As you can see, the result has less variance than Figure 14, and the contour of object is more clear than the Figure 14 too. As the result, we make a simple inference: increasing the share semantic of image could increase the property which locates in different image domain!
Figure 15. The Mask-CycleGAN training result after adopting least square loss
Conclusion
In this article, we implement the CycleGAN to try to transfer the image to the waiting-for-you image space. Second, to avoid to fit the lyrics region in the film image, the mask idea is purposed to get rid of the computation of specific region. At last, we shows that the generator can transfer the image with soft painting impression.
The whole code and pre-trained model can be downloaded in the project site, and the re-implementation which adopt our collected dataset can be found here. Additionally, the dataset which collected by ourself can be found here. You can also do the video transfer to transfer the whole video to the waiting-for-you image space, and create your unique touching!
Reference
[1] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge, “Image Style Transfer Using Convolutional Neural Networks,” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, Nevada, USA, 27–30 June, 2016, pp. 2414–2423.
[2] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros, “Unpaired Image-to-Image Translation using CycleConsistent Adversarial Networks,” In International Conference on Computer Vision (ICCV), Venice, Italy, 22- 29, October, 2017, pp. 2223–2232.
[3] Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, and Stephen Paul Smolley, “Least Squares Generative Adversarial Networks,” arXiv: 1611.04076 [cs.CV], April, 2017.
",2112,Machine Learning,Sunner Li,https://medium.com/s/story/pursue-to-experience-the-touching-in-jay-chous-world-5689143371e8,"The real-world image and style image will extract the symbol of feature by the feature extraction network, and the loss can be computed.
The generator will try to render the image, and the discriminator will judge if it’s in the real image space or in style image space.
In the end of March 2017, there’s a classical model purposed which called Cycle-Consistent Adversarial Networks (CycleGAN)[2].
In the first path, the waiting image will be transferred by wait-to-real generator firstly.
Then the real discriminator will judge if the transfer result is at real image domain or not.
Next, this image will be transferred back into waiting space by real-to-wait generator, and the result is called reconstructed image.
The real image will be transferred by real-to-wait generator, then judge if it’s at wait image domain or not by wait discriminator.
Next, the generated image will be transferred back by wait-to-real generator, and compare the l1 loss at the end.
To make the generated image keep the original color tone, the author also included the identity mapping loss.
The idea of identity mapping loss is that the image will keep the same structure and intensity by generator whose target image space is as the same as the input’s.
The idea of identity mapping loss is shown in Figure 5.
By adding this l1 loss, the generator can ensure the identity mapping between the same image domain.
However, after I observe the image in wait space, the obstacle I imagine is that the cover text might influence the transfer result.
However, during the loss computing, we will still compute the difference between these text region, and it’s not necessary for the image domain transfer problem!
The result is called mask by myself, and the mapping between waiting image and mask is illustrated in Figure 6.
The loss computation topology of Mask-CycleGAN
The revised version of loss computation topology is shown in Figure 7.
As we can see, before the computation of l1 loss in wait space, the both image will multiply the mask to remove the text region.
There’re two models will be trained in this works: usual CycleGAN and Mask-CycleGAN.
The training data we use in wait space is the screen in the waiting-for-you video.
On the other hands, we use MS-COCO dataset to become the training data in real space.
In order to increase the shared semantic between the two image space, we drop the original Flickr dataset and adopt MS-COCO which capture more wild semantic in the real world.
However, since there’re a lot of night star images in waiting space, the color tone in waiting world is rather dark.
For the texture field, the image in real world space is more detailed.
The left one is the original image, and the right side is the render result.
According to the original paper of LSGAN[3], the least square loss will give more large penalty to the image if it’s fake, and the trend is shown in Figure 12.
In order to overcome these two disadvantages, I try to use original CycleGAN structure with ResNet-6 generator and DCGAN discriminator.
The Mask-CycleGAN training result after changing dataset
The training result after adopting our collected dataset as training data is shown in Figure 14.
The left column is the original image which locates in real world modality.
At last, we try to use least square error again, and the result is shown in Figure 15.
The Mask-CycleGAN training result after adopting least square loss
In this article, we implement the CycleGAN to try to transfer the image to the waiting-for-you image space.
At last, we shows that the generator can transfer the image with soft painting impression."
Post GDPR: Responsible AI Legislation,What will the responsibilities of marketers be in the age of automated processing?,"Post GDPR: Responsible AI Legislation
What will the responsibilities of marketers be in the age of automated processing?
Image credit: anui.com
Legislation follows innovation like hangovers follow red wine, and the impact can be extensive. In January this year, there were only rumblings of new cryptocurrency legislation in South Korea, and the value of Bitcoin fell by more than 15%, a drop from which it has not recovered. The 2018 General Data Protection Regulation (GDPR) come as a long overdue update of the 1998 Data Protection Act and the Privacy and Electronic Communications Regulations (PECR). The updates include conditions like the right to be removed from a given database, and the right to be notified of data sales to third parties.
Now we are in the midst of the Cambridge Analytica data scandal, and the amount of personal data being collected has become a central matter of public debate. Thousands of individuals are requesting to download the information that Facebook holds on them, and finding that the records extend far beyond what they expected (going so far as to include call and SMS activity), though not beyond the permissions they granted to the app (whether through active opt-in or default).
Given that artificial intelligence, (particularly big data analytics and machine learning), has become an essential tool in data collection and interpretation, it is reasonable to expect AI specific legislation in the near future, designed to sit alongside the GDPR.

Within the GDPR framework, AI is referenced in Article 22(1), in the form of automatic processing, and it states that any person — the data subject — has the right:
“not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her”
The terminology is the fascinating aspect here, as neither ‘legal’ or ‘significant’ effects are defined by the legislation. The Article 29 Data Protection Working Party (WP 29), working together with the EU data protection authorities, have adopted the Guidelines on Automated Decision Making, which offer the following broad definitions of the two terms:
Legal: “a legal effect suggests a processing activity that has an impact on someone’s legal rights, such as the freedom to associate with others, vote in an election or take legal action”
Significant: “for data processing to significantly affect someone the effects of the processing must be more than trivial and must be sufficiently great or important to be worthy of attention”
Trending AI Articles:
1. How I used machine learning as inspiration for physical paintings
2. MS or Startup Job — Which way to go to build a career in Deep Learning?
3. TOP 100 medium articles related with Artificial Intelligence
Legal cases determining which circumstances constitute ‘significant’ will provide an interesting insight into what degree of impact upon the circumstances, behaviours and choices of an individual are considered acceptable within the framework.
So, what’s the impact for marketers?
The purpose of marketing is solely to influence the behaviours and choices of any given individual. The more information a brand has about you, your preferences and your lifestyle, the closer they can get to delivering the message that will convert you.
Users of the internet generate, on average, 2.5 quintillion bytes of data each day, across a huge range of sites and applications. The challenge for brands is in determining what data is valuable to them in accurately profiling you.

Under GDPR, companies can no longer collect any data on the basis that it might be useful. Every piece of information they hold must considered necessary for the specified, explicit and legitimate purposes for which it was collected. Data can also only be held for as long as it is relevant and necessary for the consented purpose.
A second limitation is upon the sale of data to third parties, which can now only be done with a data subjects express permission. Gone are the days of buying data trawled from the backwaters of the internet, and marketing managers everywhere are starting to get a bit hot under the collar.

How, then, are brands to get close to their consumers? The aloe vera balm for marketers comes in the form of GDPR 22(2), which, as with much of GDPR, states that the automatic processing and profiling is acceptable provided the data subject has consented to it (and they have the right to contend any decision made about them). To gain this consent, GDPR requires data controllers to inform the subject of the activity, provide a meaningful explanation of the logic, and explain the significance and envisaged consequence of the processing. Anyone who thinks this sounds simple is almost certainly not the one doing the job.
The difficulty is that, as AI systems become more sophisticated, they also demonstrate a tendency towards becoming more opaque. It isn’t easy to distill an increasingly complex algorithm into a simple explanation, particularly within the constraints of proprietary data sets, a lack of data logs, and the importance of factoring in learning components when explaining algorithms.
Consequently, all B2C companies should be examining their data collection strategy, and ensuring that they have a framework for seeking the specific pieces of information that are relevant to the purpose, and most valuable in profiling a consumer in relation to their brand offer. This means that, when they ask individuals for permission, every aspect is entirely transparent, and the regulations are factored in at the AI product development level.
You’ve cleansed your data. You’ve identified what’s relevant. And you’ve found a simple way of explaining it. Now what?
We return now to the question of ‘significant impact’. One driving force of AI development is consumer lethargy; people are fundamentally lazy. The desire for convenience has so far trumped (in the majority of cases) concern for personal privacy. However, as automated decision making brings brands closer, and enables them to exert greater influence over individuals via a vast array of channels, what will constitute moral responsibility in the arena of AI application?
By 2020, in China, every one of the 1.3 billion residents will be required to participate in a Social Credit System, designed to evaluate trustworthiness derived from every type of personal data, including information on demographics, preferences and behaviours. Structures like this, including credit scores, exist now across the globe, but there has not, so far, been one which is so all encompassing. The fundamental question is eternal, quis custodiet ipsos custodes? Who owns the algorithm, what elements does it contain, and which information does it privilege? The argument against mandatory disclosure of these factors is that the system might be gamed by individuals, but the consequences of nondisclosure are much more severe.
Under the proposals, individuals who are deemed trustworthy will receive rewards including taking out loans, renting cars without deposits, faster check-ins, travel without supporting documents and fast tracked visa applications. Higher scores are a status symbol, supporting Rachel Botsman’s conclusion that the system is a form of gamified obedience based on distributed trust. The dystopian aspect comes in being deemed ‘untrustworthy’ by an undisclosed standard. In February 2017, 6.15 million citizens had been banned from taking flights for ‘social misdeeds’, and another 1.65 million blacklisted people cannot take trains.
This is an extreme example of the application of automated processing, but these outcomes clearly constitute ‘significant impacts’ upon an individual. Is it enough, then, that the data subjects opted in as the SCS is currently voluntary? This question must be asked at every level and reassessed for every application case. In the case of gambling advertisements, does it constitute a significant effect to use hyper targeting to serve up a message to an individual, to which they consented, which will prompt them to step into a betting shop even though they are on their last dollar? It isn’t new, but it’s absolutely critical that these factors are considered at the design and legislation level of applications of developing technologies under the umbrella of artificial intelligence.
If you’d like to know more, come speak to us at Adeptiv UK. We are a data-driven dialogue agency in London which specialises in helping clients get, keep and grow customers at scale.
Image credit: 360training.com




",1365,Privacy,Tessa Darbyshire,https://becominghuman.ai/post-gdpr-responsible-ai-legislation-e219d3a1cf07,"The updates include conditions like the right to be removed from a given database, and the right to be notified of data sales to third parties.
Given that artificial intelligence, (particularly big data analytics and machine learning), has become an essential tool in data collection and interpretation, it is reasonable to expect AI specific legislation in the near future, designed to sit alongside the GDPR.
Within the GDPR framework, AI is referenced in Article 22(1), in the form of automatic processing, and it states that any person — the data subject — has the right:
“not to be subject to a decision based solely on automated processing, including profiling, which produces legal effects concerning him or her or similarly significantly affects him or her”
Legal cases determining which circumstances constitute ‘significant’ will provide an interesting insight into what degree of impact upon the circumstances, behaviours and choices of an individual are considered acceptable within the framework.
The aloe vera balm for marketers comes in the form of GDPR 22(2), which, as with much of GDPR, states that the automatic processing and profiling is acceptable provided the data subject has consented to it (and they have the right to contend any decision made about them).
To gain this consent, GDPR requires data controllers to inform the subject of the activity, provide a meaningful explanation of the logic, and explain the significance and envisaged consequence of the processing.
Consequently, all B2C companies should be examining their data collection strategy, and ensuring that they have a framework for seeking the specific pieces of information that are relevant to the purpose, and most valuable in profiling a consumer in relation to their brand offer.
However, as automated decision making brings brands closer, and enables them to exert greater influence over individuals via a vast array of channels, what will constitute moral responsibility in the arena of AI application?
By 2020, in China, every one of the 1.3 billion residents will be required to participate in a Social Credit System, designed to evaluate trustworthiness derived from every type of personal data, including information on demographics, preferences and behaviours.
This is an extreme example of the application of automated processing, but these outcomes clearly constitute ‘significant impacts’ upon an individual.
It isn’t new, but it’s absolutely critical that these factors are considered at the design and legislation level of applications of developing technologies under the umbrella of artificial intelligence."
4 Steps to Good Narrow AI,"For people to start acting on a problem, much less pay attention to it, something has to really go wrong. As noise is being turned into…","4 Steps to Good Narrow AI

For people to start acting on a problem, much less pay attention to it, something has to really go wrong. As noise is being turned into information, I think we’ve had some fundamental shifts in our social contract that modern society is just beginning to pay attention to. In 2010, society was hardly aware of the kind of information organizations had on people. Now, at the advent of AI’s widespread integration into our lives, an increasing number of events in the digital world (Equifax, Snowden) is forcing us to think hard about the implications of the technology we use every day.
Many loud, influential voices (perhaps most notably Elon Musk) are wary of the next 40 years of technology, framing it figuratively and literally as its own autonomous being. AI is at the core of these discussions, and there are some related applications that deserve concern. AI is showing the ease with which pseudo-science can sneak back into our institutions and the power just a few companies have over what we think.
What can we do?
To start, practitioners could use a brush up on Data Science 101, especially concepts like “Correlation does not imply causation” and “informed consent”.
“(w) ‘Correlation does not imply causation’ is a phrase used in science and statistics to emphasize that a correlation between two variables does not necessarily imply that one causes the other.
…
(yy) ‘Informed consent’ denotes the agreement by a person to a proposed course of conduct after the data scientist has communicated adequate information and explanation about the material risks of and reasonably available alternatives to the proposed course of conduct.”
-From the Data Science Association’s Code of Conduct
Fudging these standards of science has directed questioning eyes at the tech industry. Blood is in the water when it comes to antitrust regulation. Leaders inside and outside the tech industry are also calling for laws and principles of safe technology, again with a central focus on AI. These range from Oren Etzioni’s update on Isaac Asimov’s famous Three Laws of Robotics, to the research community’s Asilomar AI Principles, to other executives’ own rules. [We have a full literature survey of principles, ethics, and other relevant rules for AI in society here.]
Most of these rules are about far-fetched AGI or high-level moral imperatives. No one really disagrees on the need for virtues, but what are they really changing? The reality of the industry is AI is still very narrow and splintered. Not only are we far away from tying those capabilities together into AGI, we are also without many common standards and the ideas put forth are not very actionable. We do have a few players coming together on missions like the Partnership on AI, but we need to do more to set high standards of quality and security and lay the foundations for even being capable of meeting the moral imperatives set by these philosophers and futurists.
These rules are not just about how we translate our human values into machine outcomes, but also how machine outcomes impact our values. In developing our methodology for AI-First Design at Element AI, we saw that as designers we can’t ignore that feedback loop and need to include it in our overall design process. It is time to stop treating AI like a black box and be willing to shine a light on what the technology is really doing in order to renovate our social contract consciously rather than automatically.
From Element AI, AI-First Design (AI1D) Methodology
Self-regulating will be as important as governmental regulation. For one, legislation will take some time to get up to speed, but tougher rules are coming thanks to the recognition of the great power big tech holds with its data.
There are those, too, who are calling for regulation to give themselves a chance to catch up. They base those calls on reasonable claims of the need for assuring consumers about the use of their data and the need for clarity and confidence in digital technologies/services.
When the government acts, hopefully it will turn into something positive, but the industry should also show some leadership and help frame this debate. We should fight for the industry to be transparent, accountable, and good for humanity so that people don’t gang up against this tech in a backlash.
4 steps to good narrow AI
Transparency is the hard part. The enforceability of the regulation and accountability of the practitioners hinge on transparency. This is a real can of worms for our industry because at first glance it goes directly against many business models.
But we have a three-legged stool problem. For us to maximize the benefit of AI, we need to balance the benefits to the user, society, and the industry. If one leg is too long, or if one leg is broken or damaged (say due to unsafe AI), the whole thing threatens to topple over. That is why having clear, well-planned rules is important: to keep AI fair and working for good.
As the creators of AI systems, we are closest to ensuring the proper setup for keeping the stool balanced, and have a vested interest in leading the healthy development of an industry that can be regulated from without and from within.
In order for our industry to start being accountable, I think we should follow four steps with the systems we are building:
Make it Predictable — What is the purpose? Have you stated your intent of how you will make use of that purpose?
Make it Explainable — Is it clear that you are achieving that intent? Can the user ascertain why a result happened?
Make it Secure — Is the stated purpose stable? Have you tested it with some shock tests for corruptibility?
Make it Transparent — Have you hit publish or made this information auditable?
1. Predictable — What is the purpose? Have you stated your intent of how you will make use of that purpose?
In laying out their ethics for narrow AI, Nick Bostrom and Eliezer Yudkowsky said, “[These are] all criteria that apply to humans performing social functions; all criteria that must be considered in an algorithm intended to replace human judgment of social functions.” When you meet someone for an exchange, you are going to want to understand their intent. The digital world has tricked us into ignoring that, and I think gotten to a point where we can no longer make a strong claim of “informed consent.”
We need to be clear that machines do not have their own intent. Right now we have many algorithms that seem to do the same thing, like image recognition, but their purposes are different. One may look at the clothes, pose, and background, while another may look solely at the permanent features of someone’s face.
Then it is what we do with these tools, our intent, that it is also important to be clear about. Why are you identifying faces? What are you doing with that output (or who are you selling it to)?
2. Explainable — Is it clear that you are achieving that intent? Can the user ascertain why a result happened?
The UI of software until recently has exposed everything that’s in the software. You could query it and get access to the database. Now software runs on the cloud and various devices, running all sorts of services in the back end the user would never know about. Sometimes it’s optimized for the user, but it doesn’t necessarily have their best interests in mind. And that’s ok if they know what those motives are, but I think most people unknowingly are being served experiences purely designed to get a hold of their attention and serve them ads. That relationship is opaque, and in my opinion unethical.
AI is making software even more of a black box. For it to be explainable, it should provide the inputs it takes into account, the purpose of the software, what feedback it is gathering, and where that feedback is being used. This is where we can get back to achieving “informed consent”, and contrary to popular opinion, this is quite doable if it is done from the beginning of a project.
3. Secure — Is that all stable? Have you tested it with some shock tests for corruptibility?
Just as we test banks to check their resilience against financial shocks, so should we test our algorithms against corruptive agents or data anomalies. Is it robust through false signals or introducing bias? Incorruptible against bots, trolls and other manipulations?
After all of this work clarifying the purpose of the machine and how it achieves that, it’s critical to show that purpose won’t change, otherwise undermining the other principles. In fact, the algorithms can become our canaries in the coal mine by alerting us as to when it is when it is time to take back control of the wheel.
4. Transparent — Have you hit publish or made this information auditable?
If we do this as an industry, we have an opportunity to be accountable. The principles others have put forth are highly subjective, so these things need to be transparent for everyone in order for our society’s collective values to be applied, not a single company’s (or board members’) interpretations.
Every stakeholder that wants AI to be for good should get moving. The users will have their consumer groups, society its policy makers, and industry its ethics boards. The key will be having regulation and consumer groups strong enough that they paralyze those who are not acting transparently.
We need to enforce transparency of what’s in software because it impacts society. If you’re a food company that believes in healthy eating, not just offering healthy options, you’re going to ask for better regulation of the industry as a whole, and at the same time invest in preparing yourself to not only meet the standards of healthy nutrition, but also preparing yourself to be transparent about meeting those standards.
Just by beginning action (beyond talk), we can create a powerful economic incentive for companies to enforce their own standards of transparency so that they can immediately jump the transparency hurdle and not disrupt their businesses.
I realize this proposal sounds like it’s blowing up business models as we know them. I think it is to an extent, but right now we face a few realities that I believe necessitate this.
We need the trust of society to carry forward and innovate
The trust is beginning to wane as externalities become apparent (to all of us)
A lot of regulation is a blank slate and can change quickly, for better or for worse
It will be for the best if we participate as an industry to enforce transparent standards
I am not proposing companies lay bare everything, but with the many splintered, narrow applications of AI, we all need to participate as we create the foundations for this fledgling industry. If you can’t prove you’re playing by the rules, should you be allowed to play at all? In order for AI to be for good, those building it have to be accountable to it, and in order for them to be accountable they have to be transparent.
Originally published at www.jfgagne.ai on September 26, 2017.
___________________________________________________________________
You can also see me speaking in more detail and with Q&A at the AI Summit in San Francisco this week and the Web Summit in Lisbon November 6–9.
See the Literature Summary of Principles for Safe AI
",1928,Artificial Intelligence,Jean-Francois Gagné,https://medium.com/s/story/4-steps-to-good-narrow-ai-def7a30ab618,"As noise is being turned into information, I think we’ve had some fundamental shifts in our social contract that modern society is just beginning to pay attention to.
To start, practitioners could use a brush up on Data Science 101, especially concepts like “Correlation does not imply causation” and “informed consent”.
Leaders inside and outside the tech industry are also calling for laws and principles of safe technology, again with a central focus on AI.
[We have a full literature survey of principles, ethics, and other relevant rules for AI in society here.]
We do have a few players coming together on missions like the Partnership on AI, but we need to do more to set high standards of quality and security and lay the foundations for even being capable of meeting the moral imperatives set by these philosophers and futurists.
For one, legislation will take some time to get up to speed, but tougher rules are coming thanks to the recognition of the great power big tech holds with its data.
They base those calls on reasonable claims of the need for assuring consumers about the use of their data and the need for clarity and confidence in digital technologies/services.
We should fight for the industry to be transparent, accountable, and good for humanity so that people don’t gang up against this tech in a backlash.
The enforceability of the regulation and accountability of the practitioners hinge on transparency.
For us to maximize the benefit of AI, we need to balance the benefits to the user, society, and the industry.
That is why having clear, well-planned rules is important: to keep AI fair and working for good.
In order for our industry to start being accountable, I think we should follow four steps with the systems we are building:
The principles others have put forth are highly subjective, so these things need to be transparent for everyone in order for our society’s collective values to be applied, not a single company’s (or board members’) interpretations.
The users will have their consumer groups, society its policy makers, and industry its ethics boards.
The key will be having regulation and consumer groups strong enough that they paralyze those who are not acting transparently.
We need to enforce transparency of what’s in software because it impacts society.
If you’re a food company that believes in healthy eating, not just offering healthy options, you’re going to ask for better regulation of the industry as a whole, and at the same time invest in preparing yourself to not only meet the standards of healthy nutrition, but also preparing yourself to be transparent about meeting those standards.
It will be for the best if we participate as an industry to enforce transparent standards
I am not proposing companies lay bare everything, but with the many splintered, narrow applications of AI, we all need to participate as we create the foundations for this fledgling industry."
"Deep learning: hey, where’s my car 2?",Now it’s time to dive into the deep end of deep neural networks for recommender engines.,"Deep learning: hey, where’s my car 2?

Now it’s time to dive into the deep end of deep neural networks for recommender engines.
Part 1 of this series regaled us with a small insightful origin story about the startup heycar and its ambitious journey to find the optimal recommender system. For those who weren’t with us for part 1, we went over a few key fundamentals regarding the creation of a base model, A/B testing and the creation of our first collaborative filtering model using LightFM.
In part 2 of this three-part series, we continue this harrowing tale and find ourselves not in the realm of libraries and quick solutions, but instead in the mystical world of Deep Learning.
Why Deep learning?
Recommending “items” in our case vehicle listings to people is an extremely challenging task for a few simple reasons:
Noise: historical behaviour is inherently difficult to gauge in terms of satisfaction or interest.
Scale: Many existing recommender algorithms work well on small problems, but struggle to scale both in serving and handling the sheer quantity of events (millions or events).
Freshness: Being able to recommend new content to existing users and existing content to new users better known as the “cold start” problem (which will be covered in part three of this series).
As discussed in part 1 we had a basic collaborative filtering model which generalised a positive interaction between our users and listings based on the mere fact that users interacted with a listing in some way or another. This is a little like saying the best country at the Olympic games was the one who entered the most athletes, it does improve their chances of getting more gold, but it does not account for the quality of the athletes. We decided that there must be a way to get something more meaningful out of our user’s interactions.
Fortunately for us, one of the key advantages of Deep Neural Networks is that arbitrary categorical and continuous features can be easily added to the model. This was great news for us because we could now take into account each user’s unique preferences towards any number of listed features. Furthermore, another distinct advantage of Deep Neural Networks is its ability to predict an outcome based on a sequence of events, such as a user’s change of preferences over time. With all this sorcery and enough buzzwords to get you through a tech conference, how do you go about creating a Deep Neural Recommender?
The look before the leap
Before we went crazy with Deep Neural Networks we decided to first look into the data we had collected about our user’s interactions with our listings. Like Big brother we see everything… well not quite, but we do collect intuitive data about our user’s interactions with our listings. We decided to use this information to quantise the interactions, for example, a user merely viewing a listing is not necessarily a sign of a positive interaction and should, therefore, be scored lower than an interaction involving contacting a dealer.
Now that we had a measure for our user’s interactions we then needed to determine which features we wanted to use in our model. The process of feature extraction involved a deep look into similarities between features and analysing their impact on what we knew were positive interactions.
Last but not least we had to normalise everything. Neural networks require numeric values as input, so we set out to convert all non-numeric values through enumerations and grouped other data such as price or mileage into manageable buckets.
Next, we decided on a framework to help us engineer the solution. We planned on searching high and low, but a few quick Google searches led us to the solution. Keras a framework built on top of the popular Tensorflow framework powered by Google was our choice as it is easy to implement, well documented and has a vast growing developer community.
A plunge into the Deep end
We continued our journey towards applying Deep Neural Networks for recommendations by taking the logic from what we already knew from collaborative filtering and asking the question: “what about latent features?”
To answer this question we created a base, featureless model which took into account only users, listings and their now quantised interactions to create what we thought would have been our candidate generation model. The idea behind the candidate generation model is that we could quickly replicate the LightFM model to produce groups of listings which we could then process further through a recommendation model, not unlike re-inventing the wheel.
We soon discovered that the candidate generation model was not actually necessary, but it provided a great starting point for us to begin adding in our listing features. One by one we added features, trained and then tested our models until we no longer observed any valuable improvements. After adding our features, we found ourselves with a very shallow neural network, little more than a splash pool compared to the ocean we are looking to create.
The next thing we did was add Embeddings for each of our features. Embedding’s although not exclusive to neural networks are just another component that makes neural networks extremely powerful. Embedding layers act like boxes, in which each distinct feature that passes through it is cleverly grouped with other similar variations of that feature. The groupings created by embedding’s allow us to reduce the dimensionality of the input data, making it more manageable and allow us to view similarities between different variations of a feature.
Lastly, it was time to add some depth to our deep neural network through the use of hidden layers. Not to bore you with technical specifics, hidden layers are simply layers which sit between an input and output layer, which take in a set of weighted inputs and produce an output value depending on the function applied to the layer.

It may seem quite complicated but it really isn’t, the model decides the path for each input value based on the weights and activation functions we give each layer. Finally, we get our deep neural network for recommendations as seen below.
End Result Model
As you can see in our model we have distinct embeddings for each input, this helps us reduce dimensionality and also allows for a couple of extra little tricks, which we will cover in part 3. We merge our listings and then merge the users and listings. At first it was decided to merge the users and listings using the dot merge function, however, after some experimentation, we determined that with a deeper network a concatenation function works better.
Dropout layers are used to prevent overfitting, they merely tell the network to “ignore” a couple of random neurons during training. By ignoring some neurons it helps to prevent neurons from developing a dependency on each other. Much like a lazy person may find a shortcut to perform a task which is not necessarily efficient, the dropout teaches the network that it cannot rely on any other neurons and has to put in a maximum effort.
Finally, we get our hidden dense layers, two dense Rectified Linear Unit (ReLU) layers. The ReLU activation function is the most commonly used activation function in the world of neural networks and for our model seemed to work best.
You may be thinking that this is a great way to make recommendations for returning users, but what about new users and listings? In part 3 of this series, we will explore various methods employed to produce meaningful content for both user and listing cold start problems.
By the way, if you want to work with Machine Learning, Tensorlow or any related topics, take a look at our careers page:

",1293,Machine Learning,Jonathan Greve,https://medium.com/s/story/deep-learning-hey-wheres-my-car-2-ab193ac89223,"Fortunately for us, one of the key advantages of Deep Neural Networks is that arbitrary categorical and continuous features can be easily added to the model.
Furthermore, another distinct advantage of Deep Neural Networks is its ability to predict an outcome based on a sequence of events, such as a user’s change of preferences over time.
Before we went crazy with Deep Neural Networks we decided to first look into the data we had collected about our user’s interactions with our listings.
We continued our journey towards applying Deep Neural Networks for recommendations by taking the logic from what we already knew from collaborative filtering and asking the question: “what about latent features?”
We soon discovered that the candidate generation model was not actually necessary, but it provided a great starting point for us to begin adding in our listing features.
After adding our features, we found ourselves with a very shallow neural network, little more than a splash pool compared to the ocean we are looking to create.
The groupings created by embedding’s allow us to reduce the dimensionality of the input data, making it more manageable and allow us to view similarities between different variations of a feature.
Lastly, it was time to add some depth to our deep neural network through the use of hidden layers.
Finally, we get our deep neural network for recommendations as seen below.
At first it was decided to merge the users and listings using the dot merge function, however, after some experimentation, we determined that with a deeper network a concatenation function works better.
The ReLU activation function is the most commonly used activation function in the world of neural networks and for our model seemed to work best."
The World as We Know is Dying — Comparing How We Deal with AI to a Griefing Journey,There is an interesting discussion about whether or not the widespread adoption of Artificial Intelligence will generate demand for new…,"The World as We Know is Dying — Comparing How We Deal with AI to a Griefing Journey

There is an interesting discussion about whether or not the widespread adoption of Artificial Intelligence will generate demand for new kinds of jobs, but this is not what we should be talking about at all. Since the dawn of times new technologies have demanded new jobs, the carriage demanded the coachmen, computers demanded developers, and this goes on and on… In light of this, why do we continue asking if one thing, in particular, will have the exact same behavior of everything that preceded or followed it?? The bad news is: we’re probably not ready for the answer yet.
At first, I thought about using Gartner’s hype assessment to set the stage for this discussion, but it felt as boring as obvious. So I’ve decided to be a bit unorthodox and use the seven stages of grief as the underlying guide. But why?? Technology evolves in a velocity we cannot keep track, either as society or individual, this often creates a continuous feeling that “the world as we know is dying”. This is not to be mournful, it’s just a simple way to help people grasp a better understanding of what is happening, and foster the discussion on what’s coming. On top of that, it’s worth highlighting this is likely to apply to any technology, but artificial intelligence is something with the potential of “replacing” people, thus anything about it is heightened by the everlasting discussion of how do we embed humanity into a computational system, and about envisioning a sustainable future. In this context, humanity is not about having feelings, but it might be about understanding it or how it affects the outcome of a given scenario… it’s also about ethics, and about what a given society understands from right to wrong.
So let’s do it!!
First and foremost, as grief, people might be in different stages, which is important to foster this kind of discussion, that requires largely different points of view, but it might also create barriers for a healthy debate. To do not extend this a lot, we’ll focus on how each stage express itself when it comes to embracing technology. In the end, we will reflect on how to push forward this discussion, what the actual discussion should be about, and how to move forward.
Shock
Say what??
From the seven stages, this resonates to me as the trickiest one… So I got this definition from Google: “a sudden upsetting or surprising event or experience” so we can start by creating a shared understanding of it. “Event or experience” is not much for debate, I personally see the difference as event being that singular mark that by itself embarks you into a grief journey, whist experience is a sum of events or ongoing situation that culminates towards it (it’s pretty much like being suddenly fired versus ending a relationship that just wasn’t working anymore).
For the sake of this discussion, we’ll focus on the “experience” because here is where it gets trick, “upsetting or surprising” comprises that you have been trough something, processed it, and done an assessment on how it impacts you. In this scenario, the “something” is the awareness of what artificial intelligence is and “its potential”.
Awareness because we’re not naturally wired to deeply understand something before drawing conclusion, we have a survival instinct that propels us to immediately respond to anything that might be a threat. And “its potential” because, as we have limited information from the first time we draw a conclusion, it is often misleading or biased by the perception of those who provided us with that information.
Denial
That’s not possible…
The first reaction to “machines will be able to mimic human actions” is either “but they’ll never be able to *whatever*” (which quite often is something the person proud herself of doing), or “but I will no longer be alive when that happens”. In light of this, denial is not restricted to how feasible it is, but often is associated to how soon that future is. The funny thing is that people seem to forget they already solve their credit card issues by naturally talking to a chat bot; or that they find information by asking an entity to search for a large information base and bring back a short selection of relevant information. Both activities not long ago performed by call center assistants and librarians.
The curious thing is that, due to our confirmation bias, this is the moment where we start to search for cues that will reinforce our perception. Whether it’s “computers will never be as creative as people” or “there will be a need to basic income as computers will do all the work”.
Anger
It was all for nothing…
Once we stop denying, confronting the possibilities might make arise new feelings we haven’t foreseen, and might even not recognize as our natural selves. Now, imagine this, you are probably at the stage you caught yourself thinking that a machine will (or already is) better at something you’ve been dedicating your life to, and there’s no way you can compete with it. Your natural reaction is to get angry… It probably feels as if you’ve put a lot of effort into nothing.
So here’s the thing: you shouldn’t be going around cursing every Alexa device you see… but it might eventually happen. In case it does, compare your reaction to your normal behavior, if it was an overreaction, that’s a good moment to start reflecting on how does it affect you.
Bargaining
What if …
Bargaining is about trying to get control back, about negotiating with yourself and the universe in order to go back to a scenario you feel comfortable with. Here is where people start to envision how they can adapt, or how technology should be in order to get their “approval”. Although this reflection is a great step towards action, during this stage people still driven by emotions and with limited understanding about the technology, which causes misled assumptions and actions.
The problem is: when it comes to artificial intelligence, and technology as a whole, most people stop at this stage. We have a natural optimism that allows us to believe “things will work out for the best” so we can “wait and see what the future holds”. One of the trade-offs we do with the universe is that we agree to dedicate attention when it becomes part of our lives even if there’s a possibility it’ll be too late.
It is quite common for people to play with the odds until it’s too late. But here’s the problem: everyday will eventually arrive… and in that day you will be suddenly dragged into the next stage…
Depression
What if I can not make it?
By the time you stumble upon it, it’ll be too late, because this is how technology works: if you realize it exists but don’t know how it works, you’re probably doomed. Technology is something that underpins our lives, it is built to feel invisible and highly intuitive, but it is inherently complex, it’s not something you’ll understand overnight, and trying to do so by the time you start working with, or is being replaced by, artificial intelligence might drive you into a depressive stage.
Depression on what relates to technology is about being overwhelmed or feeling hopeless. Even if you try to break free from this feeling, it’s the kind of journey where there are too many roads and no maps to guide you.
Friendly advice: you are going through the same thing your parents or grandparents went through the first time they attempted to use a computer or a smartphone… the most important thing is to embrace this as an opportunity for something new, and not as a case of personal failure…allow yourself to embrace what’s new, learn from other people experience and try not to lose your mind in the process.
Testing
I can experiment, or at least understand it better…
This is where the self-reflection actually begins… Evaluating which are your strengths, weaknesses and identifying opportunities is the first step to enable you to start experimenting with technology. For some people this is about dedicating time to attend a conference, lecture or meetup; for other people it’s about going deep on the theoretical aspects; there’s also those that prefer to get their hands dirty and go directly into start trying to code, even if it’s something as easy as a conversational interface.
Regardless of how, the important thing is the fact that this is the moment when people start to truly dedicate themselves to understand the technology, and start to envision how they can build a feasible future of human-computer collaboration.
Friendly advise: be honest with yourself. One common cognitive bias we have is relates to our desire to see ourselves as unique (and often as unbeatable). It’s important to understand which qualities of yours stand out, and focus on it. But this is something to be used as a lever, not mistaken by an armour.
Acceptance
If that’s how the future looks like… embrace it!!
There’s not much to say here… the important thing on what relates to acceptance is your approach towards it. Having a negative approach will create an unconscious blocker for new advancements and different points of views. I’m defining a neutral approach as “I’m open to learn and do just as much as I need for my current work/life”. This is not harmful, but won’t get you really far, and this is a growing field where we really need people that are willing to push that extra mile.
In this context, having a positive approach is about be willing to change the game. Engage on the discussion, define new ways of human-machine collaboration, craft the future that better suites our society, or the society we want to live in. It’s not about agreeing or disagreeing, is about having a well argument point of view that is helpful to the discussion, not a gut based feeling with no reasonable arguments backing it.
Moving the needle
Focusing on what really matters
“ Which one is the best economic model”, “should recreational cannabis be legal”, “can we have dessert before dinner” or “is winter better than summer”.
There’s a few dilemmas we face as society to which there’s no actual right or wrong, it’s that continuous pursue of alignment between our personal point of view and how it actually impacts our lives — whether it’s by legislation, common sense or simply agreeing to disagree.
By the time you’re ready to move on and embrace technology advancements, or in this case Artificial Intelligence, you can focus on the discussion that really matters. It’s not about “will there be jobs”, it’s about “what the concept of job will be” or “how large will the marketplace be”. There’s a few fundamental topics you should keep in mind, and even have a personal point of view, just like you do for other society’s dilemmas.
Ethics
How do we embed “humanity” into a computer is a great dilemma when we discuss the widespread adoption of Artificial Intelligence. Although we still have a long way to go from a technical perspective, we often overlook this is actually about “how do we define what is right on subjects we can not reach consensus on what the right answer is”, than it is “how do we develop an algorithm that properly weights the available variables in order to make a decision”. We probably shouldn’t embed the kind of bias that would make an AI treat one person differently from the other… At the other hand, culture and the specific group behavior is one of the biggest demonstrations of our humanity, how do we deal with this?? And this is quite a fundamental question that underpins any AI application, we could push further and use the controversial example of a self-driving car facing an inevitable accident, how to decide whose life should be prioritized??
Work & society 
How much automation is enough, if there’s any limit at all! We probably will never reach a consensus on where is the line which we should not transition from a strictly human, to an artificial intelligence performed activity. It’s important to highlight this is not restricted to work, this goes beyond to any kind of activity, including personal relationships. 
On what is related to work, the marketplace will probably gradually adapt, as it has always done, but the discussion on basic income still needed. This still kind of a background topic, to which we haven’t agree on what the right answer is, so I’ll share my personal take on it. A quite common argument on basic income discussion is “in the pace technology is evolving, we’ll soon reach levels of automation that will drastically increase unemployment levels — without basic income people won’t be able to consume, which might crash our economic model while dragging people to poverty”. Although presenting a hard to argument logic, this still as truthful as shortsighted, as only provides a short-term mitigation that does not solve the problem. The discussion of how work will be, if our economic model would still work on a scenario where anything could be produced without interference of humans, will not be an agreed plan which we’ll all gladly execute, but we should know where we’re heading, as it involves a large scale transition of all parts of our society.
Pushing the envelop
Takeaways on how to be prepared to move forward…
Be realistic
Test your conclusion, if you believe “my work will never be replaced by a robot”, conduct a quick research on what is already being done. Medicine already has ground breaking examples that will help you reflect on where human-computer collaboration is heading.
Define your personal point of view, but keep yourself open to adapt it
Be aware of your personal believes and how it works on a technology-driven world. Having a point of view is helpful when being exposed new scenarios,
Be aware of which role you want to play, and be ready for it
I don’t like when I read things that say “lead the change” or “be a champion of…”, I truly believe in personal accountability, but let’s be honest: not everyone is a leader, an earlier-adopter or a front-line warrior… and that’s okay! We are naturally wired to adapt, this is what made us exist for so long, but we should strive to use this as something positive and not as an evolutionary-based excuse to delegate action for the next generations. In light of this, the important thing is being prepared, open and keep in mind the role you’d like to play and what do you need to accomplish it.
Wondering where to go from here?
Crafting Data Experiences
User-centered frameworks have become key on many different discussions including, but not restricted to, software…medium.com
Technology regulations and the future we forget to think about
At any time you will find people testing, experimenting and pushing the known boundaries of technology to its limits…medium.com
",2537,Artificial Intelligence,Dai Lins,https://medium.com/s/story/the-world-as-we-know-is-dying-comparing-how-we-deal-with-ai-to-a-griefing-journey-361db0550ccd,"There is an interesting discussion about whether or not the widespread adoption of Artificial Intelligence will generate demand for new kinds of jobs, but this is not what we should be talking about at all.
Since the dawn of times new technologies have demanded new jobs, the carriage demanded the coachmen, computers demanded developers, and this goes on and on… In light of this, why do we continue asking if one thing, in particular, will have the exact same behavior of everything that preceded or followed it??
This is not to be mournful, it’s just a simple way to help people grasp a better understanding of what is happening, and foster the discussion on what’s coming.
On top of that, it’s worth highlighting this is likely to apply to any technology, but artificial intelligence is something with the potential of “replacing” people, thus anything about it is heightened by the everlasting discussion of how do we embed humanity into a computational system, and about envisioning a sustainable future.
In this context, humanity is not about having feelings, but it might be about understanding it or how it affects the outcome of a given scenario… it’s also about ethics, and about what a given society understands from right to wrong.
First and foremost, as grief, people might be in different stages, which is important to foster this kind of discussion, that requires largely different points of view, but it might also create barriers for a healthy debate.
“Event or experience” is not much for debate, I personally see the difference as event being that singular mark that by itself embarks you into a grief journey, whist experience is a sum of events or ongoing situation that culminates towards it (it’s pretty much like being suddenly fired versus ending a relationship that just wasn’t working anymore).
Although this reflection is a great step towards action, during this stage people still driven by emotions and with limited understanding about the technology, which causes misled assumptions and actions.
The problem is: when it comes to artificial intelligence, and technology as a whole, most people stop at this stage.
Technology is something that underpins our lives, it is built to feel invisible and highly intuitive, but it is inherently complex, it’s not something you’ll understand overnight, and trying to do so by the time you start working with, or is being replaced by, artificial intelligence might drive you into a depressive stage.
Friendly advice: you are going through the same thing your parents or grandparents went through the first time they attempted to use a computer or a smartphone… the most important thing is to embrace this as an opportunity for something new, and not as a case of personal failure…allow yourself to embrace what’s new, learn from other people experience and try not to lose your mind in the process.
Regardless of how, the important thing is the fact that this is the moment when people start to truly dedicate themselves to understand the technology, and start to envision how they can build a feasible future of human-computer collaboration.
Engage on the discussion, define new ways of human-machine collaboration, craft the future that better suites our society, or the society we want to live in.
It’s not about agreeing or disagreeing, is about having a well argument point of view that is helpful to the discussion, not a gut based feeling with no reasonable arguments backing it.
There’s a few dilemmas we face as society to which there’s no actual right or wrong, it’s that continuous pursue of alignment between our personal point of view and how it actually impacts our lives — whether it’s by legislation, common sense or simply agreeing to disagree.
By the time you’re ready to move on and embrace technology advancements, or in this case Artificial Intelligence, you can focus on the discussion that really matters.
There’s a few fundamental topics you should keep in mind, and even have a personal point of view, just like you do for other society’s dilemmas.
It’s important to highlight this is not restricted to work, this goes beyond to any kind of activity, including personal relationships.
On what is related to work, the marketplace will probably gradually adapt, as it has always done, but the discussion on basic income still needed.
Be aware of your personal believes and how it works on a technology-driven world.
At any time you will find people testing, experimenting and pushing the known boundaries of technology to its limits…medium.com"
"Applying AI in Business, Demystified","In short, I am writing this post to demystify the business side of the topic. The field of AI and data science is currently going through…","Applying AI in Business, Demystified

In short, I am writing this post to demystify the business side of the topic. The field of AI and data science is currently going through an extreme hype cycle. The technology carries enormous promise, but all the noisy hype makes it daunting or intimidating to filter out where the value actually is; and, how to make use of the existing AI components that are already easily available and accessible.
To illustrate the hype situation — within one week in August I visited 2 conferences in Toronto, one was IT-centric and the other was FinTech centric, both of them were talking, almost exclusively on two topics: blockchain and AI. There was almost nothing else discussed. The only difference was that the presentations and speeches at the IT conference were 2/3 on AI and 1/3 about blockchain, while the FinTech conference presentations were reverse.
While everyone is talking about AI as the next leap forward in business and people’s lives the natural emotional questions for most business leaders at all levels are the following:
How does my organization make sure we are not missing out on something important that continues to pick up speed?
How can I embrace these new technologies so they bring high-value impact to my company without risking too much money or other valuable resources and energy in the process?
In this post, I will be making an attempt to outline a real strategy for success in AI and data science by separating out the emotions of hype. I will attempt to lay this out in a simple to understand research and development process for a successful AI or data science project. It will be based on the hard lessons learned from many projects.
Clarifying the Big Terms: AI, Data Science, Machine Learning, Big Data, … or What?
Before getting to the business case driven research and development process, I will try to clarify “The Big Terms” a bit so we are all on the same page. There is lots of buzz around each of these techy them, but don’t let yourself be intimidated or carried away by those. Especially because those often mean different things to different people, because of heavy hype and overuse.
Artificial Intelligence (AI)
You may have noticed me use the terms AI (artificial intelligence) and data science interchangeably above. I did it deliberately. AI as a term seems to be present in lots of hype while talking about data science applications, it is an attention grabber, it has a certain coolness to it, evoking us feel like we live in the brink of a sci-fi world. But technically speaking, “AI” is a rather fuzzy term, with no clear boundaries, a kind of catch-all for every discipline that makes machines to look and behave (somewhat) intelligently.
Data Science (DS)
This term is the youngest in the family. It refers to anything that involves data analysis and algorithms, especially the science behind them: big data analytics/data mining, machine learning, statistics, mathematics, information and computer science.
Is it AI or DS then?
From now on, in this post, I am going to stick more with the term “data science” for the “all-combining” purpose.
Some wise person (I don’t recall exactly who that was) said once that as long as something is called AI, we’re dealing with “science fiction”. But as soon, as it’s implemented in real life, it becomes just a “computer science”, not AI anymore. This might explain why AI always seems to be about future rather than past or even present, though much of what we already use in our everyday lives would have certainly been considered sci-fi stuff just ten years ago.
Probably for the same reasons I personally use the AI term when I am in a playful mood with friends, talking about AI singularity, or when in the “salesman mode” ;) attempting to catch the attention. When I am hacking at home, experimenting, learning, fooling around with Kaggle competitions, or discussing projects and strategies with my team, I talk about and look up for data science.
This applies to naming the teams as well — most likely you would have a data science team work on any of your AI or data science business opportunities. Such a team would consist of data scientists (data science PhD-s who can handle the scientific side of the data exploration, research and validation of the business opportunities), and data engineers, who know how to handle big data frameworks, how to implement the research outcomes into operational environments, etc.
Machine Learning
Machine learning almost sounds like artificial intelligence, but in data science community it’s a bit more concrete and technical term, referring to the specific components or processes in AI that are focused on the learning part of a machine’s intelligence. There are many machine learning algorithms, like (deep) neural nets, decision trees, Bayesian, …, etc, and application areas or data that these can be applied to. The data could be anything, ranging from transaction data to images, videos, audio and vibration pattern analysis, even music, natural language processing (NLP), sensory diagnostic data for predictive maintenance use cases, etc. In essence, these algorithms are all based on some sort of statistical procedures.
Big Data
Though this term has also been much-hyped recently, it essentially refers to any data that exists in amounts too large to be handled or analyzed by “traditional” data handling tools in a single computer, which in turn calls for specific methods for solving problems related to handling heavy loads of data. The heavy load issues could come, for example, from the size of the data storage needed (calling for distributed storage and retrieval systems), or from the need to process information near real-time (call for machine learning methods), etc.
Other terms
There are obviously quite a few more closely-related terms you may encounter when working on this topic, including data mining, big data analytics, business intelligence (BI), etc, but for the sake of brevity, I’ll limit myself to those few boldest ones that decorate the hype scenery today.
Setting Up an AI Strategy — the Prerequisite Understanding
Establishing a data science strategy starts with understanding its basic promise, applicability, and limitations.
The Basic Promise
In terms of business, data science is useful for two main reasons. It helps you seek out new revenue streams and helps you avoid losing money due to inefficiencies, fraud, or in human error, and it does so by looking at your data and applying data analytics and machine learning tricks on it.
An example. An online merchant selling digital goods was losing 10% of their revenues due to credit card fraud chargeback requests and penalties, and was being threatened by their payment provider to be shut down if the situation didn’t improve. An AI application involving Bayesian machine learning algorithm was implemented to the merchant’s business and was successfully able to detect 90% of the fraudulent transactions in real-time, with false-positive rate kept below 0.1%. In addition, even though fraudsters were good enough at adapting their behavior so that humanly analyzed and hardcoded fraud-pattern checks became irrelevant just days after the program’s deployment, the machine-learning algorithm in question was able to learn new patterns in near real-time, maintaining its efficiency despite evolving forms of fraud.
The Applicability
The good thing about data science is that its main forms of implementation strategy are rather agnostic to the field that you apply them onto. Wherever you have data piling up or streaming around, there is a good chance you have untapped opportunities hidden there for huge positive impacts.
A top-caliber data science team is usually able to handle any type of data in the same fashion, whether dealing with transactions, images, videos, audio, vibrations, natural language text processing, etc. The applications with a significant business value based on these data could include credit scoring, fraud detection, image recognition, predictive maintenance, natural language processing (NLP) chatbots, intrusion detection (in cybersecurity), conversion and churn predictions, to name a few.
An example. A big industrial company with a network of hundreds of suppliers had its top-level management burdened by non-stop communication with suppliers, to address questions that couldn’t be left unanswered or unsolved. A successful NLP-based chatbot implementation was able to solve and remove 80% of that burden from the company’s top-level management team.
The Limitations
So far it all looks like an ordinary software project, right? Just some bit of a coolness thrown in with the touch of AI and that’s it? Wrong!
Here comes the thing that sets data science projects significantly apart from average software projects, that makes it extremely messy and almost certainly your time-and-money-wasting project if you don’t beware its limitations, and, on the other hand, makes it nicely controllable successful contribution to your business strategy, if you take the difference fully into account.
The significant difference between a data science project and an ordinary software project is rooted in its main limitations:
1. The nature of probability:
In the context of business use cases, machine learning algorithms work through probability, not determination. You will always have the question of accuracy when taking its answers into account. Remember the fraud detection example above — there will always be some amount of “false negative” and “false positive” outcomes, but detecting 90% of fraud (which translated to 9% revenue loss) still put the company in a significantly better situation, removing the risk of having its payment provider discontinue service, avoiding significant losses, and, for the greater good, making the day harder for fraudsters — even at the price of losing some less than 0.1% of the merchant’s legitimate transactions.
If your business case has zero tolerance for “false” answers, you simply can’t apply these methods. However, if your business case can work with accuracy that’s “good enough,” then it simply becomes a question of achieving it.
For example, in a very extreme case of self-driving cars, where you have cascades of AI components in play, one might ask how it is okay to have errors there!? The answer is that there may be “errors” (in machine learning terms) in the system’s sensory real-time data analytics, but these can be narrowed down to individual components in combination with the application of certain robustness principles — which call for never relying on a single data source or sensor — in such a way that these errors won’t cause danger to anyone’s property or health.
2. The question of do-ability:
Data science’s probabilistic nature leads to another important question: even if your business case is capable of accepting some “false” answers in course of actions, is the “good enough” level of accuracy achievable at all? You could develop the entire framing software that integrates your machine learning algorithm seamlessly into your operational environments, it scales etc, but if the ML algorithm is really unable to make decisions with the accuracy that would make sense for your business case, then all the product development around it would be a waste, if not even counterproductive.
And that’s a constant reality of data science projects — the necessary accuracy is not always achievable (at least not on the first try).
A successful data science strategy makes sure these two key aspects are always at the center of any project planning.
The Process
Having laid out the above notes, what follows is actually quite simple and straightforward.

Step 1: Ask “The Question”
At the heart of any data science project is the established question you want your system to answer. When you think about your first (or the next) AI application, make sure you know exactly what question you’ll be answering, and be certain it has a clear link to your business impact.
Examples of the questions:
Question: Can we predict fraud in our insurance applications? Can we adapt to fraud pattern changes in real-time? Impact: Avoid losing money to fraud.
Question: Can we detect dangerous goods (radioactive material, weapon components, etc.) being smuggled based on analyzing the related documentation, logistics information and x-ray scans of cargo in seaports and airports? Impact: A safer community to live in.
Question: Can we predict the mechanical malfunctions before a system actually breaks? (This is called a predictive maintenance question, and could be answered, for example, by listening to audio sensors attached to the body of a machine and analyzing the changes in vibration patterns and harmonics.) Impact: Avoid mechanical malfunctions and revenue loss, even damage costs, that may incur from it otherwise.
Step 2: Determine what “good-enough” accuracy will mean for you.
Once you’ve established your project’s question — but before starting to pour money, time and other resources into heavy development work — it is important to determine how good you’ll have to be at answering this question in order for your business case to make sense. In other words, you’d need to quantify for your system some kind of a Key Performance Indicator (KPI) target that makes sense in the business case.
A simple example: A monthly payment based business with hundreds of thousands of subscribers was continuously losing revenue due to some customers failing to pay their next monthly bill. The question was: could we predict those missing payments in advance? The impact of answering the question was to be the company’s ability to apply the preemptive measures. The KPI target was that it needed to be able to predict at least 75% of the customers who would miss their next payment without a significant false-positive rate, in which case it would make sense to implement the product within the company’s given budgetary limitations.
Step 3: Data Exploration, Research, and Impact Validation
Up until this step in the process you don’t spend almost any resources, other than some basic work to identify the question and establish the KPI target that makes sense.
Now the critical question is: can it really be done? Can your question be answered with a level of quality that will exceed your minimum KPI threshold? This is the business impact validation step. Its aim is to identify all the relevant data sources, explore, manipulate, restructure and tidy up the data, work out the machine learning models, etc. that would be able to create the impact. The outcome of this step includes training, test and validation data sets, which allow you to demonstrably confirm the doability of your product before the actual software product development for it even begins.
By using the term “demonstrably,” I’m referring to the repeatability of the process according to scientific standards and qualities — remember, we are dealing with data scientists (often PhD-grade) at work, with an emphasis on the word scientist. One of the key qualities of the scientific method is repeatability. This means that, technically, the outcome of your research and data exploration includes all of the exact steps, scripts, and data dictionaries showing exactly how the data was obtained, transformed, and divided into training, test and validation datasets, the machine learning model(s) and demonstration instructions.
As you can imagine, this is the first step where it becomes necessary to invest some initial resources, given that research and data exploration are efforts that need to be performed by people who, in turn, have invested heavily into building up their competencies. Still, this is usually kept quite lean and mean compared to the product development project, that follows. The idea here is to avoid investing in product development as long as the project’s do-ability is still up in the air. Investing in product development only makes sense once the research has validated the impact. You would need to be willing to risk investing into those validation cycles, applying reasonable money management methods and per-project stop-loss decision-making strategies to your funds. But, unless you enjoy messy corporate roller coasters, you shouldn’t risk investing in product development before validating your business impact in data speculations.
When research fails to validate the speculated impact during initial sprint cycles, there could be a couple of reasons why:
The data you’re working with could be just too shallow or lacking in easily discoverable and meaningfully applicable signals. In this case, it’s good that you haven’t started spending resources on the product development and you can begin looking for other ideas to make an impact with.
It could also be that the relevant signal is evident in your data but it is resisting crossing the expected KPI target threshold that would allow you to validate the business case. In this case, you can speak to your data science team and discuss the possibility of building up more data features that you don’t have yet. This could mean taking a couple of months to have your existing products, those generating the relevant data, store more information about the data being observed, after which you could try your research again and see if the KPI target in question is achievable.
Step 4: Product Development
Once your research in Step 3 has been successful, develop the appropriate data product around the results in such a way that it seamlessly integrates into your operational environment, scales, and makes you capable of creating the real impact.
This stage looks more like a regular software product development. Here you would apply the same principles, starting from the ideation and design sprints (if UI interactions are involved), to validate that the target users are going to grasp the new product and embrace the idea. You would then develop your first MVP (the Minimum Viable Product) to further validate you’re on the right track, now with hard evidence from the field, and iteratively continue developing your product and adding impact to your business case.
As long as the product stays relevant, you’ll typically always have something to improve. In addition to regular software development part of the product, you’ll keep monitoring on the performance of the data science part of the product, occasionally revisiting research cycles to either troubleshoot changes in the data sources or strengthen/optimize the impact of the outcome.
The Big Picture
Hopefully, the process layout above can shed some light on ways on how you can start with bringing a data science on board your company. In real life, as successful products go, there will be more continuous nested development cycles going on one after another, and principles you’ll apply to carry them out will evolve as you learn and familiarize yourself with the subject. Still, the essence for a thriving data science project remains the same: the successful data science product is a product of a research-first project, and the successful data science product pipeline strategy is based on such projects.
The main driver of the success is essentially maintaining these two principles throughout the process, never forgetting these at any steps:
Be sure that you always know what the impact you are looking for will be. Be certain that what you do is actually significant, having a positive impact. This is a fundamental rule for anything to survive in the business context — not just data science.
Validate the critical assumptions as early and as often as possible. When working with data science projects, validate that the needed accuracy in answering the established question is achievable at the levels for the impact to make economic sense.
Where to go from here?
If you’re a successful entrepreneur, you probably already know that most of the big things have started from a myriad of experimentation. It is not different in the data science field either. Here are a few hints for recognizing data science opportunities.
It is generally a good idea to maintain a habit of brainstorming for new, exciting ideas — no matter what field you may be working. So spend some time regularly thinking about your business as well as the work you put into it, and remember that good product ideas can emerge from the most painful problems.
The kind of issues that best lend themselves to be resolved with data science methods are, as the name suggests, those where data can be analyzed with scientific rigor. As mentioned above, the data in question could be anything: transactions, images, audio signals, natural language texts, video clips, temperature fluctuations, other environmental sensory data, and so on.
When coming across a potentially interesting (impactful) idea, start thinking about quantifying its impact (remember KPI target), what kinds of data do you have that can be analyzed, and the ways in which you could validate your speculations.
Mooncascade
If you have any promising ideas for integrating a data science project into an impactful business, I’d be more than happy to help you think things through — just get in touch via LinkedIn, for example. Mooncascade, the company I work for, is specialized in data science consultation and implementation. That’s how we make our impact!
Initial discussion and the first look by us at your data to spot the potential for high-impact opportunities are both free. After this stage, you could continue work on your own, with other partner(s), and/or with us. The main benefits that come from working with us are world-class data science team, a quality of work, and a sharp focus on positive impact and validating this impact from beginning to end.
Mooncascade data science team just recently got a top-caliber result in a credit scoring contest on kaggle.com, the worlds most popular data science competitions site — we got positioned at top 8% among more than 7000 contestants, and our result was just 1..2% less precise than the winners.
About the author
Asko Seeba is the Co-Founder and CEO of Mooncascade, a fast-growing data science and software product development company. He’s been working in the tech industry for the last 20+ years. Currently, Asko’s focus is on data science (big data, machine learning, AI, data analytics) and business impact driven product development.
",3673,Machine Learning,Mooncascade,https://medium.com/s/story/applying-ai-in-business-demystified-9b46b30f46d7,"In this post, I will be making an attempt to outline a real strategy for success in AI and data science by separating out the emotions of hype.
I will attempt to lay this out in a simple to understand research and development process for a successful AI or data science project.
AI as a term seems to be present in lots of hype while talking about data science applications, it is an attention grabber, it has a certain coolness to it, evoking us feel like we live in the brink of a sci-fi world.
When I am hacking at home, experimenting, learning, fooling around with Kaggle competitions, or discussing projects and strategies with my team, I talk about and look up for data science.
Such a team would consist of data scientists (data science PhD-s who can handle the scientific side of the data exploration, research and validation of the business opportunities), and data engineers, who know how to handle big data frameworks, how to implement the research outcomes into operational environments, etc.
Machine learning almost sounds like artificial intelligence, but in data science community it’s a bit more concrete and technical term, referring to the specific components or processes in AI that are focused on the learning part of a machine’s intelligence.
The heavy load issues could come, for example, from the size of the data storage needed (calling for distributed storage and retrieval systems), or from the need to process information near real-time (call for machine learning methods), etc.
There are obviously quite a few more closely-related terms you may encounter when working on this topic, including data mining, big data analytics, business intelligence (BI), etc, but for the sake of brevity, I’ll limit myself to those few boldest ones that decorate the hype scenery today.
Establishing a data science strategy starts with understanding its basic promise, applicability, and limitations.
In terms of business, data science is useful for two main reasons.
It helps you seek out new revenue streams and helps you avoid losing money due to inefficiencies, fraud, or in human error, and it does so by looking at your data and applying data analytics and machine learning tricks on it.
An AI application involving Bayesian machine learning algorithm was implemented to the merchant’s business and was successfully able to detect 90% of the fraudulent transactions in real-time, with false-positive rate kept below 0.1%.
The good thing about data science is that its main forms of implementation strategy are rather agnostic to the field that you apply them onto.
The applications with a significant business value based on these data could include credit scoring, fraud detection, image recognition, predictive maintenance, natural language processing (NLP) chatbots, intrusion detection (in cybersecurity), conversion and churn predictions, to name a few.
Here comes the thing that sets data science projects significantly apart from average software projects, that makes it extremely messy and almost certainly your time-and-money-wasting project if you don’t beware its limitations, and, on the other hand, makes it nicely controllable successful contribution to your business strategy, if you take the difference fully into account.
In the context of business use cases, machine learning algorithms work through probability, not determination.
The answer is that there may be “errors” (in machine learning terms) in the system’s sensory real-time data analytics, but these can be narrowed down to individual components in combination with the application of certain robustness principles — which call for never relying on a single data source or sensor — in such a way that these errors won’t cause danger to anyone’s property or health.
Data science’s probabilistic nature leads to another important question: even if your business case is capable of accepting some “false” answers in course of actions, is the “good enough” level of accuracy achievable at all?
You could develop the entire framing software that integrates your machine learning algorithm seamlessly into your operational environments, it scales etc, but if the ML algorithm is really unable to make decisions with the accuracy that would make sense for your business case, then all the product development around it would be a waste, if not even counterproductive.
When you think about your first (or the next) AI application, make sure you know exactly what question you’ll be answering, and be certain it has a clear link to your business impact.
(This is called a predictive maintenance question, and could be answered, for example, by listening to audio sensors attached to the body of a machine and analyzing the changes in vibration patterns and harmonics.) Impact: Avoid mechanical malfunctions and revenue loss, even damage costs, that may incur from it otherwise.
Once you’ve established your project’s question — but before starting to pour money, time and other resources into heavy development work — it is important to determine how good you’ll have to be at answering this question in order for your business case to make sense.
The KPI target was that it needed to be able to predict at least 75% of the customers who would miss their next payment without a significant false-positive rate, in which case it would make sense to implement the product within the company’s given budgetary limitations.
Up until this step in the process you don’t spend almost any resources, other than some basic work to identify the question and establish the KPI target that makes sense.
Investing in product development only makes sense once the research has validated the impact.
But, unless you enjoy messy corporate roller coasters, you shouldn’t risk investing in product development before validating your business impact in data speculations.
In this case, it’s good that you haven’t started spending resources on the product development and you can begin looking for other ideas to make an impact with.
Once your research in Step 3 has been successful, develop the appropriate data product around the results in such a way that it seamlessly integrates into your operational environment, scales, and makes you capable of creating the real impact.
When working with data science projects, validate that the needed accuracy in answering the established question is achievable at the levels for the impact to make economic sense.
If you have any promising ideas for integrating a data science project into an impactful business, I’d be more than happy to help you think things through — just get in touch via LinkedIn, for example.
Currently, Asko’s focus is on data science (big data, machine learning, AI, data analytics) and business impact driven product development."
How Artificial Intelligence Will Change Sales in Fintech,"When we think of Fintech, we think of innovation in financial services. But for Fintech companies to be successful, they must not only have…","How Artificial Intelligence Will Change Sales in Fintech
When we think of Fintech, we think of innovation in financial services. But for Fintech companies to be successful, they must not only have amazing products but a market-winning sales and distribution team.
In the wake of the financial crises, Fintech companies are faced with unprecedented opportunities, but they face many challenges as they attempt to scale their revenue-generating activities. Take fictitious P2P mortgage lender CrowdNdowed. With a sales team of 100 reps selling P2P-backed mortgages across New York State, CrowdNdowed receives about 500 leads a day of varying quality. Customers routinely call their sales line to ask basic questions but more often than not, they get queued in automated phone trees. Life for sales reps at CrowdNdowed is stressful. Each rep manages about 30 deals at once. Sales activity is measured at forensic levels by management.

Life isn’t much better for the 10 sales managers. CloudNdowded has a 10:1 rep to manager ratio. Managers are maxed out hiring new staff whilst trying to meet monthly targets. The 3 VPS of sales are under pressure to meet revenue targets and introduce new savings and credit card products across the sales team. CrowdNdowed is growing full tilt, but their VC investors have huge expectations and the management team is under pressure to grow the business by 300% this year, whilst trying to keep customer acquisition costs at a minimum.
Sound familiar?
The people who feel the burden of fast-growing fintech companies the most are in the sales department. The good news is new AI-driven applications will play a huge role in improving sales productivity and will allow sales teams to focus on more valuable activity, whatever their role is in the business. In the future, its likely humans will be accompanied by an AI-powered virtual assistant, for example, Einstein is Salesforce’s new AI super hero. CRM and marketing automation have been hugely helpful at improving the productivity of sales teams over the years, but the stage is set for AI to take things to the next level.
Circling back to CrowdNdowed, here are 6 ways AI will allow this budding fintech company to meet the demands of their VCs without burning out the sales team.
Predictive lead scoring
With over 500 leads being generated a day, our friends at CrowdNdowed have a hell of a job qualifying and converting leads to real opportunities.

However, with the of power sentiment analysis and inference, AI will be able to prioritise real opportunities. This can be done using basic logic but with the power of machine learning applications can start to learn what a good lead looks like for CrowdNdowed – not just if it meets the qualification criteria. Fintech companies love selling to customers with a particular “inflection points” e.g. recently married, bought first house, new born. When marketing becomes predictive, it starts to figure when that inflection point might happen. If a customer is not yet married, but “likes” a ring on his Instagram account, this might be an indication he is about to get married and probably buy a house soon afterwards.
Deal Intelligence
Our sales reps at CrowdNdowed are managing a big number of opportunities each month. It can be a real challenge to keep track of all those deals at once. Sales reps have 3 key responsibilities: originating deals, progressing deals and closing deals. AI can track whether deals are progressing and make recommendations to move them forward. AI is especially good at spotting risks to deals.

The fuel for AI is data. The more data an AI can pull from multiple sources the better. Picture a customer who is applying for a mortgage but recently posted their down-sizing on Facebook, or a customer who published photos of their new Porsche on Snapchat after they said they live below their means on a personal loan application. AI will be able to crawl the web for important events like these which will help reps predict whether a deal will close. This will help improve CrowdNdowed’ forecast accuracy and allow reps to focus on deals that are actually going to happen.
Living insights
We are data rich and insight poor, but imagine having your own data scientist on demand. For our sales managers at CrowdNdowed, they could really do with some strategic help. As a start-up, they have one strategy person who reports to the CEO.

It’s a real challenge to spot gaps in their deal pipeline, weaknesses in key deals, and stay on top of mortgage renewals. Between hiring new people and helping reps to close deals, they simply don’t have time to analyse all their sales data. In the future, sales managers will have an AI-powered sales strategy assistant that will whisper insights in their ear as they buy their morning coffee. When they are not listening to their new-found virtual data analyst on their way to work, they will be telling it what to do because this guy is capable of understanding speech due to the power of natural language processing. Say goodbye to the drudgery of creating reports or sifting through endless spreadsheets.
Robo-advice
Automated advice is one of the strongest use cases for AI in fintech and banking in general. When we think of a fast-growing company like CrowdNdowed, receiving 500+ leads a day with VC scrutiny on the bottom line , an extra hand from a robo-advisor tied to the website or an app will make a huge difference to employee and customer experiences alike. Modern Robo-advisors are capable of qualifying leads, answering customer questions and in some cases processing quite complicated sales transaction — Rep Emptor!

A cutting edge Insurtech company like lemonade is a great example. Lemonade are changing the game with AI-driven apps that are profoundly different to what incumbents are offering — check ’em out. As well as automating the sales process, applications like Lemonade are delivering a fresh customer experience at a far lower cost than their human counterparts.
Smart activity logging
Logging activity is a necessary evil for all sales professionals, but AI will be able to unburden even the most willing of sales reps. Future AI solutions will log calls, gauge the sentiment of the customer and pen follow-up emails with key points and next steps discussed.

Reps will never forget to log a site visit again! An driven AI app will be able to cross-correlate the meeting address in your diary with your location and log the meeting for you. Walking out of meeting, your AI solution will be able infer where you are and recommend a site visit to a new customer a few doors down.
CRMs have been at the heart of sales productivity over the past 20 years. Until now the interface between the CRM and human has been the humble web-brower.. with a sprinkling of mobile apps in recent years. CRMs won’t go away, but in the future, the interface will be voice, virtual assistants and peripheral applications like Alexa. The future of revenue generating activities in fintech companies will be slick, efficient and unencumbered by slow human IO.
",1202,Sales,Navirum,https://medium.com/s/story/how-artificial-intelligence-will-change-sales-in-fintech-b49d9b35b954,"In the wake of the financial crises, Fintech companies are faced with unprecedented opportunities, but they face many challenges as they attempt to scale their revenue-generating activities.
With a sales team of 100 reps selling P2P-backed mortgages across New York State, CrowdNdowed receives about 500 leads a day of varying quality.
CrowdNdowed is growing full tilt, but their VC investors have huge expectations and the management team is under pressure to grow the business by 300% this year, whilst trying to keep customer acquisition costs at a minimum.
The good news is new AI-driven applications will play a huge role in improving sales productivity and will allow sales teams to focus on more valuable activity, whatever their role is in the business.
CRM and marketing automation have been hugely helpful at improving the productivity of sales teams over the years, but the stage is set for AI to take things to the next level.
Circling back to CrowdNdowed, here are 6 ways AI will allow this budding fintech company to meet the demands of their VCs without burning out the sales team.
This can be done using basic logic but with the power of machine learning applications can start to learn what a good lead looks like for CrowdNdowed – not just if it meets the qualification criteria.
Our sales reps at CrowdNdowed are managing a big number of opportunities each month.
AI will be able to crawl the web for important events like these which will help reps predict whether a deal will close.
Between hiring new people and helping reps to close deals, they simply don’t have time to analyse all their sales data.
When we think of a fast-growing company like CrowdNdowed, receiving 500+ leads a day with VC scrutiny on the bottom line , an extra hand from a robo-advisor tied to the website or an app will make a huge difference to employee and customer experiences alike.
Modern Robo-advisors are capable of qualifying leads, answering customer questions and in some cases processing quite complicated sales transaction — Rep Emptor!
As well as automating the sales process, applications like Lemonade are delivering a fresh customer experience at a far lower cost than their human counterparts."
Introducing the DSX Network,DataStreamX is excited to launch its decentralized network that will revolutionize the AI and Data-as-a-Service industries and create a…,"Introducing the DSX Network

DataStreamX is excited to launch its decentralized network that will revolutionize the AI and Data-as-a-Service industries and create a level playing field for data driven organizations. The network will provide improved access to quality data with offer improved mechanisms for creating new data and intelligence products that will ultimately improve the lives of every single person.
Why the DSX Network?
Data drives today’s world, but we recognize that the industry is broken; there is an imbalance of power. The Big 5 (Google, Facebook, Amazon, Apple, and Microsoft) are hoarding data, which limits innovation and stifles competition among up-and-coming AI and data firms. With the increasing pervasiveness of AI and machine learning, this growing imbalance in power is going to be catastrophic.
As The Economist suggests, the data economy demands a new approach to antitrust laws, as the large companies are much better equipped to predict future demand, industry trends, and potential competitors , which in turn enable them to impede the growth of budding start-ups.
Large companies are hoarding data
The power of data is no longer a question, as it can give organizations the capability to manipulate industries, economies, and even elections. However, the bigger their power grows, the harder it will be for smaller companies and new market entrants to compete.
We already see what happens when too few have control of the data, such as in the advertising industry where companies like Google and Facebook obtain almost all growth in advertising spend, preventing others from competing.
Google recently lost a 11-year battle against its practices, which leverages on data and give them an unfair advantage. Although the law is on the side of fair competition, taking 11 years to implement laws and regulations is too long a wait. With the rapid developments taking place in the big tech companies, regulations will never be able to catch up.
In some cases, misguided privacy laws give companies more reasons not to share their data. Data laws are imperative, but they should be established in a way that protects people while enabling the development of new innovations and improving the quality of our lives.
Creating new silos will not solve the problem
Creating a new silo is not the proper way of solving this problem. DataStreamX has been successful in accessing large volumes of data from organizations. However, although we continue to grow and become stronger, becoming the next major data controller will prevent us from fulfilling our mission.
To enable the data economy, we want a network of partners who can solve a range of problems. Increasing the quantity and quality of data in the network. By doing so, we want to help enhance the quality of life for everyone.
What’s the DSX Network Solution?
To build the data economy, we realize that the best approach is to decentralize data access. This makes data available to anyone who wants to build data products on the network. In the process of building these data products, it is likely that more data will be added to the network thereby increasing both quality and quantity of data available on the network.
Through the blockchain, the DSX network can ensure authenticity and provenance even though data products might be built upon and combined in a complex manner. Through smart contracts we can ensure that pricing and payment terms are fair and transparent for both the buyer and the seller.
How are we Building the DSX Network?
· The core of the DSX Network is to enable anyone to build upon what has already been created. This will enable Master Nodes and Data Refineries to process, ingest, and create new data products in a transparent and compliant manner where fair competition and data provenance are ensured.
· We will use DSX Data Smart Contracts to allow sellers to stipulate the rules around how their data will be used and paid for. Revenue is automatically shared with the original data providers, in the process creating a system of trust and transparency. This will increase the entire network’s value, as more innovations will be developed with every new node or data feed added into the network.
· We will create DSX Mineral Pools to allow buyers to stipulate the rules and define the metrics that will facilitate the ingestion of distributed streams. This amazing innovation will allow companies to incentivize the masses by distributing a mobile SDK, an IoT SDK, an app, or others, which can allow others to contribute data to their pool.
Anyone — from the general public to established businesses — can support the different mineral pools they want, in turn earning incentives for their participation. As an example, a mobile SDK may incentivize users to share location or histories. Another is a smart fridge company that wants users to contribute data by sharing their grocery list. A third one is an IoT air quality company that wants to collect data from its distributed air quality sensors.
Like oil, data needs to be refined. It goes through various layers before its value is realized. We will enable this. With a decentralized network, there will no longer be a single marketplace for those who build the next generation data economy.
What’s Our Timeline?
Our next step is to build the trusted DSX Network, which we will launch in April 2018 and that will enable companies to authenticate their data on the blockchain.
Our test net is up, and we are looking forward to bringing in our partners and start stamping their data for authentication and provenance.
By the fall of 2018, we will be launching our Data Smart Contracts and Data Mineral Pools.
Data for All
At DataStreamX, we are decentralizing our data marketplace, eliminating the creation of new silos, and creating a network that will drive innovation.
We are creating a data economy that will enable every data scientist, AI company, and data-driven organization to have a fair chance at competing.
We will bring back the concept of garage start-ups that has been obliterated by the data era.
Welcome to the new data economy!
Are you as excited as we are to build this new data ecosystem? Join our Telegram group at https://t.me/datastreamx or follow us on Medium at https://medium.com/datastreamx.
",1050,Blockchain,Mike Davie | Quadrant Protocol,https://medium.com/s/story/introducing-the-dsx-network-203c5d3e9b2a,"DataStreamX is excited to launch its decentralized network that will revolutionize the AI and Data-as-a-Service industries and create a level playing field for data driven organizations.
The network will provide improved access to quality data with offer improved mechanisms for creating new data and intelligence products that will ultimately improve the lives of every single person.
As The Economist suggests, the data economy demands a new approach to antitrust laws, as the large companies are much better equipped to predict future demand, industry trends, and potential competitors , which in turn enable them to impede the growth of budding start-ups.
Data laws are imperative, but they should be established in a way that protects people while enabling the development of new innovations and improving the quality of our lives.
To enable the data economy, we want a network of partners who can solve a range of problems.
This makes data available to anyone who wants to build data products on the network.
· The core of the DSX Network is to enable anyone to build upon what has already been created.
This will enable Master Nodes and Data Refineries to process, ingest, and create new data products in a transparent and compliant manner where fair competition and data provenance are ensured.
Our next step is to build the trusted DSX Network, which we will launch in April 2018 and that will enable companies to authenticate their data on the blockchain.
At DataStreamX, we are decentralizing our data marketplace, eliminating the creation of new silos, and creating a network that will drive innovation.
We are creating a data economy that will enable every data scientist, AI company, and data-driven organization to have a fair chance at competing."
Five Steps to Take Before Kicking Off A Clickstream Data Initiative,"With the ability to track every step of a user’s journey across a website or app, we can truly get a 360° view of a customer. Often…","Five Steps to Take Before Kicking Off A Clickstream Data Initiative

With the ability to track every step of a user’s journey across a website or app, we can truly get a 360° view of a customer. Often, organizations gather this information via multiple analytics tools, each representing a different aspect of user behavior. Many of these rely on data generated from when a user does (or doesn’t) “click.” We call that clickstream data.
Clickstream is the tracking of all the events you want, which can then be broadcast to your data warehouse, CRM, marketing/sales analytics tool or any other destination in real-time. But if you’re interested in tracking new events (ie. beginning a clickstream initiative), here are the top 5 things to keep in mind before kicking it off:

Image from laboratory-manager.advanceweb.com
1. Ask the questions that are important to you.
Steven Covey said it best, “Begin with the end in mind.” In that same vein, start your data initiative with a set of unanswered questions to guide your efforts.
There are many great tools (e.g. Mixpanel and Amplitude) that answer a generic yet important set of questions like, “How can I get more subscribers?” or “How can I retain my users?” or “How can I get more shares?” These are key questions to leverage first and can be done by measuring page views, session length, sources of traffic and more.
But let’s say you have a few key customers you want to replicate. Or you just invested a significant amount of time and money to redesign your website and can’t figure out why your sales have decreased. To find answers to more sophisticated business questions, it’s important to dig deeper. When digging, though, keep in mind #2…

Image from newsreplica.com
2. Use data processing to make the data more understandable.
Raw data is like crude oil; it requires refining before it becomes truly useful.
What does “refining” look like in the data world? It’s data processing, and includes filtering, cleaning, enrichment and aggregation. In the clickstream space, one example is cohort analysis. Say you’ve got 100 active users. By grouping users by the day, week or month they signed up, you can discover whether 90 of those users remained active in a subsequent month … or only 10. Suddenly, you have a much clearer picture of those 100 active users. Especially when aggregated, summarized and combined with other data from earlier (ads, referrers, etc.) or later (activation, retention) in your customer lifecycle.
All data processing should directly tie into an effort to answer one of the questions important to you. In other words, don’t pre-optimize your data processing for “future needs.” Have a plan to mine the answers to your current questions. A little data processing can go a long way to making those answers accessible (and if you’re not sure where to start, one of our data analysts offers a few ideas in this post). But before you begin collecting clickstream data, let alone processing it, pause to consider #3…

From giphy.com
3. Explore non-analytics uses of your clickstream data.
Clickstream data isn’t just for analytics:
You can send user clickstream data to tools such as Facebook and Google Ads to help target ads more precisely.
You can send data to customer interaction tools such as Intercom or Drift to send user identity information in order to augment the user experience of their platform.
You can send data to Optimizely or Visual Website Optimizer to power your A/B tests.
If you’re going to initiate a new kind of clickstream data collection, don’t miss a key opportunity to gain additional value from it. Maximize your use of user event data from the get-go by exploring how it can make the non-analytics tools you use even more impactful. None of the prior steps matter, however, if you’re not ready for #4…

Image from wallpaperswide.com
4. Be poised for action.
All the analysis in the world is useless if it’s not driving actions and reactions. Strike a balance between the time you spend poring over data and the time you spend reacting to it. This can be formalized through a recurring process where numbers are reviewed, with two outputs:
What actions or experiments should we conduct to improve the numbers?
What changes do we need to make to our data collection, processing or visualization to have better information for the next review?
Of course, it’s hard to be ready to change without having some semblance of an idea how: new marketing strategy? Web redesign? Change course on the product roadmap? Anything goes. And instrumenting your website and app for clickstream is only worth it if you’re prepared to do what the data tells you. Which will almost certainly include #5…

Courtesy of Trong
5. Get ready to explore new technologies.
Once you get the basics up and running, keep in mind that machine learning and AI have emerged from academia into the real world and raised the bar for everyone in data. In fact, machine learning can be accessed through cloud services or through open-source tools such as TensorFlow and PredictionIO. There may be opportunities for you to gain a competitive advantage or breakthrough performance by innovating beyond simple data collection and linear mathematics.
Getting in on these technologies early could be the key to success for your organization — and amping up your current data analysis is the foundation for that next step into new territory.
Looking for a little help?
This may seem overwhelming, but you don’t have to go it alone. Connect with marketing and/or product managers at fast-growing companies near you and buy them lunch or coffee — you’ll likely discover a rich community of individuals who are all working on putting data to work for them, with similar challenges to you.
Jumpstart your journey by signing up for Astronomer today (the first 50K events per month are free). Our platform allows you to level up your clickstream game beyond raw data collection. We have dedicated support on standby if you need any help getting set up.
Happy clickstreaming!
Originally published at wwww.astronomer.io.
",1025,Analytics,Ry Walker,https://medium.com/s/story/five-steps-to-take-before-kicking-off-a-clickstream-data-initiative-c6f6b03c8911,"Five Steps to Take Before Kicking Off A Clickstream Data Initiative
Clickstream is the tracking of all the events you want, which can then be broadcast to your data warehouse, CRM, marketing/sales analytics tool or any other destination in real-time.
There are many great tools (e.g. Mixpanel and Amplitude) that answer a generic yet important set of questions like, “How can I get more subscribers?” or “How can I retain my users?” or “How can I get more shares?” These are key questions to leverage first and can be done by measuring page views, session length, sources of traffic and more.
All data processing should directly tie into an effort to answer one of the questions important to you.
In other words, don’t pre-optimize your data processing for “future needs.” Have a plan to mine the answers to your current questions.
A little data processing can go a long way to making those answers accessible (and if you’re not sure where to start, one of our data analysts offers a few ideas in this post).
But before you begin collecting clickstream data, let alone processing it, pause to consider #3…
3. Explore non-analytics uses of your clickstream data.
You can send user clickstream data to tools such as Facebook and Google Ads to help target ads more precisely.
If you’re going to initiate a new kind of clickstream data collection, don’t miss a key opportunity to gain additional value from it.
Maximize your use of user event data from the get-go by exploring how it can make the non-analytics tools you use even more impactful.
What changes do we need to make to our data collection, processing or visualization to have better information for the next review?
And instrumenting your website and app for clickstream is only worth it if you’re prepared to do what the data tells you."
"House Prices in Ames, Iowa — Working with Kaggle","This summer kicked off my interest with Data Science, starting of a journey of self-learning and exploration.","House Prices in Ames, Iowa — Working with Kaggle
This summer kicked off my interest with Data Science, starting of a journey of self-learning and exploration.
House prices are an interesting dataset to look at because they usually have a lot of features in them and not all of the columns and values have a direct correlation to the price of the house. House prices depend on a lot of factors, mostly because of the general variation of preferences across the population, one thing however that is trivial for us to understand that bigger means more expensive. That is why house prices datasets are interesting to analyze and visualize, even more so, building a predictor to the prices of the house from the features of the house. This makes the dataset good to work with while learning the art of data science.
You can find my Jupyter notebook for the data analysis here and the dataset here.
“A large suburban house with a garden on a sunny day” by Jesse Roberts on Unsplash
Here I am using a very structured way to do this data analysis, and here are the steps I am taking. Each step will have a heading in this article.
Question or problem definition.
Acquire training and testing data.
Wrangle, prepare, cleanse the data.
Analyze, identify patterns, and explore the data.
Model, predict and solve the problem.
Visualize, report, and present the problem solving steps and final solution.
Supply or submit the results.
Question or problem definition
So of to Kaggle, where there is this amazing dataset is located. In this dataset, we have a list of house prices and information about the houses themselves from Ames, Iowa. Since it is a very descriptive dataset, there are a lot of things we can do with this dataset. However, to keep it short and sweet, let us define the scope of this analysis to be to complete a comprehensive exploratory data analysis (EDA) of the house prices and to build a regressor model of the prices of the houses based on the columns in the training set to predict house prices in the test set/validation set.
Before you get started with this notebook, it is highly recommended to go through this paper from the author of the dataset to get an overview of the dataset and how the usage is intended. This notebook uses techniques used by Alexandru Papiu in his Jupyter notebook. I figured learning how to approach a dataset with some help from the community is a good start.
Acquire training and testing data
Now let us get right down to it. Let us import all the required packages and training and test set first, and display the structure of the dataset using the shape and head functions
This is the how the dataset looks like returned after the calling df_train.head(5)
Here are the columns in the dataset, that we find out using this command.
These are the columns that we get from the command df_train.columns
Now that we are aware of the structure of the dataset, let us take a closer look at the dataset to analyze the features. This brings us to the next step, wrangling, prepare and cleanse the data.
Wrangle, prepare, cleanse the data
In this section we are going to go through multiple techniques used to wrangle, prepare, cleanse the data.
Here is a breakdown of the techniques we are going to use:
Finding out if there are any duplicate entries in the dataset
Finding out about the type of columns: numeric and categorical
Finding out the missing data in each column
Filling in missing values with relevant data
Finding and removing outliers from the data
Lets us get started right away!
Finding out if there are any duplicate entries in the dataset
Duplicate entries affect the accuracy of the predictor we intend to build, as the features of a specific house will affect the predictions of the classifier if there are duplicate entries of it. Here is the code to check if the data has duplicate columns.
Finding out about the type of columns: numeric and categorical
Now we have to see which columns have numerical columns and which columns have categorical columns in order to build features from each column. This would be used later in order to do normalization and fixing the numerical columns in the dataset.
These are the numerical and categorical columns in the dataset
Finding out the missing data in each column
Missing columns in the data can either signify lack of data or a specific negative value within a categorical column of the data.
Now we identify the columns in the dataset that are missing, and to find out to find out the missing data for each column, we would be using this piece of code to find out the number of missing rows in each column.
The Y-axis here is the number of missing entries for each column in the x-axis.
We can clearly see the columns PoolQC, MiscFeature, Alley, Fence have a lot of values that are missing. But taking a closer look at the data, we understand the some part of each of these columns have information regarding the Alley or PoolQC and Fence. We will be filling in the data with the mean value of each of the columns later during the analysis.
Filling in missing values with relevant data
We will be doing this later when preparing the data for the model. Now let us move on to removing the outliers in the data
Finding and removing outliers from the data
Here we are trying to identify the outliers in the data which we cannot use in the model because it skews the analysis towards values that are unlikely. Here we intend to visualize the data to find out the spread of the data so we can remove the outliers in the data.
This is the entire dataset with all the outliers in the data.
Here we are removing houses from the dataset that are larger than 4000 sqft and more than 700,000 in value because they look like outliers in the dataset.
Here is the data without the outliers
Analyze, identify patterns, and explore the data.
Finding out the distribution of data within the dataset
Now to find out how each of the variables are distributed amongst the dataset, we can pivot the table into its constituent features and find out how each variable is distributed amongst the dataset using pd.melt http://pandas.pydata.org/pandas-docs/stable/generated/pandas.melt.html
For this we can only use numeric features and not any categorical features because of the nature of sns.FacetGrid module we are going to use to find correlations between the dataset.
Distribution of various columns in the dataset and if we can model the data. Cleaner curves with rectangles within them are more accurately modeled.
House prices over the years
Now let us analyze the house prices over the course of the years and visualize this on a histogram. In this graph, each x column would represent the year and y-axis would represent the house prices for the year.
Prices on the y-axis and year of sale on the x-axis.
Overall condition of the house
Now let us plot the correlation between three columns: Year the house was sold, Overall condition of house and Living Area in the house. I wanted to visualize these leads in a seaborn pairplot so I used the following code to implement this.
A nice complex graph to find out the relation between these three columns
Correlation matrix
Now let us plot the correlation matrix for each of the columns in the dataset. This is done to have a clear picture on the correlation between columns in the dataset.
Correlation matrix for the dataset, the darker the color, the more the correlation between the two columns.
Normalization
To model and predict the problem, we first need to understand that our data by itself has to be represented in a form that is easy for the predictor to learn from, hence we have to prepare the data; normalize the numerical columns in the data and encode the categorical columns in the dataset, in order to build an effective model for the problem. This warrants a detailed explanation, and is based on the ability of the algorithm to work better given the need of the algorithm minimize the distance of the data point from the classifier’s prediction. This is also based on the ability of the computer to be able to deal with computation of smaller numbers more easily because of their representation.
One example of normalization is using a log function: Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally. Notice the x-axis in the following graphs, they indicate the range of values taken up by the column. This following code show us how to get the graph.
After and before of taking the log of the prices column
We take the log of the SalePrice in order to build a normalized model of the data, the reason why we do this is because predictors work better when the data is not distributed sparsely.
In this we are going to use the skew function to find out the columns in which the numerical data is not too skewed here.
I learned how to do this from https://www.kaggle.com/apapiu/regularized-linear-models where the skew function essentially returns the skewedness of the distribution of values in a column. We do not want columns with too much of skewedness because that affects the value returned by the log function.
In the following steps we are going to compute the skewedness and normalize the numerical columns in the data
Now let us encode the categorical columns in the data with the following code. This is done in order to convert the categorical columns into a feature based on the values in the dataset pd.get_dummies() takes a data of the form [a, b, b, c, null] and converts it into [0, 1, 1 , 2, 3]. This function has both the property of categorizing each value in the dataset and filling in the null values.
Now let us make the test and training set from the normalized data that we will use for fitting the model and predictions.
Model, predict and solve the problem
In this analysis we are going to try implement regression models from sklearn to try to fit this data. We can find more about these basic regression models in the link which has a detailed explanation for each model or look at sklearn documentation
In this analysis we would using the RMSE value to find out the error of the model for a given set of the data. The root mean square error (RMSE) of the dataset is the square root of sum of the squared distance between the predictions of the classifier and the actual value in the dataset. In simple terms, Root mean square error for a dataset is a way to measure accuracy of the classifier with respect to the dataset. Closer to 1, worse the model. Closer to 0 better the model.
Here we are going to define the RMSE on the training set so we can train our classifiers on the training set.
Ridge Regression
Now let us train our models on a Ridge regressor from sklearn.linear_model package. Ridge regularization has a parameter alpha which is used to tune the data to fit the model to the training or validation data. In this code below we are going to train the Ridge regressor and store the RMSE value in cv_ridge_train
Now let us plot the values of cv_ridge_train in order to find out the alpha for which we will have the lowest RMSE
Finding out the best regularization parameter, here its probably 5.
Now let us find out the lowest RMSE for the Ridge regressor.
So the RMSE is about 0.110, which is pretty good.
Lasso Regression
Implementing a lasso regressor is easy, this regressor works by training on columns that are relevant to the dataset and does not include data from columns that are not needed.
The RMSE is 0.108 which is lesser than the Ridge regressor model. The following code allows to me to find out the number of features that the Lasso model used for training and the columns its discarded.
Lasso picked 105 variables and eliminated the other 182 variables. Nice!
XGBoost model
Here we are going to try out the XGBoost model which is created as a package which implements the Gradient Boosting Regression. Here we use the XGBoost as a blackbox with some parameters. Here you can find a detailed explanation on how this model works. Here is the documentation for this package.
The following code is going to give us the RMSE for the training and test set for XGBoost model and plot the data.
Blue is the RMSE mean for the test set, Orange is the RMSE mean for the training set. Y-axis is the value of RMSE and x-axis signifies the number of epochs in training.
The following code below is going to fit the XGBoost model on the training data
The following code is going to predict values from the Lasso model and the XGBoost model and convert them back to normal after normalization.
The following code shows the correlation between the predictions of the two regressors, showing that the predictions from the two regressors have a pattern.
Correlation between predictions on the XGB model and predictions on the Lasso model.
Now using the predictions from two different predictors, we can combine the models using a simple linear function
Supply or submit the results.
Now we are going to export the data into a csv file to supply and submit the predictions
The end!
Thanks for reading my article, the code for this analysis is available here. You can follow me on Medium at Raghav Ravisankar. Please let me know your feedback through the comments!
",2713,Data Science,Raghav Ravisankar,https://medium.com/s/story/house-prices-in-ames-iowa-working-with-kaggle-91656d5ef6e0,"House prices are an interesting dataset to look at because they usually have a lot of features in them and not all of the columns and values have a direct correlation to the price of the house.
However, to keep it short and sweet, let us define the scope of this analysis to be to complete a comprehensive exploratory data analysis (EDA) of the house prices and to build a regressor model of the prices of the houses based on the columns in the training set to predict house prices in the test set/validation set.
Let us import all the required packages and training and test set first, and display the structure of the dataset using the shape and head functions
This would be used later in order to do normalization and fixing the numerical columns in the dataset.
These are the numerical and categorical columns in the dataset
But taking a closer look at the data, we understand the some part of each of these columns have information regarding the Alley or PoolQC and Fence.
We will be filling in the data with the mean value of each of the columns later during the analysis.
Here we are trying to identify the outliers in the data which we cannot use in the model because it skews the analysis towards values that are unlikely.
Distribution of various columns in the dataset and if we can model the data.
Now let us plot the correlation matrix for each of the columns in the dataset.
To model and predict the problem, we first need to understand that our data by itself has to be represented in a form that is easy for the predictor to learn from, hence we have to prepare the data; normalize the numerical columns in the data and encode the categorical columns in the dataset, in order to build an effective model for the problem.
We take the log of the SalePrice in order to build a normalized model of the data, the reason why we do this is because predictors work better when the data is not distributed sparsely.
In this we are going to use the skew function to find out the columns in which the numerical data is not too skewed here.
I learned how to do this from https://www.kaggle.com/apapiu/regularized-linear-models where the skew function essentially returns the skewedness of the distribution of values in a column.
In the following steps we are going to compute the skewedness and normalize the numerical columns in the data
Now let us encode the categorical columns in the data with the following code.
This is done in order to convert the categorical columns into a feature based on the values in the dataset pd.get_dummies() takes a data of the form [a, b, b, c, null] and converts it into [0, 1, 1 , 2, 3].
Now let us make the test and training set from the normalized data that we will use for fitting the model and predictions.
In this analysis we are going to try implement regression models from sklearn to try to fit this data.
In this analysis we would using the RMSE value to find out the error of the model for a given set of the data.
The root mean square error (RMSE) of the dataset is the square root of sum of the squared distance between the predictions of the classifier and the actual value in the dataset.
Now let us train our models on a Ridge regressor from sklearn.linear_model package.
Now let us plot the values of cv_ridge_train in order to find out the alpha for which we will have the lowest RMSE
Implementing a lasso regressor is easy, this regressor works by training on columns that are relevant to the dataset and does not include data from columns that are not needed.
The following code allows to me to find out the number of features that the Lasso model used for training and the columns its discarded.
The following code is going to give us the RMSE for the training and test set for XGBoost model and plot the data.
The following code below is going to fit the XGBoost model on the training data
The following code is going to predict values from the Lasso model and the XGBoost model and convert them back to normal after normalization."
Cryptocurrency Rating |【ObEN-Project PAI】| Madman Research Institute,Verdict:,"Cryptocurrency Rating |【ObEN-Project PAI】| Madman Research Institute

Verdict:
Based on the study of Madman Research Institute, Project PAI (PAI) is rated:
Score: 85.2 Grade: A-
Abstracts of the Rating:
1. Project Analysis:
PAI is a public decentralized platform (blockchain), on which both Artificial Intelligence (AI) and the blockchain protocol are complementarily administered. With the integration of AI and the blockchain, the decentralized network continuously provides computational power and collects authentic data to feed and optimize machine learning algorithm as to advance the development of the AI ecosystem hosted on the PAI blockchain. The objectives of Project PAI is well-defined, as the innovation has clear application scenarios that are adoptable by a vast market and commercially viable.
2. Team Analysis：
Established in 2014 and well-known for their strong technical background, ObEN is resourcefully well-connected in the industry. The company attracted a large array of reputable institutional investments from business giants including Tencent, Softbank, S.M. Entertainment Group and many other high-profile VCs.
3. Marketing, Social Network and Ecosystem Growth：
The marketing campaign is steadily growing existing social network communities and opening new channels within the social media space. Currently, China and South Korea have the most active social network communities while the Japanese fans community is catching up in growth. However, more exposure is needed in English-Speaking social media space to grow potential user base.
In terms of commercial collaboration of PAI, ObEN has signed up to cooperate with an abundance of well-known enterprises entities, institutions and projects.
4. Project Progress：
Due to strict SEC regulatory requirements in USA, overall project development is slightly trailing behind their roadmap. However, this guarantees the legitimacy of the Project PAI. At time of writing, PAI is yet to release the open-source code publicly. We will continue to follow the overall progress of Project PAI and adjust our rating based on latest information.
5. Token Economy：
PAI is a public blockchain platform which inevitably requires a long research and development cycle. The crowdsale capitalization is relatively on the high side.
The initial token distribution structure is designed to facilitate the future development of project and ecosystem. In term of liquidity, PAI token has been listed on Huobi cryptocurrency exchanges at time of writing. We will follow up on this as information becomes available.
【Expectation】：
Project PAI encompasses the two hottest technological potentials i.e. the blockchain and Artificial Intelligence. Presently, PAI’s MainNet has been launched and PAI token has been listed on cryptocurrency exchanges. Under current market condition, we expect the neutral market performance. Long-term wise, as a public blockchain project, to achieve a competitive edge in the AI realm, PAI will need to focus on their on-chain dApp development and ecosystem building. Based on their team strengths, funding and experience from ObEN’s deep-pocketed investors / stakeholders, we maintain a long term bullish view on PAI.
PART ONE. Project Analysis: (Weighing: 30%, 27/30)

Project PAI is developing an open-source blockchain protocol to decentralize AI development for individuals. It allows people to use this unique blockchain to manage their AI assets, to make an authentication of their identity, to start data learning and to develop application. PAI blockchain protocol enables the decentralization of PAI and contains three layers: authentication, network, and storage. The purpose of this protocol is give people trackable ownership and management rights (e.g., portability, limited sharing, and interoperability) to their intelligent personal data profiles or their PAI. The project aims to provide various customized AI services including social networks、entertainment 、health care, etc. This unique blockchain protocol sets a decentralized platform and provides information for advancing the technology of Artificial Intelligence, incorporating decentralized artificial intelligence development into the blockchain
In specific, PAI is a 3D intelligent avatar designed to look like you, speak like you, and behave like you in the digital space. Everyone can own their PAI, meanwhile, users receive rewards for the contributions of the data and resources.
1、Project Positioning： （45/50）
At present, Artificial Intelligence is one of the hottest areas in the development of science and technology. The AI industry has seen rapid growth since 2015. In the light of the market projection, by 2018, market size of AI is expected to achieve 270 billion CNY.

In recent years, the application of AI and “pan-entertainment” (multiple forms of entertainment including games, TV shows, movies and animations etc. branched from the same celebrity-themed intellectual property) are gaining familiarization and acceptance by the public.
In China, “Pan-Entertainment” is widely recognized as one of the “Top 8 internet hot trends” now. Smart AI model training can generate like-for-like models of celebrities or their literature. The focus of project PAI is exactly on the integration of AI and entertainment.
PAI enables creation of virtual characters, which can be applied in the movie and gaming industry. By commercial collaboration with celebrities (or the business entity that manages a celebrity), AI-powered digital clones of celebrities can also be created.
Different from common applications of AI such as daily chore/routine assistants (such as Siri) or in medical industry or in manufacturing, Project PAI aims to promote and innovate AI applications in the entertainment industry. For instance, PAI opens more interaction possibilities between celebrities and their fans through the use of intelligent virtual characters. This will in turn promote the growth of fans economy.
2、 Innovative Solution ： （Scores: 45/50）
(1) ObEN PAI Overall introduction
· The combination of AI and Blockchain
Based on official web publication and the whitepaper of Project PAI, we observed an inseparable relationship between PAI and ObEN. As partners, they work together to develop the PAI Network and relevant applications.
Founded in 2014, ObEN is headquartered in California. In 2017, by Series A investment funding size, ObEN ranked №1 as the most valuable startup company in south California. In the same year, ObEN also won the Partick Soon-Shiong
Innovation Awards from Los Angeles Business Journal. In the subsequent year of 2018, ObEn was amongst the Top 3 at the Entrepreneurship World Cup 2018 in Silicon Valley.
To date, ObEN has obtained investment funding from a number of well-known institutional investors including Tencent, Softbank, K11 Shanghai, HTC Vive , CMC and many other global business entities. ObEN, as an AI tech company, encompasses AI, AR and VR realms. So far, ObEN has registered 18 patents on its Personal AI products in U.S and exclusively owned the intelligence rights of those patents.
There are three crucial elements for the development of Artificial Intelligence are comprised of algorithm, computational power and data. Now, the major obstacles to AI development are computing power and data. Although PAI inherits the algorithm model from ObEN, computing power and data access are yet to be harnessed. The blockchain protocol of PAI adopts proof-of-work consensus, which collates fragmented computational resources from third parties to manage the increasing demand for computational power. Data access wise, despite the global explosive growth of total amount of data, the majority of usable and organized data is often possessed by large corporates while some other valuable data is isolated from access in hands of individuals. Therefore, the PAI blockchain network promotes and rewards users to participate by storing their individual data on-chain, and subsequently create value through the PAI network.

· Development approaches of the technology
Data to blockchain interface and smart contracts: Enable users to record data on the block and develop smart contracts interface. This will form the backbone source code of the PAI blockchain.
Application on the PAI blockchain: collaboration with business partners, actualize applications scenarios of different industries on the PAI blockchain.
Blockchain upgrades and evolutions: On the basis of the backbone blockchain, personal data and smart contracts, continue to develop ways to improve efficiency of the blockchain and iterate for more refined AI modules (e.g. POUW、Edge Computing) and provision of Developer’s API kit.
【Comment】：
The application of blockchain enables decentralization of data sampling, more efficient data distribution and circulation, and attaches economy value to data. The value of data is eventually returned to participants as rewards. The blockchain solution also gathers discrete and idle computational power from individuals to contribute to AI algorithm execution. This method drastically reduces computing cost. As obstacles on computation and data collection are eliminated, efficiency of AI is significantly enhanced to enable real-world applications.
(2）“Data Pointer”+“ Torrent network”, The Guarantee of Data Security and Anonymity
All PAI coin transactions are accompanied by data pointers, therefore, coin transactions can also be used to convey data. This is made possible by a “Torrent” based data storage network which operates in parallel with the PAI MainNet. Every PAI coin transaction can include a reference to a particular piece of data on the Torrent network, which allows data transactions between all parties to remain secured, anonymous and direct way. The encryption of message delivery ensures that data transaction can only occur when the sender willing to share data with the specific receiver who is also willing to receive.
This system shares data by means of using cryptocurrency, several advantages are as follow:
· Data transactions are not necessary bundled with public keys： In other words, all the activities leave a footprints on the blockchain and are easily traceable in any case of malicious usage.
· Data transactions are anonymous：All the data is transacted with PAI coins using individual public keys, therefore, it allows all transactions remain in a secured and anonymous status.
· It ensures that data is always accessible: Data is stored in BitTorrent network; thus, data will be always here to stay perpetually even if application developers no longer operates the system.

This two-tier system allows individuals to maintain control of their own digital identity and individual data profile. When developers create applications on the blockchain, they can use this data transaction system as well. That is to say, all data must be stored in Torrent network instead of being stored in traditional centralized servers. Let us imagine a worst-case scenario such as server destruction or business permanent closure, thanks to the decentralized storage, individual users still can access data.
(3）PAI’s Ecosystem — — Two-level structure of cryptocurrencies

PAI’s ecosystem is built on an AI distributed platform powered by a public blockchain protocol. ObEN and other developers will use the PAI platform to develop dApps. There are three types of roles within the ecosystem, which include miners, authenticators, and customers. Miners contributes their computing power gain PAI coin rewards. Authenticators gain PAI coins as rewards by verifying transactions. Consumers use dApps to perform activities such as socializing, dating, shopping and healthcare etc.

The PAI platform accepts a two-level structure of cryptocurrencies to keep the blockchain ecosystem and end-user experiences separate:
· The native and decentralized PAI Coin by Project PAI.
· Dedicated application-level credits, which can be designed by people and organizations for their own applications.
【Comment】：
Project PAI is systematically designed to service the whole ecosystem. Miners are rewarded by contributing the necessary computational power for AI development. DApps on the PAI platform attract users to participate and contribute data, which feeds into the PAI neuro network. This data feeding process provides machine learning training material, allowing the neuro network to further evolve in excellence.
PART TWO. Team Analysis(Weight 30%,Score 26.4)
1.The Founding Team (26/30)

Dr. Adam Zheng, co-founder and COO of ObEN, was also the co-founder of Baihe. com which is the largest dating platform of China. Before then, Dr. Adam Zheng was Venture Partner of US Lightspeed investment, director of Tongfang Co. , investment manager of Newbridge Capital. Dr. Adam Zheng has invested in BestTVNewMedia (600637), STAR Group Limited, New Silkroad Group.
Adam received his Master of Environmental Science from Tsinghua University and his Ph. D in Environmental Engineering from UC Davis and a Master of Financial Engineering from UC Berkeley.

Nikhil Jain is the cofounder and CEO of ObEN. Prior to ObEN, Nikhil worked at a healthcare company, Kaiser Permanente, developing electronic medical record applications. He previously cofounded two start-ups: “On Green”, a B2B marketplace for cleantech, and “Up and Running Software”, an open source software development company. Nikhil has an MBA from the University of Southern California, and a B.S in Electrical Engineering from Pondicherry University.
【Comment】:
The founding team has excellent academic background and successful entrepreneurial experience. Among them, Adam Zheng’s entrepreneurial experience in baihe.com, as well as a series of investment experiences will bring a lot of resources to this project for business development purposes.
2.Technical Team (32/40)

According to the whitepaper, the blockchain laboratory cofounded by Project PAI will be led by Alex Waters, who is the core developer of Project PAI and a major problem solver on a series of core technical issues of blockchain.
According to the description, , Alex was the core developer (now Bitcoin Core) of Satoshi Bitcoin Client from 2010, and also one of the original developers of Bitcoin. He was also the founder of CoinApex which is a blockchain Angel investors. He also founded Coin.co, a Bitcoin payment processing facilitator, and Coin Validation, a due diligence service for Bitcoin businesses.

In the area of artificial intelligence(AI) technology, the chief engineer of the technical team, Dr. Mark Harvilla, graduated from Carnegie Mellon University with a PhD in electrical engineering. He has been responsible for OBEN’s overall technology research and development. The whitepaper shows that he conducted research on AI technology at OBEN as early as 2014 and has developed AI singing technology.
In addition, the technical team also include Dr. Wang Rui Zhe who is the head of computational vision, Dr. Pierre Lanchantin who is the head of computational acoustics, Dr. Taehwan Kim who is the head of the special effect，and Dr. Elnaz Nouri who is the head of natural semantic understanding .
【Technical reserve】：
In terms of technical reserve, ObEN’s technical team, which is established in 2014, also has a considerable technical know-how in the AI field. Team members have published numerous academic papers on international tech journals. For instance, Dr. Taehwan Kim has published more than 10 papers in the machine learning field; Dr. Wang Rui Zhe has published 14 paper in the field of computational vision; Dr. Mark Harvilla and his team has published or joint published 169 papers in computational acoustics. All in all, the team has in total an accumulation of 200 verifiable academic papers.
【Comment】:
For blockchain experiences, as the first batch of blockchain experts, Project PAI blockchain laboratory’s principle Alex has rich experience in blockchain development. In the field of AI technology, the technical team members are from elite universities and has a solid technical foundation. Furthermore, the R&D on AI started as early as 2014. Overall, the technical team is a solid, mature and stable development team.
3. Investors and consultants (30/30)

ObEN has attracted many top investment institutions from around the world, including Tencent, Softbank, Cerberus, HTC VIVE X, South Korea’s SM Entertainment. It also assembles Gordon Cheng, LSM and other famous Angel investors.

ObEN’s advisory team is made up of top experts from elite universities, such as Abeer Alwan who is a speech recognition specialist at the University of California, Los Angeles; Walter Greenleaf, a behavioral neuroscientist at Stanford University; Sun Yajun who is the professor of linguistics at UCLA;
Kevin Knight, the professor at the institute of information science at the university of southern California, who is also a recognized pioneer in machine translation and a leading proponent of statistical machine translation. Last but not least, chairman of South Korea’s largest entertainment company SM, Lee Soo-man.
【Comment】:
Many excellent blochchain entrepreneurial firms can get investments from top blockchain incubator and blockchain investment funds. However, ObEN is a rare case because it is mainly invested by Tencent， Softbank and alike, it shows us the recognition from mainstream technology giants and venture capitals. Besides a group of scientists who can help the project with technical issues, the chairman of SM, LSM also brings huge benefit in marketing and business applications, whereas the company AI-Stars which Oben and SM cofounded will build personalized imaged AI for all celebrities of the SM corporation, and market them in Japan, South Korea and Southeast Asia.
PART THREE. Ecology Construction(Weight 30%, Score 25）
1. Community Ecology (50/60)
(1) Media Channel Integrity (25/30)
In terms of community building and social media channels, PAI has covered several mainstream social medias such as: Facebook / Twitter / WeChat / Medium and so on. On community building, PAI currently has chat groups on four mainstream social networks, which are Telegram / Wechat / Kakao / Line , the statistics are as follows:

(2) Community Activity (25/30)
At present, the community construction of Project Pai is mainly focused on the Asian market. Currently, the largest and most active communities are on China’s WeChat groups, with about 60 groups of 26,000 followers, followed by Telegram / Kakao for Korean. However, Facebook / Twitter / Telegram communities for English speaking countries show a huge lack in activity compared to Project PAI’s Chinese communities. Meanwhile, the community in Japan and Taiwan are also relatively small. Generally, US and EU blockchain projects attach great importance to medium.com, whereas the content and subscription number of Project PAI on medium are relatively low.
【Comment】:
At present, the community construction of Project PAI is mainly focused on China and Korea, among which Chinese community is huge and heated. On the other hand, the communities of other languages are Relatively under-developed, and still needs to be improved.
2. Business Ecology (40/40)
At present, Projet PAI has many important commercial partners, including Tencent, Softbank, Indonesia Lippo Group, South Korea S.M entertainment, and so on. The following information on business co-operations is gathered from reports of public sources:
(1) Tencent, the largest investor of ObEN, is using PAI technology to develop personalized AI products for some of its departments. For example, WeChat users can easily generate personalized AI images for social activities in the virtual world by using ObEN’s official WeChat channel: http://m.ikanchai.com/pcarticle/161354
(2) ObEN’s another major investor is SB Next Media Innovation Fund which is the VC fund under softbank south Korean who focused on AI. ObEN is the first investment project of this particular Fund. Softbank will use its global resources to help ObEN’s PAI technology. https://m.ithome.com/html/297739.htm
(3) ObEN and South Korea’s largest entertainment company S.M entertainment cofounded Al Stars which is the first AI celebrity agency. It just signed the AI copyright of all the stars under SM entertainment in May 2017. It also signed an agreement with wildly popular idol group SNH48. ObEN will use its PAI public chain to create exclusive virtual images of the wildly popular SNH48 girl idol group. In the future, all the stars’AI figure will interact with their fans on the PAI blockchain. https://mp.weixin.qq.com/s/aAvUP0iXdN6jiyrj_JOabA

(4) ObEN has signed a formal agreement with Southeast Asia’s corporate giant Lippo Group , the latter will use AI technology of the PAI public chain to develop AI general practitioner and other AI blockchain project in medical insurance area. It will also use the PAI public chain and PAI coin for fund raising. According to public sources, Lippo Group is one of the largest financial holding consortium in Indonesia. It controls publicly listed enterprises from more than 20 countries. As a result, AI general practitioners will be launched in hundreds of chinese and southeast Asian hospitals which the Lippo Group controls. https://mp.weixin.qq.com/s/5aT7bFmpwcCy_S3aB0kJjQ
(5) ObEN has completed a B round financing of 10 million USD in convertible bonds from K11. K11 belongs to Hongkong’s New World Development Company Limited, which owns retail brands such as Chow Tai Fook and K11. Mr. Zheng Zhi Gang, the founder of K11, used PAI’s technology to create his own Personal AI. He also used his PAI to create promotional video of the K11 art palace. K11 is also working with ObEN to further implement retail applications of PAI. https://mp.weixin.qq.com/s/OOjWR5SEOzyhe2ky2jtitQ
(6) ObEN will jointly operate a blockchain laboratory with two chinese public chain project BTM and QTUM.
https://mp.weixin.qq.com/s/jejbYtTVee-akNgcAjdh-g http://www.8btc.com/bytom-ObEN-1226
(7) PAI mobile APP — Paiyo

Project PAI already has a beta version of its mobile APP, allowing users to create their own personal AI by taking selfies and short video recordings.The official version will also be available shortly. You can also apply for a selective testing of the beta version. https://paiyo.com/coming-soon/
【Comment】:
The ObEN — Project PAI is carrying out commercial co-operation and promotion in the area of entertainment, medical treatment, blockchain technology among other fields. If PAI’s cooperation with Tencent can lead to its implementation and integration into WeChat ecosystem, there will be a huge number of users. By partnering with South Korea’s S.M. entertainment, it can also leverage the many stars of S.M. to expand its influence. The cooperation with BTM and QTUM will enhance one another in blockchain technology itself.
PART FOUR. Development Progress (Weight 10%, Score 7)
1. Promise fulfilment (40/50)

According to the project’s development plan released on the official website, the launch time of its mainnet was set in the first quarter of 2018, but the actual launch time was on June 5, which is the second quarter of 2018. The PAI wallet was released at the same time. In terms of development progress, it is a little behind the roadmap’s schedule. However, the delay is due to fulfillment of the American SEC requirements.
Here we layout the relevant regulations of SEC:
1. Fundraising is restricted to private offering for qualified investors , public offer is not allowed. Therefore the white paper and relevant information can not appear on media.
2. Utility-token is a form of digital asset, only utility-token can be traded. Therefore, crypto-currencies can only be clasified as a utility-token if the following conditions are met:
· The blockchain mainnet which generates the token has already been launched.
· The blockchain mainnet has at least one DApp running which allowed users to use the tokens immediately once they get them.
It is known that the PAI has met the above conditions in the second quarter, including the launch of the mainnet and the running of DApp on the blockchain.
【Comment】:
The project was slightly delayed compare to the roadmap released earlier. However, according to our investigation, the delay is due to SEC compliance purposes, where the launch of mainnet and running of DApps are prerequisites. Therefore, the promise is kept. Follow-up performance tracking of post-launch progress will be conducted accordingly.
2. Code Qualicy (30/50)
According to the white paper, the PAI chain extends the capacities of the original bitcoin code, adding databases and smart contracts functionality. It is reported that Project PAI will make public the source code of PAI chain by the end of June 2018, so we are not able to read its code on Github yet.
【Comment】:
The source code has not been made public yet. However, given that core developer Alex Waters was once a member of the bitcoin development team, we will score the code quality as 30 points. We might adjust the score after the code is released.
PART FIVE. Token economy(Weight 10%, Score 6.8)
1. Financing situation (18/30)
According to relevant regulations of SEC, fundraising is restricted to private offering for qualified investors , public offer is not allowed. Therefore PAI has not disclosed any fundraising information to the public yet. However, according to various sources of information, it is widely known that the total amount of financing is approximately $100 million to $150 million from the end of 2017 to the end of March 2018 .
【Comment】:
Given that this financing amount is correct, it is relatively large. Considering PAI is a public chain project and an AI project, the development period is long, therefore this financing amount is acceptable.
2. Liquidity (15/30)
At the time of writing, PAI has already been listed on Huobi international site in June 29th, and trading has started on July 2nd. Besides Huobi, Lbank has also listed PAI coin. On the first day of trading , the trading volume has reached about 0.5 billion, ranking top 25 in all cryptocurrencies. For a newly listed project, this market performance indicates its recognition from the public.
【Comment】:
In terms of liquidity, supporting exchanges are few. However, considering the fact that PAI has been supported by top exchanges like Huobi, a neutral score of 15 is given under the current circumstances and may be adjusted according to the actual situation in the future.
【Friendly reminder】:
ObEN PAI is a public chain and it is not based on the ERC20 token of Ethereum. At present, the mainnet has been launched, and there are lots of look-alike tokens on Ethereum with the same symble as “PAI”. Investors should identify them carefully.
3. Token Holding Structure (35/40)

Oben-PAI’s token is PAI, totalling 2.1 billion, of which 35% are owned by investors; 15% by project developers; 15% goes to contributors to the PAI project development; 5% goes to the PAI foundation, and the final remaining 30% comes from mining, with the first block awarded at 1,500 PAI token, after which the number will be halved for every 210,000 blocks or about every four years. In terms of token holdings, the tokens holded by PAI project developers, PAI ambassadors, and the PAI foundation will be locked up for a year.
【Comment】:
The holding structure of PAI token is reasonable. Besides investors, 15% share allocated to the project development team is reasonable, the remaining 50% share is allocated to miners, PAI contributor and PAI foundation who are important pushers of PAI project development in the future. Here we determine the allocation structure of PAI token reasonable and helpful to the long-term ecological development of the project.
Summary:
Based on the above rating analysis, PAI project positions itself as an AI public chain, aiming to build an application development platform and promote the integration of blockchain and AI. Accurate industry positioning leads to effective improvement of efficiency. A solid team background, abundant industry resources, top level of investors and consultants, etc., these are all merits of PAI project.However, the codes are not publicized yet, and the communities of other languages is underdeveloped. Therefore, a score of 85.2 and rating of A- is given.
As a public chain, PAI also faces the competition from other AI projects. The subsequent technological development, community construction and ecological implementation are vital to the project.Under the current circumstances, relying on the abundance of resources of the team, the PAI ecosystem is gradually developing and we look forward to subsequent expansion to European and American market as the community develops. The codes should be open-sourced as soon as possible. Providing that the capability of the team is optimized and the resources of the ecosystem is utilized, a subsequent raise of the rating is expected.

",4473,Blockchain,Madman Research Institute,https://medium.com/s/story/crypto-currency-rating-oben-project-pai-madman-research-institute-20f689d4ec38,"PAI is a public decentralized platform (blockchain), on which both Artificial Intelligence (AI) and the blockchain protocol are complementarily administered.
With the integration of AI and the blockchain, the decentralized network continuously provides computational power and collects authentic data to feed and optimize machine learning algorithm as to advance the development of the AI ecosystem hosted on the PAI blockchain.
The objectives of Project PAI is well-defined, as the innovation has clear application scenarios that are adoptable by a vast market and commercially viable.
The company attracted a large array of reputable institutional investments from business giants including Tencent, Softbank, S.M. Entertainment Group and many other high-profile VCs. 3.
In terms of commercial collaboration of PAI, ObEN has signed up to cooperate with an abundance of well-known enterprises entities, institutions and projects.
We will continue to follow the overall progress of Project PAI and adjust our rating based on latest information.
PAI is a public blockchain platform which inevitably requires a long research and development cycle.
The initial token distribution structure is designed to facilitate the future development of project and ecosystem.
In term of liquidity, PAI token has been listed on Huobi cryptocurrency exchanges at time of writing.
Project PAI encompasses the two hottest technological potentials i.e. the blockchain and Artificial Intelligence.
Presently, PAI’s MainNet has been launched and PAI token has been listed on cryptocurrency exchanges.
Long-term wise, as a public blockchain project, to achieve a competitive edge in the AI realm, PAI will need to focus on their on-chain dApp development and ecosystem building.
Based on their team strengths, funding and experience from ObEN’s deep-pocketed investors / stakeholders, we maintain a long term bullish view on PAI.
Project PAI is developing an open-source blockchain protocol to decentralize AI development for individuals.
It allows people to use this unique blockchain to manage their AI assets, to make an authentication of their identity, to start data learning and to develop application.
The purpose of this protocol is give people trackable ownership and management rights (e.g., portability, limited sharing, and interoperability) to their intelligent personal data profiles or their PAI.
Different from common applications of AI such as daily chore/routine assistants (such as Siri) or in medical industry or in manufacturing, Project PAI aims to promote and innovate AI applications in the entertainment industry.
Based on official web publication and the whitepaper of Project PAI, we observed an inseparable relationship between PAI and ObEN.
As partners, they work together to develop the PAI Network and relevant applications.
To date, ObEN has obtained investment funding from a number of well-known institutional investors including Tencent, Softbank, K11 Shanghai, HTC Vive , CMC and many other global business entities.
Although PAI inherits the algorithm model from ObEN, computing power and data access are yet to be harnessed.
Therefore, the PAI blockchain network promotes and rewards users to participate by storing their individual data on-chain, and subsequently create value through the PAI network.
This will form the backbone source code of the PAI blockchain.
When developers create applications on the blockchain, they can use this data transaction system as well.
PAI’s ecosystem is built on an AI distributed platform powered by a public blockchain protocol.
ObEN and other developers will use the PAI platform to develop dApps.
The PAI platform accepts a two-level structure of cryptocurrencies to keep the blockchain ecosystem and end-user experiences separate:
Among them, Adam Zheng’s entrepreneurial experience in baihe.com, as well as a series of investment experiences will bring a lot of resources to this project for business development purposes.
ObEN has attracted many top investment institutions from around the world, including Tencent, Softbank, Cerberus, HTC VIVE X, South Korea’s SM Entertainment.
Besides a group of scientists who can help the project with technical issues, the chairman of SM, LSM also brings huge benefit in marketing and business applications, whereas the company AI-Stars which Oben and SM cofounded will build personalized imaged AI for all celebrities of the SM corporation, and market them in Japan, South Korea and Southeast Asia.
On community building, PAI currently has chat groups on four mainstream social networks, which are Telegram / Wechat / Kakao / Line , the statistics are as follows:
At present, the community construction of Project Pai is mainly focused on the Asian market.
Generally, US and EU blockchain projects attach great importance to medium.com, whereas the content and subscription number of Project PAI on medium are relatively low.
At present, the community construction of Project PAI is mainly focused on China and Korea, among which Chinese community is huge and heated.
At present, Projet PAI has many important commercial partners, including Tencent, Softbank, Indonesia Lippo Group, South Korea S.M entertainment, and so on.
(1) Tencent, the largest investor of ObEN, is using PAI technology to develop personalized AI products for some of its departments.
For example, WeChat users can easily generate personalized AI images for social activities in the virtual world by using ObEN’s official WeChat channel: http://m.ikanchai.com/pcarticle/161354
ObEN is the first investment project of this particular Fund.
Softbank will use its global resources to help ObEN’s PAI technology.
ObEN will use its PAI public chain to create exclusive virtual images of the wildly popular SNH48 girl idol group.
(4) ObEN has signed a formal agreement with Southeast Asia’s corporate giant Lippo Group , the latter will use AI technology of the PAI public chain to develop AI general practitioner and other AI blockchain project in medical insurance area.
K11 is also working with ObEN to further implement retail applications of PAI.
(6) ObEN will jointly operate a blockchain laboratory with two chinese public chain project BTM and QTUM.
Project PAI already has a beta version of its mobile APP, allowing users to create their own personal AI by taking selfies and short video recordings.The official version will also be available shortly.
The ObEN — Project PAI is carrying out commercial co-operation and promotion in the area of entertainment, medical treatment, blockchain technology among other fields.
· The blockchain mainnet has at least one DApp running which allowed users to use the tokens immediately once they get them.
It is known that the PAI has met the above conditions in the second quarter, including the launch of the mainnet and the running of DApp on the blockchain.
Considering PAI is a public chain project and an AI project, the development period is long, therefore this financing amount is acceptable.
ObEN PAI is a public chain and it is not based on the ERC20 token of Ethereum.
Here we determine the allocation structure of PAI token reasonable and helpful to the long-term ecological development of the project.
Based on the above rating analysis, PAI project positions itself as an AI public chain, aiming to build an application development platform and promote the integration of blockchain and AI.
A solid team background, abundant industry resources, top level of investors and consultants, etc., these are all merits of PAI project.However, the codes are not publicized yet, and the communities of other languages is underdeveloped.
As a public chain, PAI also faces the competition from other AI projects.
The subsequent technological development, community construction and ecological implementation are vital to the project.Under the current circumstances, relying on the abundance of resources of the team, the PAI ecosystem is gradually developing and we look forward to subsequent expansion to European and American market as the community develops."
DeepBrain Chain Monthly Report August 2018,Valued Members of DeepBrain Chain Global Community:,"DeepBrain Chain Monthly Report August 2018

Valued Members of DeepBrain Chain Global Community:
August saw a significant and industrious month for the development of DeepBrain Chain. We are elated to have brought the DBC AI Training MainNet online, a particularly major event in the road-map of DeepBrain Chain. Succeeding this milestone was yet another great stride in the journey towards a new era of AI as we proudly introduced DeepToken Exchange- the world’s first digital asset exchange dedicated to AI. Concurrently, the team has been making major progress reaching out to AI clients, including students and teachers from renowned universities as well as AI companies in the U.S. and around the world. These esteemed clients will be the trailblazing pioneers of our DBC AI Training MainNet, providing the solid foundation upon which the structure of the DeepBrain Chain Platform will develop and expand.
1. Marketing
⦁ Media Meetup (August 7th)
On August 7th, DeepBrain Chain were invited to attend a media gathering with journalists from 30 finance and blockchain media agencies. With such a wide and influential audience, Feng He, our CEO, seized the opportunity to re-emphasize DeepBrain Chain’s main proposition; using blockchain to solve AI companies’ computing power bottleneck, thereby saving 70% on computing costs.
Feng He promotes the DeepBrain Chain vision
⦁ NewBlockchain CHINA 2018 (August 11th)
NewBlockchain CHINA is a series of blockchain events held in 20 Chinese cities. On August 11th, DeepBrain Chain CEO Feng He was invited to speak about DeepBrain Chain’s innovative integration of the blockchain and how this technology provides the catalyst to bring about a new era of AI.
CEO Feng He sits on the expert panel at NewBlockchain CHINA
⦁ 2018 China Blockchain Technology Applications Award (August 18th)
On August 18th, the 2018 China Blockchain Technology Applications Conference, co-hosted by the State Information Center of China and Shenzhen Municipal Internet Society, themed “Chain the World to Create a Better Future” gave voice to a heated discussion regarding the future of blockchain technology, including its potential applications in both AI and, in a more general sense, real-world solutions in general. DeepBrain Chain were proud to be awarded the “2018 China Blockchain Technology Applications Award” and receive media coverage from Shenzhen Municipal TV Station.
2018 China Blockchain Technology Applications Award
⦁ “AI + Blockchain” Meetup (August 25th)
On August 25th, DeepBrain Chain was invited to attend Shanghai’s “AI + Blockchain” meet up, together with SingularityNET, Cortex and Bottos to discuss trends in Artificial Intelligence and Blockchain integration. At the event, Eric, Marketing Director of DeepBrain Chain, shared the ecological strategic layout of our Artificial Intelligence program. Unlike other projects, DeepBrain Chain will employ a symbiotic relationship with DeepToken Exchange, the world’s first AI industry digital asset exchange, to accelerate and proliferate the era of Artificial Intelligence integrated with Blockchain technology, to the mutual benefit of investors, providers and requesters.
⦁ Blockchain Tech Summit (September 2nd)
On September 2nd, a Blockchain Tech Summit was held in Shanghai and hosted by Donghua University, one of the most prestigious universities in China. At the invitation of the host, Steve Miao, our Blockchain Test Architect, attended the summit and provided the audience with a technical demonstration, including displaying the architecture of the DBC AI Training MainNet and the multiple advantages of distributed AI computing power. He also discussed the real-time AI training being conducted on the DeepBrain Chain platform, a topic which piqued the interest of the audience.
Steve Miao, Blockchain Test Architect, conducts technical demonstration
2. Media Attention
Reports on DeepBrain Chain
⦁ Jinse Finance: “Constructing AI computing infrastructure DeepBrain Chain breaking artificial intelligence robustness”
⦁ Dark Reading: “The Enigma of AI & Cybersecurity” — CAO Dr. Dongyan Wang muses on the potentialities of a future in which AI is the basis of advanced cybersecurity

⦁ CryptoBriefing published “How Blockchain Will Work With The Cloud and AI” and mentioned DeepBrain Chain, specifically the ability of the platform to reduce computing costs by 70%.
⦁ Coin Report published : “Unlocking Incentives with Blockchain”, featuring contributions from DeepBrain Chain’s CEO Feng He, discussing the ways in which blockchain technology achieves decentralized, self-sufficient operation via motivation of incentives akin to traditional market practices
⦁ IdeaMensch published an interview with DeepBrain Chain CEO Feng He, delving into behind-the-scenes stories about his business journey and the illustrious framework of ideas behind DeepBrain Chain.
AI Training MainNet
The DBC AI Training MainNet went live on August 8th; a technical milestone that allows AI users to conduct real-time AI training and pay an equivalent amount of DBC for computing resources. Teachers and students from universities and AI companies around the globe have started to use the DBC AI Training MainNet. These early users will provide the much needed stepping stone to broader, actualized commercial availability of our platform, including and accompanied by the recognition and interest of seasoned industry partners.
Currently, the Institute of Computing Technology of the Chinese Academy of Sciences has successfully accessed the Deepbrain Chain AI cloud computing platform. The Chinese Academy of Sciences, Shanghai Institute of Microsystems, Beijing Institute of Technology, “Work Together”, other research institutions and typical customers of AI companies have begun to use the GPU power provided by DeepBrain Chain. Simultaneously, students from a number of colleges and universities in the United States (such as the University of California at Berkeley, University of Texas, and Illinois Institute of Technology) have begun using the DeepBrain Chain AI platform as the first AI training network.

Sludgefeed report “DeepBrain Chain Training Net Goes Live, Offering an Alternative for GPU Miners”
PR Log: “AI Training Net — Affordable Computing Power Made Possible”
Netease News: “DeepBrain Chain AI Training Officialy Online to create Global AI CCloud Computing Platform”
Jinse Golden Finance: “DeepBrain Chain AI Training Officially Online”
Russian Media
Bitjournal
Kriptovalyuta
Crypto-mining
Coinrater
Coinrace
Criptonomica
Blockonomi: “DeepBrain Chain Goes Online: DLT Could Power AI Development”

DeepToken Exchange Reports

24–7
Sludgefeed
BlockTribune
Reuters
NASDAQ
Yahoo Finance
Yahoo
Canadian Money
Morningstar
Digital Journey


KOL

Cryptocurrency YouTubers Keith Wareing and Crypto Fiend interviewed Dr. Wang, the Chief Artificial Intelligence Officer of Deepbrain Chain, and conducted in-depth discussion regarding Deepbrain Chain’s ambitious venture with DeepToken Exchange, the world’s first AI industry digital asset exchange. In the interviews, Dr. Wang explains how the mutually beneficial collaboration between DeepToken Exchange and DeepBrain Chain will provide computing support, algorithm model trading, big data circulation for AI enterprises and enable the sharing of data use and ownership rights.
Keith Wareing Interview
Crypto Fiend Interview

3. Technological Progress
Silicon Valley Team
China and the United States, collaborating on the popular “Warhawk” project, the largest online training institution for international students, will utilize the DeepBrain Chain AI training network and senior experts from Silicon Valley to train their students in AI application. Thousands of undergraduate and high school students who wish to stay in the US will become DeepBrain Chain users. Moreover, the data and models generated internally by the reputable AI application project will also be able to be shared and traded in the AI ​​marketplace provided by the DeepBrain Chain platform.
Cybereason, the world’s leading endpoint security monitoring company, will establish a strategic partnership with DeepBrain Chain to jointly explore the application of the most advanced real-time “endpoint awareness” technology in the blockchain field. This cooperation will greatly enhance the reliability and security of the DeepBrain Chain AI platform nodes around the globe.
The AI ​​application team’s successful cross-industry AI video anomaly detection technology developed in the DeepBrain Chain AI platform will participate in the World Smart Manufacturing Conference in October. The team will demonstrate blockchain technology to professionals from around the world in the field of intelligent manufacturing to develop the advancement and efficiency of cross-industry AI technology.
The AI ​​application development team successfully completed the training of the deep learning model for the detection of surface flaws in steel plates.
Distributed high-efficiency training of complex models on 2,4, and 8 GPUs respectively using Horovod technology.
The AI ​​application team trained the arrhythmia (Arrhythmia) AI model for portable electrocardiograph manufacturer QT medical, achieving a detection accuracy of over 97%. Successfully demonstrating the AI ​​software development capability of the DeepBrain Chain platform to service the majority of hardware manufacturer’s AI requirements.
Slovenian data center with 768 enterprise-class GPUs and huge storage space has become a DeepBrain Chain in Europe, laying the foundation for providing enterprise-class high-end users with secure and reliable AI computing power.
Shanghai Team
Key tasks of the R&D team: including AI Training Net v0.3.4.0 release, v0.4 main chain design, etc.
Architecture design
Multi-chain + fragmented architecture network; architecture design: cross-sliced transaction design; state fragmentation; organizational technology sharing and discussion of state fragmentation architecture design;
DGP solution analysis: read and analyze DGP source code;
Data encryption scheme analysis, container enhancement technology;
Testing
v0.3.4.0 version completed the second round of testing, the entire network upgraded;
Development
v0.3.4.0 version: fixed the second round of test problems, upgraded the whole network;
AI Training Net v0.3.4.0 version manual refresh;
Further enhance the writing of UT unit test code for key components of Matrix core;
4. Future Events
DeepBrain Chain and Silicon Valley Business School bring ConsenSys, Danhua Capital, Defengjie founder to GDIS
GDIS October 2018
On October 1, 2018, the second GDIS Global Disruptive Innovation Summit (and the first International Blockchain Expo) will be held in Silicon Valley, focusing on three major themes: blockchain application and investment, blockchain + artificial intelligence and blockchain technology talents. The conference is divided into two sections. The main venue area will be used by top technical experts in order to share their extensive blockchain experience. Well-known investors impart relevant wisdom, and dinners are arranged for VIPs and speakers. The event provides a global, renowned social platform for investors, entrepreneurs and technical elites, engendering the best communication opportunities. The Summit will present the “Global Disruptive Innovation Award” for quality projects and companies participating in the exhibition. The Expo Pavilion will gather a number of companies including blockchain and AI companies, in order to showcase emergent products and technologies while also providing opportunities for all participants to seek out valued business cooperation and talent.
World-renowned, high-caliber guests will be in attendance at the event providing cogent and wise insight into the the topic of blockchain and artificial intelligence!
Just some of the experts scheduled to be in attendance at GDIS
Tim Draper, the famous American venture capitalist, founder of DFJ Investment Fund, third generation of the Draper family and originator of Silicon Valley venture capital. Some of the well-known companies he has invested in include Baidu, Tesla, Hotmail, Skype, and SpaceX.
Zhang Zhang, the founder and chairman of Danhua Capital, a member of the National Academy of Sciences, a tenured professor of physics at Stanford University, and the discoverer of “Angel Particles”. He is committed to investing in the most disruptive innovative technology in the United States, covering a number of emerging and expanding fields such as blockchain and artificial intelligence.
Guo Hongcai, a famous angel investor, early proponent of Bitcoin, and the founder of the Digital Money Training Club known as the “Digital Currency Circle Whampoa Military Academy”. He is the lead consultant for several well-known ICO projects and is the founder and promoter of BitcoinGod, the charitable eco-blockchain project, dedicated to the promotion and development of global blockchain businesses.
Experts and enthusiasts in the field of artificial intelligence and blockchain are welcome to purchase tickets!
Sponsorship and media cooperation please contact: contact@deepbrainchain.org
DeepToken Exchange Overseas Road Show
DeepBrain Chain will be bringing DeepToken Exchange, the world’s first personal smart industry digital asset exchange, on its first overseas road show. The current planned areas include Vietnam (September 12), Thailand (September 15) and South Korea (TBA). CEO Feng He will visit Vietnam and Thailand to raise international awareness of the ways in which DeepBrain Chain and DeepToken Exchange work in harmony to create the “Blockchain + Artificial Intelligence” ecosystem and contribute to the development of the AI ​​industry. Stay tuned for further information.
5. Recruitment
Warm Welcome to New Member of Silicon Valley Team
Meimei Ouyang, Personnel Director
Meimei Ouyang is from Chengdu, Sichuan. A year after graduating from Beijing, she went to the United States to obtain a Master’s Degree in Human Resources Development. Subsequently, she has worked in the United States for more than 10 years. She believes that life is like a journey and likes to constantly learn and challenge herself. She has worked in both already powerful and startup companies, accumulating relevant experience in recruitment, employee relations, compensation and benefits, institutional processes, corporate culture, and leadership training. In her spare time, she enjoys playing the piano, reading, traveling and spending time with her family.
We are hiring development engineers, security architects and other positions. All self-provided/recommended resumes are given feedback within 48 hours, and we welcome external referrals.
If you recommend a friend/classmate/colleague successfully, after the month of the candidate’s entry you will be given a recommendation reward of 20,000RMB.
Contact email: erin@deepbrainchain.org
Please attach the resume of your recommendation, as well as referee contact information, so that we can issue bonuses as awarded.
YouTube
Medium , Apple Podcast
Twitter , Facebook
Telegram English Group: ( https://t.me/deepbrainchain )
Telegram Korean Group: (https://t.me/DeepBrainChainKor )
Telegram Vietnam Group:( https://t.me/DeepBrainChainVietnam)
Telegram Indonesia Group:(https://t.me/DeepBrainChainIndonesia )
Telegram Thai Group: ( https://t.me/DeepBrainChainThai )
Telegram Russian Group:(https://t.me/DeepBrainChainRussia )
Telegram Mining Machine Group:(https://t.me/DeepBrainChainAIminers )
Twitter: (http://twitter.com/DeepBrainChain )
Facebook Page:( https://www.facebook.com/OfficialDeepBrainChain/ )
Reddit: ( https://www.reddit.com/r/DeepBrainChain/ )
About DeepBrain Chain

DeepBrain Chain is the world‘s first AI computing platform driven by blockchain. It uses blockchain technology to help AI companies save up to 70% of computing costs while protecting data privacy in AI training. Its vision is to build a “Decentralized AI Cloud Computing Platform” and become “The AWS in AI”
-Yours sincerely, the DeepBrain Chain Team
https://www.deepbrainchain.org
",2265,Blockchain,DeepBrain Chain,https://medium.com/s/story/deepbrain-chain-monthly-report-august-2018-1dae00246e42,"We are elated to have brought the DBC AI Training MainNet online, a particularly major event in the road-map of DeepBrain Chain.
These esteemed clients will be the trailblazing pioneers of our DBC AI Training MainNet, providing the solid foundation upon which the structure of the DeepBrain Chain Platform will develop and expand.
On August 7th, DeepBrain Chain were invited to attend a media gathering with journalists from 30 finance and blockchain media agencies.
On August 11th, DeepBrain Chain CEO Feng He was invited to speak about DeepBrain Chain’s innovative integration of the blockchain and how this technology provides the catalyst to bring about a new era of AI.
DeepBrain Chain were proud to be awarded the “2018 China Blockchain Technology Applications Award” and receive media coverage from Shenzhen Municipal TV Station.
On August 25th, DeepBrain Chain was invited to attend Shanghai’s “AI + Blockchain” meet up, together with SingularityNET, Cortex and Bottos to discuss trends in Artificial Intelligence and Blockchain integration.
At the event, Eric, Marketing Director of DeepBrain Chain, shared the ecological strategic layout of our Artificial Intelligence program.
Unlike other projects, DeepBrain Chain will employ a symbiotic relationship with DeepToken Exchange, the world’s first AI industry digital asset exchange, to accelerate and proliferate the era of Artificial Intelligence integrated with Blockchain technology, to the mutual benefit of investors, providers and requesters.
At the invitation of the host, Steve Miao, our Blockchain Test Architect, attended the summit and provided the audience with a technical demonstration, including displaying the architecture of the DBC AI Training MainNet and the multiple advantages of distributed AI computing power.
He also discussed the real-time AI training being conducted on the DeepBrain Chain platform, a topic which piqued the interest of the audience.
⦁ CryptoBriefing published “How Blockchain Will Work With The Cloud and AI” and mentioned DeepBrain Chain, specifically the ability of the platform to reduce computing costs by 70%.
⦁ Coin Report published : “Unlocking Incentives with Blockchain”, featuring contributions from DeepBrain Chain’s CEO Feng He, discussing the ways in which blockchain technology achieves decentralized, self-sufficient operation via motivation of incentives akin to traditional market practices
Teachers and students from universities and AI companies around the globe have started to use the DBC AI Training MainNet. These early users will provide the much needed stepping stone to broader, actualized commercial availability of our platform, including and accompanied by the recognition and interest of seasoned industry partners.
Currently, the Institute of Computing Technology of the Chinese Academy of Sciences has successfully accessed the Deepbrain Chain AI cloud computing platform.
The Chinese Academy of Sciences, Shanghai Institute of Microsystems, Beijing Institute of Technology, “Work Together”, other research institutions and typical customers of AI companies have begun to use the GPU power provided by DeepBrain Chain.
Simultaneously, students from a number of colleges and universities in the United States (such as the University of California at Berkeley, University of Texas, and Illinois Institute of Technology) have begun using the DeepBrain Chain AI platform as the first AI training network.
Cryptocurrency YouTubers Keith Wareing and Crypto Fiend interviewed Dr. Wang, the Chief Artificial Intelligence Officer of Deepbrain Chain, and conducted in-depth discussion regarding Deepbrain Chain’s ambitious venture with DeepToken Exchange, the world’s first AI industry digital asset exchange.
In the interviews, Dr. Wang explains how the mutually beneficial collaboration between DeepToken Exchange and DeepBrain Chain will provide computing support, algorithm model trading, big data circulation for AI enterprises and enable the sharing of data use and ownership rights.
China and the United States, collaborating on the popular “Warhawk” project, the largest online training institution for international students, will utilize the DeepBrain Chain AI training network and senior experts from Silicon Valley to train their students in AI application.
Moreover, the data and models generated internally by the reputable AI application project will also be able to be shared and traded in the AI ​​marketplace provided by the DeepBrain Chain platform.
Cybereason, the world’s leading endpoint security monitoring company, will establish a strategic partnership with DeepBrain Chain to jointly explore the application of the most advanced real-time “endpoint awareness” technology in the blockchain field.
The AI ​​application team’s successful cross-industry AI video anomaly detection technology developed in the DeepBrain Chain AI platform will participate in the World Smart Manufacturing Conference in October.
The team will demonstrate blockchain technology to professionals from around the world in the field of intelligent manufacturing to develop the advancement and efficiency of cross-industry AI technology.
DeepBrain Chain and Silicon Valley Business School bring ConsenSys, Danhua Capital, Defengjie founder to GDIS
On October 1, 2018, the second GDIS Global Disruptive Innovation Summit (and the first International Blockchain Expo) will be held in Silicon Valley, focusing on three major themes: blockchain application and investment, blockchain + artificial intelligence and blockchain technology talents.
He is committed to investing in the most disruptive innovative technology in the United States, covering a number of emerging and expanding fields such as blockchain and artificial intelligence.
DeepBrain Chain will be bringing DeepToken Exchange, the world’s first personal smart industry digital asset exchange, on its first overseas road show.
CEO Feng He will visit Vietnam and Thailand to raise international awareness of the ways in which DeepBrain Chain and DeepToken Exchange work in harmony to create the “Blockchain + Artificial Intelligence” ecosystem and contribute to the development of the AI ​​industry.
DeepBrain Chain is the world‘s first AI computing platform driven by blockchain.
It uses blockchain technology to help AI companies save up to 70% of computing costs while protecting data privacy in AI training."
"Sentiment Analysis: Concept, Analysis and Applications","Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a…","Sentiment Analysis: Concept, Analysis and Applications

Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations. However, analysis of social media streams is usually restricted to just basic sentiment analysis and count based metrics. This is akin to just scratching the surface and missing out on those high value insights that are waiting to be discovered. So what should a brand do to capture that low hanging fruit?
With the recent advances in deep learning, the ability of algorithms to analyse text has improved considerably. Creative use of advanced artificial intelligence techniques can be an effective tool for doing in-depth research. We believe it is important to classify incoming customer conversation about a brand based on following lines:
Key aspects of a brand’s product and service that customers care about.
Users’ underlying intentions and reactions concerning those aspects.
These basic concepts when used in combination, become a very important tool for analyzing millions of brand conversations with human level accuracy. In the post, we take the example of Uber and demonstrate how this works. Read On!
Text Classifier — The basic building blocks
Sentiment Analysis
Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral. You can input a sentence of your choice and gauge the underlying sentiment by playing with the demo here.
Intent Analysis
Intent analysis steps up the game by analyzing the user’s intention behind a message and identifying whether it relates an opinion, news, marketing, complaint, suggestion, appreciation or query.
Analyzing intent of textual data
Contextual Semantic Search(CSS)
Now this is where things get really interesting. To derive actionable insights, it is important to understand what aspect of the brand is a user discussing about. For example: Amazon would want to segregate messages that related to: late deliveries, billing issues, promotion related queries, product reviews etc. On the other hand, Starbucks would want to classify messages based on whether they relate to staff behavior, new coffee flavors, hygiene feedback, online orders, store name and location etc. But how can one do that?
We introduce an intelligent smart search algorithm called Contextual Semantic Search (a.k.a. CSS). The way CSS works is that it takes thousands of messages and a concept (like Price) as input and filters all the messages that closely match with the given concept. The graphic shown below demonstrates how CSS represents a major improvement over existing methods used by the industry.
Existing approach vs Contextual Semantic Search
A conventional approach for filtering all Price related messages is to do a keyword search on Price and other closely related words like (pricing, charge, $, paid). This method however is not very effective as it is almost impossible to think of all the relevant keywords and their variants that represent a particular concept. CSS on the other hand just takes the name of the concept (Price) as input and filters all the contextually similar even where the obvious variants of the concept keyword are not mentioned.
For the curious people, we would like to give a glimpse of how this works. An AI technique is used to convert every word into a specific point in the hyperspace and the distance between these points is used to identify messages where the context is similar to the concept we are exploring. A visualization of how this looks under the hood can be seen below:
Visualizing contextually related Tweets
Time to see CSS in action and how it works on comments related to Uber in the examples below:

Similarly, have a look at this tweet:

In both the cases above, the algorithm classifies these messages as being contextually related to the concept called Price even though the word Price is not mentioned in these messages.
Uber: A deep dive analysis
Uber, the highest valued start-up in the world, has been a pioneer in the sharing economy. Being operational in more than 500 cities worldwide and serving a gigantic user base, Uber gets a lot of feedback, suggestions, and complaints by users. Often, social media is the most preferred medium to register such issues. The huge amount of incoming data makes analyzing, categorizing, and generating insights challenging undertaking.
We analyzed the online conversations happening on digital media about a few product themes: Cancel, Payment, Price, Safety and Service.
For a wide coverage of data sources, we took data from latest comments on Uber’s official Facebook page, Tweets mentioning Uber and latest news articles around Uber. Here’s a distribution of data points across all the channels:
Facebook: 34,173 Comments
Twitter: 21,603 Tweets
News: 4,245 Articles
Analyzing sentiments of user conversations can give you an idea about overall brand perceptions. But, to dig deeper, it is important to further classify the data with the help of Contextual Semantic Search.
We ran the Contextual Semantic Search algorithm on the same dataset, taking the aforementioned categories in account (Cancel, Payment, Price, Safety, and Service).
FACEBOOK
Sentiment Analysis
Breakdown of Sentiment for Categories
Noticeably, comments related to all the categories have a negative sentiment majorly, bar one. The number of positive comments related to Price have outnumbered the negative ones. To dig deeper, we analyzed intent of these comments. Facebook being a social platform, the comments are crowded random content, news shares, marketing and promotional content and spam/junk/unrelated content. Have a look at the intent analysis on the Facebook comments:
Intent analysis of Facebook comments
Intent analysis of Facebook comments
Thus, we removed all such irrelevant intent categories and reproduced the result:
Filtered Sentiment Analysis
There is noticeable change in the sentiment attached to each category. Especially in Price related comments, where the number of positive comments has dropped from 46% to 29%.
This gives us a glimpse of how CSS can generate in-depth insights from digital media. A brand can thus analyze such Tweets and build upon the positive points from them or get feedback from the negative ones.
TWITTER
Sentiment Analysis
A similar analysis was done for crawled Tweets. In the initial analysis Payment and Safety related Tweets had a mixed sentiment.
Category wise sentiment analysis
To understand real user opinions, complaints and suggestions, we have to again filter the the unrelated Tweets(Spam, junk, marketing, news and random):
Filtered sentiment
There is a remarkable reduction in number of positive Payment related Tweets. Also, there is a significant drop in the number of positive Tweets for the category Safety(and related keywords.)
Additionally, Cancel, Payment and Service (and related words) are the most talked about topics in the comments on Twitter. It seems that people talked most about drivers cancelling their ride and the cancellation fee charged to them. Have a look at this Tweet:

Brand like Uber can rely on such insights and act upon the most critical topics. For example, Service related Tweets carried the lowest percentage of positive Tweets and highest percentage of Negative ones. Uber can thus analyze such Tweets and act upon them to improve the service quality.
NEWS
Sentiment Analysis for News headlines
Understandably so, Safety has been the most talked about topic in the news. Interestingly, news sentiment is positive overall and individually in each category as well.
We classified news based on their popularity score as well. The popularity score is attributed to the share count of the article on different social media channels. Here’s a list of top news articles:
Uber C.E.O. to Leave Trump Advisory Council After Criticism
#DeleteUber: Users angry at Trump Muslim ban scrap app
Uber Employees Hate Their Own Corporate Culture, Too
Every time we take an Uber we’re spreading its social poison
Furious customers are deleting the Uber app after drivers went to JFK airport during a protest and strike
The age of getting meaningful insights from social media data has now arrived with the advance in technology. The Uber case study gives you a glimpse of the power of Contextual Semantic Search. It’s time for your organization to move beyond overall sentiment and count based metrics. Companies have been leveraging the power of data lately, but to get the deepest of the information, you have to leverage the power of AI, Deep learning and intelligent classifiers like Contextual Semantic Search and Sentiment Analysis. At Karna, you can contact us to license our technology or get a customized dashboard for generating meaningful insights from digital media. You can check the demo here.
ParallelDots AI APIs, is a Deep Learning powered web service by ParallelDots Inc, that can comprehend a huge amount of unstructured text and visual content to empower your products. You can check out some of our text analysis APIs and reach out to us by filling this form here or write to us at apis@paralleldots.com.
",1440,Machine Learning,Shashank Gupta,https://hackernoon.com/sentiment-analysis-concept-analysis-and-applications-6c56dcb430b,"Sentiment analysis is contextual mining of text which identifies and extracts subjective information in source material, and helping a business to understand the social sentiment of their brand, product or service while monitoring online conversations.
Sentiment Analysis is the most common text classification tool that analyses an incoming message and tells whether the underlying sentiment is positive, negative our neutral.
Intent analysis steps up the game by analyzing the user’s intention behind a message and identifying whether it relates an opinion, news, marketing, complaint, suggestion, appreciation or query.
On the other hand, Starbucks would want to classify messages based on whether they relate to staff behavior, new coffee flavors, hygiene feedback, online orders, store name and location etc.
CSS on the other hand just takes the name of the concept (Price) as input and filters all the contextually similar even where the obvious variants of the concept keyword are not mentioned.
Time to see CSS in action and how it works on comments related to Uber in the examples below:
We analyzed the online conversations happening on digital media about a few product themes: Cancel, Payment, Price, Safety and Service.
For a wide coverage of data sources, we took data from latest comments on Uber’s official Facebook page, Tweets mentioning Uber and latest news articles around Uber.
We ran the Contextual Semantic Search algorithm on the same dataset, taking the aforementioned categories in account (Cancel, Payment, Price, Safety, and Service).
Noticeably, comments related to all the categories have a negative sentiment majorly, bar one.
The number of positive comments related to Price have outnumbered the negative ones.
Have a look at the intent analysis on the Facebook comments:
Intent analysis of Facebook comments
A brand can thus analyze such Tweets and build upon the positive points from them or get feedback from the negative ones.
In the initial analysis Payment and Safety related Tweets had a mixed sentiment.
To understand real user opinions, complaints and suggestions, we have to again filter the the unrelated Tweets(Spam, junk, marketing, news and random):
Also, there is a significant drop in the number of positive Tweets for the category Safety(and related keywords.)
Additionally, Cancel, Payment and Service (and related words) are the most talked about topics in the comments on Twitter.
Uber can thus analyze such Tweets and act upon them to improve the service quality.
Companies have been leveraging the power of data lately, but to get the deepest of the information, you have to leverage the power of AI, Deep learning and intelligent classifiers like Contextual Semantic Search and Sentiment Analysis."
Meaning Matching without Parts of Speech,"Real NLU matches the meaning of a word in a sentence, based on the meanings of the other words in the sentence. Today we review how NLU is…","Meaning Matching without Parts of Speech
How to do NLU without POS
Real NLU matches the meaning of a word in a sentence, based on the meanings of the other words in the sentence. Today we review how NLU is progressing, using our experience in implementing a model based on linguistics, the science of language.
As usual, you can see the companion video with demonstrations on our YoutTube page: https://youtu.be/a14n1b9bd9I.
I view NLP as middleware: a discrete system, that interacts with sequences of symbols. If meaningful, NLU understands them in context and similarly, NLG responds with symbol sequences. The meaning of the response takes advantage of the known context, but in general the choice of response greatly exceeds the focus of language.
Why middleware and not a full solution?
First, the full solution is AI. D’uh. Turing demonstrated human intelligence with a simple wrong answer. The full solution would be more like the HAL9000, passing aspects of the Turing test, as it also plays chess .
And second, language is the system that communicates, learning all the time, even using previously unknown words. As an aside, most machine learning systems today violate this approach by loading first, and then operating on that static data.
Lastly, when you talk to an expert — say a doctor, engineer or linguist — you may not be able to follow them because of vocabulary and a deeper knowledge of the predicates they use, but you still speak your own language. A joke here illustrates: Mechanic says: “You have a failing compressor solenoid.” English speaker: “I don’t understand! OMG! I don’t speak English!”
The Dictionary Goal
The dictionary is a set of network associations to enable automatic recognition. When WordNet was created by the legendary psychologist and cognitive scientist, George Miller, the path to NLU was hinted at[i]. Miller, his wife and three others[ii] set out in 1985 to create an “automated dictionary” that was expected to provide educational benefits, but instead it became a resource for NLP that, probably due to losing battles against computational linguistics, is no longer being developed[iii].
Where Miller’s model hints at the future is with its network of associations. One level has the English words and phrases that make up headwords. They connect to a set of meanings (word-senses), semantic-level elements, that in turn connect to a single part of speech (that we factor out of our model completely). We will come back to the model when we have more time, but the key point is that the word sense is associated with the part of speech, not the word itself.
Where we are going with the dictionary is first to remove the parts of speech that duplicate the definitions, which also duplicate and confuse phrase patterns due to overlap (lost information reduces the clarity of phrases).
Start with table 1, representing forms of ‘destroying’.
Table 1. Samples of the predicate ‘destroy’ definition. Containing phrase underlined.
Today, the meanings are 4 separate dictionary definitions. For NLU we convert those to a single definition and associate a set of elements to control phrase matching correctly.
Table 2. Target definition of the predicate ‘destroy’. Only category is predicate (semantic terms meaning it relates arguments)
Here in table 2, there is one definition for destroy, which is a predicate (a semantic category that relates referents as arguments, in contrast to a referent which categorizes to a ‘kind of’). The definition of ‘destroy’ doesn’t need a gloss to describe it; it can generate a definition in a target language if needed based on its semantic associations. The relations that define the predicate will be covered in detail another time as will the attributes that are used in the phrase patterns to complete the meaning matching.
Our goal is to take the various parts of speech for single definitions and collapse them into a single semantically based one. Keep it simple!
Patom theory
Our NLU begins with Patom theory: a brain model in which the smallest representation can only store, match and use hierarchical, bidirectional linkset patterns. It took a long time to come up with that conclusion, in fact most of the 1990s. It is based on the observations of a variety of cognitive scientists, including neuroscientists and psychologists.
Patom theory looks primarily at what the brain does, not how it does it.
Linguistics application
If a brain works the way Patom theory explains, brain emulation should cater to element decomposition with representation as either sets or lists (as we have done in the meaning matcher).
For example, the word ‘cats’ is decomposed as the meanings of cat (referents, including a kind of animal), plus the meaning ‘plural’, plus the meaning ‘3rd person’. ‘Cats’ as a word is therefore like a set.
The phrase ‘the cats’ is composed of two pieces — the operator[iv] first indicating this is a new, accessible element for context and then the referent (we saw last time that there is a different pattern to recognize “the travelling” where the second word is a predicate). Phrases are lists which we will show in detail for acquisition and use over the next couple of articles.
Note that the meaning of a referent like this isn’t some fixed token like NP, but it retains the meanings of the word “cats” after intersection. So “the cats” is replaced in the matcher by the meanings of “cats”.
In modern mainstream linguistics, the problem of decomposition is evident in many places. The symbol NP for example loses the clarity of whether it is a predicate (e.g. the running) or a referent (the cat). It also loses the singular/plural feature (e.g. the cat, the cats).
Designs like the Penn Treebank use arbitrary combinations for English, like VB (verb base form), VBD (verb past tense), VBZ (verb first person singular present)[v]. For nouns there are a number like NN (noun singular or mass), NNS (Noun plural), NNPS (Proper noun plural) and PRP (personal pronoun).
The Slot Grammar[vi] takes a more decompositional approach but retains parts-of-speech including noun, verb and adj; and then extends them with features like vinf (infinitive), vpast (past tense) and vpers3 (third person). Rather than use the meaning of words, slot grammar adopts (for English) something like 90 ‘basic semantic types’. These types include artf (artefact), cognsa (cognitive state or activity), and cty (named city).
The alternative is to retain the sets of information. If you used noun as an element (we don’t) you could track the set of active elements to avoid language by language redesigns.
As meanings of words are sets (a word often has multiple meanings), intersection is the set concept in which only elements that are in both sets are retained. So we have the set of elements in the phrase and the set of elements in the word to intersect, effectively starting the process of Word Sense Disambiguation (WSD).
WSD allows ambiguous words to be resolved based on syntactic properties (phrase patterns) or predicate properties (semantic patterns).
Example of Word Sense Disambiguation
Let’s look at an example of linkset intersection in action with some simple sentences because a picture is worth a thousand words. “The water ran” can be compared with “the man ran”.
The meaning of ‘ran’ is synonymous with ‘flow’ in the first case and with ‘move quickly using legs’ in the second case.

Here, the meaning of ran is seen — ‘p:flow2’ (with its gloss, “move along, of liquids”). Intersection compares the actor (water) for this activity with its predicate (ran) as predicates determine their arguments. The concept has also been known as “selectional restrictions”, but without the flexibility we have here.

Here, the intersection of ‘r:man’ with ‘ran’ leaves the motion predicate p:run36. In more complex sentences, meaning is retained with multiple predicates possible when their arguments also overlap. All senses are retained and, as usual, can be resolved in context or questioned.
In this next sentence, we add an embedded proposition (the man the dog saw) and tense and aspect and some ‘how’ types: “the man the dog saw has been running continuously evidently”.
Note that in a more complex sentence, the meaning is retained, unchanged from the simpler one.
There can be more than one valid interpretation, and so context is needed for the next level of resolution.
In context, the ultimate intersection tool is available — the clarifying question. Think of Mr. Fantastic, Reed Richards, from the Fantastic 5 (the elastic man). Now understand “Mr. Fantastic ran”. Does that mean he melted or he moved fast on his legs? As both possibilities can be recognized in the sentence, we can resolve the meaning from context, or validate with something like: “You mean, he melted?” Once context is unambiguous, all parties to the conversation have an unambiguous representation to build on the definitions of those words (learning).
The Cup is Shattered
In figure 1, the predicate ‘shattered’ shows a variety of semantic representations in different sentences. While the first form and the third is different (the unspecified causer in the achievement case), the underlying meaning is retained: the cup is broken in many pieces. Whether that difference is relevant is a question for context.
Figure 1. Is shattered an adjective or a verb?
Summary
In summary, NLU is based on meanings, and the best human source of word meanings is a dictionary. Storing definitions as associations (the relations between the predicate and its allowable arguments like actor and undergoer) allows automatic recognition based on them.
Real NLU retains the meanings of the words as sets — in line with Patom theory. This allows us to create a dictionary that is automatically disambiguated based on the sentence in context.
(next — we continue this exploration with how Patom theory enables language acquisition without ‘processing’ to achieve the end result: an unambiguous dictionary based on meaning sufficient to pass the Facebook AI Research conversational tests and beyond.)
[i] John Ball, Speaking Artificial Intelligence, Chapter 6, 2016.
[ii] Fellbaum (ed.), WordNet: An Electronic Lexical Database, The MIT Press, 1998, P xvii.
[iii] John Ball, Miller’s WordNet gives A.I. knowledge, 2nd from last paragraph, 2015, https://www.computerworld.com/article/2935578/emerging-technology/miller-s-wordnet-paves-the-way-for-a-i.html
[iv] In RRG, an operator is an element of meaning that affects the sentence but isn’t a constituent of any element. Tense, illocutionary force and aspect elements are examples of operators which simplify the sentence pattern to allow easier disambiguation.
[v] Beatrice Santorini, Part-of-Speech Tagging Guidelines for the Penn Treebank Project, June 1990.
[vi] Michael C. McCord, IBM Research Report: Using Slot Grammar, March 24, 2010, P 12–14.
",1703,Machine Learning,John Ball,https://medium.com/s/story/meaning-matching-without-parts-of-speech-ead3ba9e526f,"Today we review how NLU is progressing, using our experience in implementing a model based on linguistics, the science of language.
They connect to a set of meanings (word-senses), semantic-level elements, that in turn connect to a single part of speech (that we factor out of our model completely).
For NLU we convert those to a single definition and associate a set of elements to control phrase matching correctly.
Only category is predicate (semantic terms meaning it relates arguments)
The relations that define the predicate will be covered in detail another time as will the attributes that are used in the phrase patterns to complete the meaning matching.
Our goal is to take the various parts of speech for single definitions and collapse them into a single semantically based one.
Our NLU begins with Patom theory: a brain model in which the smallest representation can only store, match and use hierarchical, bidirectional linkset patterns.
If a brain works the way Patom theory explains, brain emulation should cater to element decomposition with representation as either sets or lists (as we have done in the meaning matcher).
The phrase ‘the cats’ is composed of two pieces — the operator[iv] first indicating this is a new, accessible element for context and then the referent (we saw last time that there is a different pattern to recognize “the travelling” where the second word is a predicate).
Note that the meaning of a referent like this isn’t some fixed token like NP, but it retains the meanings of the word “cats” after intersection.
The Slot Grammar[vi] takes a more decompositional approach but retains parts-of-speech including noun, verb and adj; and then extends them with features like vinf (infinitive), vpast (past tense) and vpers3 (third person).
Rather than use the meaning of words, slot grammar adopts (for English) something like 90 ‘basic semantic types’.
WSD allows ambiguous words to be resolved based on syntactic properties (phrase patterns) or predicate properties (semantic patterns).
In more complex sentences, meaning is retained with multiple predicates possible when their arguments also overlap.
As both possibilities can be recognized in the sentence, we can resolve the meaning from context, or validate with something like: “You mean, he melted?” Once context is unambiguous, all parties to the conversation have an unambiguous representation to build on the definitions of those words (learning).
In figure 1, the predicate ‘shattered’ shows a variety of semantic representations in different sentences.
In summary, NLU is based on meanings, and the best human source of word meanings is a dictionary.
Storing definitions as associations (the relations between the predicate and its allowable arguments like actor and undergoer) allows automatic recognition based on them.
Real NLU retains the meanings of the words as sets — in line with Patom theory.
This allows us to create a dictionary that is automatically disambiguated based on the sentence in context.
(next — we continue this exploration with how Patom theory enables language acquisition without ‘processing’ to achieve the end result: an unambiguous dictionary based on meaning sufficient to pass the Facebook AI Research conversational tests and beyond.)"
Why We’ll Never Be Able To Use Siri Like Dwayne Johnson,This article was originally posted on Forbes.com,"Why We’ll Never Be Able To Use Siri Like Dwayne Johnson
This article was originally posted on Forbes.com
Shutterstock
There are a couple of things we may never be able to do like Dwayne Johnson. Rocking a black turtleneck, for instance. Or using Siri to order a Lyft for you in under 60 seconds.
You might check your phone in the middle of a conversation. There’s a physical person sitting across from you, but you get that itch to link up with the digital world. The result: You unintentionally ignore every word they’re saying.
So how can you stop this from happening? There has to be a way (other than just leaving your phone in your pocket), right?
As it turns out, there is’t. We weren’t programmed to accommodate for Siri.
The Rise Of Siri
The divide between the digital and physical worlds is becoming more evident. Apple first introduced Siri in 2011 with the iPhone 4, advertising it as a personal assistant right in the palm of your hands. The greatest attraction was the interaction between the user and artificial intelligence.
Siri wasn’t intended as a digital assistant whose purpose was to obey the orders put forth by the users. Siri was a step forward in the world of artificial intelligence (AI), creating a technology that could understand and respond to human needs the way other humans do. It wasn’t another search engine, it was a “do” engine.
Too tired to open your Lyft App? Just ask, “Hey Siri, can you get me a ride home?” and she’ll do the rest. Maybe ask her about the meaning of life on the way home. She’ll give you some thoughtful answers.
Artificial intelligence is continuously progressing to blur the lines between the physical and digital world. But you’ll soon find a problem: We can’t put our full attention on both the physical and digital worlds at the same time. There’s a reason for that.
Humans Vs. AI
When it comes to concentration, your visual and mental capacity are working together. Your brain is telling you what your eyes process. But do you actually see everything that’s in your field of vision? Do you perceive the world to be exactly what it is?
The simple answer is no. In a chaotic, restless world, where our senses are bombarded by countless stimuli each second, the brain has evolved defenses and workarounds; it filters out what it deems to be white noise and based on past experience and generates predictions for what to expect. Your brain decides the amount of data it can process without overloading and ignores what doesn’t fit within that capacity.
In one experiment, researchers had subjects switch their eyes between two flashing squares on a computer screen while hooked up to a fMRI machine to gauge brain activity. Scientists expected to find a delay in the fMRIs, given that our eyes move more quickly than our minds do. Instead, they found a seamless process, as the brain compensated for its slower speed by predicting the position of the next flashing square. There was no delay between eye movements and brain activity, which shows the power of our predictive minds.
But our brains aren’t perfect. Say you send a quick text while at a red light. You put your phone down once the light changes to green. Your brain may still be processing the text instead of the road that’s ahead of you, which creates blind spots and slows reaction time.
This is also known as inattentional blindness.
Multitasking: A Myth?
Inattentional blindness proves the impossibility of multitasking. In reality, the brain can only concentrate on one thing at a time — anyone who claims to be good at multitasking isn’t. Our brains just switch between tasks very quickly, giving the illusion of multitasking.
This is actually a good thing. Our brains developed this way so that we could retain as much information as possible. When you’re multitasking, your brain ends up ignoring some of the information in your surroundings so it can try to filter what’s most important.
But too much stimulation can mean that we can’t focus on anything. With the emergence of technology in our everyday lives, this can increase the occurrence of inattentional blindness. Although many may view their physical and digital lives as one, there is always an increased focus on one versus the other.
The Future Of Human-AI Interactions
Programs like Siri and Google Assistant highlight the limitations of the human-AI interface. For example, Siri can read that text aloud for you since your hands are occupied with the delicate timing of flipping a pancake. But while you’re focused on the stove, you’ve completely missed what the text even said. Siri can note down that you owe Emily $12 for lunch while you’re crossing the street. But that’s only after you’ve realized that you were supposed to take a left instead of crossing.
Though Siri is often used to streamline things for frantic, stressed out humans, she is held back by the limited abilities of our brains — specifically our inability to multitask. This means that as AI capabilities grow by leaps and bounds, they could potentially displace more and more humans. For instance, an AI air traffic controller could efficiently juggle hundreds of flights in the air at any given time, assign out runways to arrivals and departures and minimize the cost of operations (one algorithm versus an entire team of controllers) as well as reduce inefficiencies (like time wasted on the runway).
To avoid mass layoffs (and the ensuing economic instability), AIs must work as partners, not replacements. This is already happening. Even as investment firms use AIs to trade thousands of stocks each second, humans are still dominant; while algorithms recognize patterns and mine data, humans set the parameters, come to conclusions and make the final decisions. AI may burn through reams of data and excel at multitasking, but they cannot pull off higher-level creative work as well as humans.
So while our brains can’t quite keep up with AI in processing, we are unlikely to go the way of the dodo. At least until someone perfects artificial creativity.
",1029,Artificial Intelligence,Ed Sappin,https://medium.com/s/story/why-well-never-be-able-to-use-siri-like-dwayne-johnson-62949428e816,"Why We’ll Never Be Able To Use Siri Like Dwayne Johnson
There’s a physical person sitting across from you, but you get that itch to link up with the digital world.
Siri wasn’t intended as a digital assistant whose purpose was to obey the orders put forth by the users.
Siri was a step forward in the world of artificial intelligence (AI), creating a technology that could understand and respond to human needs the way other humans do.
Artificial intelligence is continuously progressing to blur the lines between the physical and digital world.
But you’ll soon find a problem: We can’t put our full attention on both the physical and digital worlds at the same time.
Your brain decides the amount of data it can process without overloading and ignores what doesn’t fit within that capacity.
Your brain may still be processing the text instead of the road that’s ahead of you, which creates blind spots and slows reaction time.
In reality, the brain can only concentrate on one thing at a time — anyone who claims to be good at multitasking isn’t.
When you’re multitasking, your brain ends up ignoring some of the information in your surroundings so it can try to filter what’s most important.
Programs like Siri and Google Assistant highlight the limitations of the human-AI interface.
Though Siri is often used to streamline things for frantic, stressed out humans, she is held back by the limited abilities of our brains — specifically our inability to multitask.
So while our brains can’t quite keep up with AI in processing, we are unlikely to go the way of the dodo."
Why the future of customer service is in your face,Originally posted at Orange Silicon Valley.,"Why the future of customer service is in your face
Image Credit: Aamon — stock.adobe.com
Originally posted at Orange Silicon Valley.
Chatbots may be fast when it comes to delivering dynamic feedback to lots of users. The best customer service offers an empathetic ear, however, and moreover, it gives the customer signals that concerns are being taken seriously. The secret to getting chatbots to the point where they can fulfill that calling might be right underneath our noses.
“Face is the new interface,” said Mark Walsh, the CEO of Motional.AI. He boldly stated this as if to say that such text and voice bots were already mainstream. He spoke at Humanity.AI — an AI and chatbot conference in San Francisco run by Capital One. The conference’s theme was “People and Bots” and the overarching message was that bots designed with humans in mind can make us smarter, more informed, and more efficient.
Walsh’s point was that face has the potential to be the new user interface because — for human beings — it was the original interface. First, face is better because 55% of communication is visual. The earliest cavemen were entertained — they laughed, learned, and connected by telling stories around fires. “Face time” is important because we check to make sure people’s words match their language as we ask questions such as “Are they being sincere? Are they telling the truth? Can I trust them?” The need to connect and verify that facial expressions match words explains why we continue to have retail-based customer service — e.g. where a segment of the population still walks into their bank branch to financially transact with another human being.
As Walsh’s talk continued, he showed us a video of an animated, healthcare bot interacting with his retired mother via tablet. The bot’s avatar was Pixar-like, enthusiastic, and ready to help — it happily sent a note off to Mom’s doctor asking for a follow up. The bot was non-threatening and almost human even in cartoon form, offering a fun experience to take in that made the audience visibly warm.
But as consumers, what we really want is the benefits of bots — the always-on, the instant gratification — paired with the human connection — the trust, the ability of people to read between the lines, and the scratching of our itch — providing answers and solving problems in a cooperative relationship that lasts.
Walsh’s idea for Motional.AI is to humanize bots with empathy and character a.k.a. “Character Intelligence(TM) Artificial Intelligence” to lay the groundwork for a trusting relationship between consumers and brands. Diving a little deeper, he wants to bridge the uncanny valley, i.e. that intuitive, not-quite-right feeling you get when you look at a robot failing to impersonate a human. In the short term, his goal is to offer brands and their audiences the best face-based bot experience.
Foundations for Better Service
Over the past year, I have observed startups, trends, and research that support Walsh’s vision — seeds of technology that could sprout, grow, and bloom into new face-based customer service experiences. Below are some of the examples that I’ve seen.
Buddy, retirees’ best friend
Truth be told, I have seen Walsh’s demo before in a different context. One of Orange Fab’s startups is a company called GeriJoy, which offers 24/7 geriatric care through a team of remote human caregivers. A canine avatar pops up on a tablet, reminds seniors to take their medicine and exercise, decreases their loneliness through daily interaction, and allows working children to know how their parents are doing.

Two noticeable differences strike me. First, GeriJoy uses an animated dog to deliver care vs. Motional.AI’s human. GeriJoy explains that the avatar offers consistency and having an animal avatar offers some benefits related to pet therapy. Second, Walsh’s demo was pre-recorded, so the depth and scope of the AI or human + AI is unclear, while the GeriJoy dog avatar is explicitly controlled live by trained human beings.
Emotion detection
Companies like Microsoft, Kairos and Affectiva are staking claims in the world of emotion detection. These technologies read faces to detect emotions, while a recent article by MIT highlighted early research into detecting emotions using wireless signals based on your heartbeat.
Today, banks such as JPMorgan and Wells Fargo are already using voice detection to fight fraud. Emotion detection will initially be used to measure ad effectiveness and to optimize ads and content. This same emotion detection technology will be able to help customer service agents better manage the their customers’ expectations. Human agents will be able to monitor emotions and mood and make offers to increase satisfaction or cross sell additional products at opportune times.
Optimizing VR-based learning
I first learned of Jeremy Bailenson’s lab at Stanford when Facebook CEO Mark Zuckerberg visited to better understand the potential for social virtual worldsbefore acquiring Oculus in 2014.
Bailenson runs Stanford University’s Virtual Human Interaction Lab with the stated goals of understanding the nuance of face-to-face interaction and how to use VR to improve everyday life including conservation, empathy, and communication. In one of his projects, social researchers are currently running experiments on virtual classrooms to optimize for learning outcomes. There is an opportunity here to take these learnings from VR in the education sector, fund additional research for mobile interaction, and re-apply them to customer service.
Customer-Driven Customer Service
What happens when we bring it all together? Blending Motional.AI’s Character Intelligence with GeriJoy’s avatar-based geriatric care (driven by a human team), emotion detection, and research on how to optimize customer experience could make our lives better. Here’s how.
The HealthTime app
Imagine that you had a questions that you wanted to ask your doctor, so you launch the HealthTime* app and the image of a human agent pops up on your phone asking, “How can I help?” The service always loads instantly, and no matter whether your question is about your teeth, appointment, diet, exercise plan, vision, bill, current symptoms, or latest lab results, you never have to re-explain your issue or question. The agent is consistent — it’s the same human avatar every time — and he/she is non-threatening and even fun to chat with. For you, it’s seamless. HealthTime never feels like you have been transferred to a different human agent (even if that happens in the background). The avatar is never rude or condescending because the AI literally knows what that experience looks like and can intervene before such behavior appears.
HealthTime may take a minute to research or come back to you with more specific questions, but if the service can’t find the answer shortly, then it calls you back at a convenient time. In fact, the service can tell how you are feeling right now, whether you want to be chatty, cheered up, or you just want to cut to the chase; it senses when you start to get irritated and compensates; and it intentionally ends on a high note delivering a shot of satisfying dopamine right at the end of the call.
HealthTime is a one-stop shop for your health, and the system knows all your medical history. It’s where Kaiser Permanente meets Doctor on Demand on your mobile phone. It’s beautiful because the onus of the experience and communication is on the system and the providers instead of customers. The system offloads the overhead that we face today with managing our healthcare and the associated bills, and free us up to take action.
Now, re-imagine any customer-oriented business this way. Imagine DollarTime for banking, loans, and investment. What about MobileTime, CableTime, and UtilityTime? How can we make customer service proactive instead of reactive? How can we make customer service customer-centric — not managed and driven by the customer? With HealthTime, you manage your health without managing your healthcare.
*Note HealthTime is purely fictional and bears no reference to any real application.
Open Questions
This new system brings up several ethical questions — what’s the top level metric for companies? Should they be optimizing for satisfaction, retention, revenue, or something else? How can enterprises leverage the cumulative rich customer data set to build not only customer personas, but bot personas that optimize customer service based on company goals?
From the customer’s perspective, how personalized should the customer service bot be? If you are a 25-year old Latino man who speaks Spanish and English, then what age, sex, race, and accent should the bot have? Beyond demographics, if you are an impatient extravert who calls up once a month because you might have a basic question, but you are lonely and just want someone to chat or entertain you, then what personality should the bot adopt? Alternatively, if you are a low-margin/high-maintenance customer, should the bot intentionally treat you worse to nudge you towards canceling your service?
Contact me if you are interested in talking more over coffee in San Francisco. I will also be attending Habit Summit 2017 in April and am excited to see Jane McGonigal talk about gamifying life and Buster Benson discuss behavior change.
Disclaimer: The views and opinions expressed in this article belong to the author and do not necessarily reflect the position or views of Orange or Orange Silicon Valley.




",1554,Artificial Intelligence,David Martin,https://chatbotslife.com/why-the-future-of-customer-service-is-in-your-face-33fd2addfe81,"Walsh’s point was that face has the potential to be the new user interface because — for human beings — it was the original interface.
Can I trust them?” The need to connect and verify that facial expressions match words explains why we continue to have retail-based customer service — e.g. where a segment of the population still walks into their bank branch to financially transact with another human being.
The bot was non-threatening and almost human even in cartoon form, offering a fun experience to take in that made the audience visibly warm.
But as consumers, what we really want is the benefits of bots — the always-on, the instant gratification — paired with the human connection — the trust, the ability of people to read between the lines, and the scratching of our itch — providing answers and solving problems in a cooperative relationship that lasts.
In the short term, his goal is to offer brands and their audiences the best face-based bot experience.
Over the past year, I have observed startups, trends, and research that support Walsh’s vision — seeds of technology that could sprout, grow, and bloom into new face-based customer service experiences.
One of Orange Fab’s startups is a company called GeriJoy, which offers 24/7 geriatric care through a team of remote human caregivers.
This same emotion detection technology will be able to help customer service agents better manage the their customers’ expectations.
There is an opportunity here to take these learnings from VR in the education sector, fund additional research for mobile interaction, and re-apply them to customer service.
Blending Motional.AI’s Character Intelligence with GeriJoy’s avatar-based geriatric care (driven by a human team), emotion detection, and research on how to optimize customer experience could make our lives better.
Imagine that you had a questions that you wanted to ask your doctor, so you launch the HealthTime* app and the image of a human agent pops up on your phone asking, “How can I help?” The service always loads instantly, and no matter whether your question is about your teeth, appointment, diet, exercise plan, vision, bill, current symptoms, or latest lab results, you never have to re-explain your issue or question.
The agent is consistent — it’s the same human avatar every time — and he/she is non-threatening and even fun to chat with.
HealthTime never feels like you have been transferred to a different human agent (even if that happens in the background).
HealthTime may take a minute to research or come back to you with more specific questions, but if the service can’t find the answer shortly, then it calls you back at a convenient time.
How can enterprises leverage the cumulative rich customer data set to build not only customer personas, but bot personas that optimize customer service based on company goals?
From the customer’s perspective, how personalized should the customer service bot be?"
Announcing Aqueduct: Using Machine Learning to Strengthen Water Infrastructure,Fresh water is our most precious resource on Planet Earth. This is an indisputable fact; life cannot exist without it. A society without a…,"Announcing Aqueduct: Using Machine Learning to Strengthen Water Infrastructure

Fresh water is our most precious resource on Planet Earth. This is an indisputable fact; life cannot exist without it. A society without a steady stream of clean and accessible freshwater is no society at all — it is an apocalyptic wasteland.
Considering the gravity of importance of fresh water, it is wildly unnerving that our country is so bad at managing it. Public water systems regularly lose between 10% and 30% of treated water to non-revenue leaks — that is, clean water uselessly leaking out pipelines due to breaks and leaks. Data suggests that nationwide, utilities lose more water to these leaks than all of the water used by domestic households in the United States, as much as two trillion gallons a year. Non-revenue water is a loss for everyone —lower revenue for the water utilities, less water for society.¹
This is an absurdity, especially in the context of the painful droughts and water rationing in the Western States. There are farmers in central California advocating to drain rivers to extract more water (subsequently killing off sensitive wildlife)², and yet a significant percentage of our water is simply exiting out of the system.
Already, this trend should sound the alarm. But it gets worse — a lot worse. First, global warming is here, it is real, and it is affecting our society in debilitating ways. Because global warming has a dramatic effect on weather patterns, we can expect to see more severe droughts in arid areas over the coming years, leading to water scarcity and desertification.³
Municipalities and metropolises all over the world are already struggling with the basic question of whether they have enough water for their citizens. In many places, water scarcity is already here; see Cape Town’s Day Zero⁴; see the Syrian Civil War, where water scarcity played a direct roll in plunging the country into chaos⁵; see water shortages in cities like Mexico City, Cairo, Sao Paulo, and of all places — London.⁶
And if the volatility from global warming wasn’t enough, back at home the United States is dealing with a massive water infrastructure crisis, where water assets all over the country are nearing the end of their lifetime. Our country will need to invest at least $1 trillion over the coming years to upgrade legacy systems and invest in new plants to meet population growth.⁷
And if you’re as skeptical of our Federal Government as I am, you probably agree that there is neither the political will nor the foresight to invest that enormous sum of money in our future, however necessary it might be. Absent of some kind of 21st Century FDR-style New Deal program, we may be out luck.
And so, as a nation we face a horrific equation where older infrastructure continues to deteriorate, while getting increasingly strained by volatile weather patterns and resource demand. It’s not a fun thing to think about.
And yet, perhaps this infrastructure crisis is a call to action. Perhaps this is time to innovate, to do more with less, and engineer our way out of a disaster.
Israel’s Water Miracle
My co-founder Ethan Durham and I stumbled upon the water problem recently, and realized that there was something we could do about it. Ever since I moved to Silicon Valley in 2015, I’ve been frustrated by the in-group nature of the technology community, and the lack of collaboration with more old school industries or organizations outside of tech. For every social impact startup, it seems, there are ten photo-sharing apps, wannabee social media platforms, or enterprise software add-ons. Sure, there is room for those companies too, but it seems like Silicon Valley is obsessed over creating the next Facebook, producing a myopia that blinds it to the problems in the world that we actually need to solve.
California is the world’s leader in tech, and yet we still haven’t figured out how to solve our basic water issues in our own state. This is particularly frustrating for me — I studied abroad in Israel in 2011, a country historically plagued by water issues. Israel’s drought got so bad at one point that that Sea of Galilee’s water levels dropped to historically low levels⁸ and local celebrities appeared on television to plead people to use less water. Then, within the span of a few years, Israel used desalinization technology* to entirely eliminate water scarcity. The small nation now has more than enough water for its population, industry, and agriculture.⁹
Even before it solved its water issues, Israel has consistently pioneered solutions that reduced agricultural use of water (drip irrigation), water recycling, better leak detection and more.
If Israel can solve its water issues, why can’t California?
My theory is that we can — we have more than enough engineers to go around, but there haven’t been enough entrepreneurs to step up and make it a reality. Time to change that.
Enter Aqueduct
Last month, Ethan and I launched Aqueduct. Aqueduct is a machine learning startup using predictive analytics to help water utilities operate more efficiently and save resources.
Based on our research, we realized that while water utilities around the country faced budgetary crises and volatile weather patterns, they weren’t taking advantage of the one resource that they had plenty of: data. It turns out that the majority of water utilities are collecting enormous amounts of data about the operations of their facility, and simply letting it sit. No analysis, no mining, no predictive algorithms, no nothing.
Herein lies the opportunity: water infrastructure is leaking, eroding, and perhaps even rusting away, and data can tell us where that is happening in a plant. Machine learning can pinpoint that likelihood that a water main is going to break within the next year, or whether the water pressure is too high in a pipe, or whether a holding reservoir is being operated correctly. Advanced algorithms can identify anomalies, predict future outcomes, determine optimal plant settings and far more.
These are the kind of insights that can drastically improve plant performance and save money. After all, asset breakdown is an incredibly expensive incident, and facilities can spend millions of dollars fixing systems that break down without notice. Predictive analytics can reduce or eliminate those kinds of surprises. Furthermore, data insights can defer investments that don’t need to happen — in other words, “If it ain’t broke, don’t fix it.”
There are 50,000 water utilities in the United States, a largely fragmented market compared to more centralized municipal water providers in Europe (the volume is misleading — the distribution skews towards the long tail, as there are thousands of water providers in the United States serving less than twenty five households).¹⁰ Only a slim minority of these utilities have implemented any kind of analytics solutions, even as many sit on large amounts of data. We are working to provide a seamless solution for utilities that benefits their efficiency without interrupting their ability to operate.
To better understand the problem and the market opportunity that we are working on, I’d recommend that you start with this article.
At the core, Aqueduct is about providing intelligence to water utilities that allow them to do more with less, and improving infrastructure resiliency in an increasingly volatile environment. Aqueduct is about strengthening the physical foundation of communities, and creating a better world.
To be clear, the water problem is a gargantuan issue. It will be solved by an ecosystem of private-public partnerships, smart regulators, entrepreneurs, policy advocates, environmental activists, and more. There is only so much that any one organization can do. But we truly believe that we can make a difference, and we are taking the leap forward.
The Road Ahead
We launched the company over the last month and half. Ethan designed a beautiful website to reflect our company mission, and I wrote a ten page white paper highlighting the problem (which you can download on the site if you’d like to learn more about this topic).
In June, we started working on our first pilot with Fairfield-Suisan Sewer District, a wastewater treatment facility in the North Bay. Though our initial scope was to focus on distribution of clean water, we’ve expanded our scope to work on any large scale water infrastructure facility, as there are core similarities in function and purpose. We are helping this Fairfield-Suisan analyze their historical data of large pumps and wet wells, as well as reduce their energy consumption (did you know? Water infrastructure accounts for 3% of the electricity in the United States and 35% of municipal consumption¹¹).
This is not going to be an easy journey — we’re fully aware of the technology, regulatory, and sales challenges that lie ahead. But we’re excited like never before, because this is something that the world needs, and we look forward to making a difference in communities in the United States.
If you have any thoughts, feedback, or wisdom, I would love to hear from you. Furthermore, if you know someone in water tech who might be interested in what we are doing, please don’t hesitate to connect them to me.
Send me an email at Jeff@Aqueduct.ai.
*There is no shortage of controversy around desalinization technology, a process that turns salt water into fresh water. Opponents charge that it is energy intensive and expensive. I understand the opposition but I believe we’ve arrived at a point where we are going to have to include desalinization as a method of water sourcing. The consequences of water scarcity — both socially and environmentally — are enormous. It’s better to have a high energy bill to pull water out of the ocean than to dry up our wells and rivers.
Citations:
https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=1173&context=mae_facpub
https://www.sacbee.com/news/state/california/water-and-drought/article162696018.html
https://www.climatehotmap.org/global-warming-effects/drought.html
https://www.cnn.com/2018/03/09/africa/cape-town-day-zero-crisis-intl/index.html
https://journals.ametsoc.org/doi/10.1175/WCAS-D-13-00059.1
https://www.bbc.com/news/world-42982959
https://www.infrastructurereportcard.org/cat-item/drinking-water/
http://www.israeltoday.co.il/default.aspx?tabid=178&nid=22403
https://www.scientificamerican.com/article/israel-proves-the-desalination-era-is-here/
https://www.smartresilient.com/using-machine-learning-assess-infrastructure-replacement-needs
https://www.mge.com/saving-energy/business/bea/article_detail.htm?nid=2431
",1669,Water,Jeff Pawlak,https://medium.com/s/story/announcing-aqueduct-using-machine-learning-to-strengthen-water-infrastructure-d2245cf9deaf,"Announcing Aqueduct: Using Machine Learning to Strengthen Water Infrastructure
Data suggests that nationwide, utilities lose more water to these leaks than all of the water used by domestic households in the United States, as much as two trillion gallons a year.
Because global warming has a dramatic effect on weather patterns, we can expect to see more severe droughts in arid areas over the coming years, leading to water scarcity and desertification.³
And if the volatility from global warming wasn’t enough, back at home the United States is dealing with a massive water infrastructure crisis, where water assets all over the country are nearing the end of their lifetime.
California is the world’s leader in tech, and yet we still haven’t figured out how to solve our basic water issues in our own state.
Then, within the span of a few years, Israel used desalinization technology* to entirely eliminate water scarcity.
Even before it solved its water issues, Israel has consistently pioneered solutions that reduced agricultural use of water (drip irrigation), water recycling, better leak detection and more.
Aqueduct is a machine learning startup using predictive analytics to help water utilities operate more efficiently and save resources.
Based on our research, we realized that while water utilities around the country faced budgetary crises and volatile weather patterns, they weren’t taking advantage of the one resource that they had plenty of: data.
It turns out that the majority of water utilities are collecting enormous amounts of data about the operations of their facility, and simply letting it sit.
At the core, Aqueduct is about providing intelligence to water utilities that allow them to do more with less, and improving infrastructure resiliency in an increasingly volatile environment.
Water infrastructure accounts for 3% of the electricity in the United States and 35% of municipal consumption¹¹).
https://www.sacbee.com/news/state/california/water-and-drought/article162696018.html
https://www.climatehotmap.org/global-warming-effects/drought.html
https://www.bbc.com/news/world-42982959
https://www.scientificamerican.com/article/israel-proves-the-desalination-era-is-here/
https://www.smartresilient.com/using-machine-learning-assess-infrastructure-replacement-needs"
AI as Augmented Intelligence,In the upcoming years AI will primarily serve as “augmented intelligence” to assist us in our everyday lives and help us do our jobs…,"AI as Augmented Intelligence
In the upcoming years AI will primarily serve as “augmented intelligence” to assist us in our everyday lives and help us do our jobs better.
AI has traditionally referred to “artificial intelligence” coming from the imagination of science fiction: omnipotent robots and sentient intelligences in the cloud, changing the world drastically but also stoking our fears of losing control (and our jobs) to our own creations.
AI as Augmented Intelligence
But this is not the reality of what AI is actually doing for us, nor what AI will bring in the coming decade.
Actually AI is going to be much more “Augmented Intelligence” than artificial intelligence. Its primary accomplishment will be to assist us in our everyday lives, and to help us do our jobs better by “augmenting” and multiplying what types of problems we humans can solve on our own.
This term “augmented intelligence” has been promoted by IBM as what AI should be. It was coined in 1962 by Douglas Engelbart:
“Increasing the capability of a man to approach a complex problem situation, to gain comprehension to suit his particular needs, and to derive solutions to problems.
…We do not speak of isolated clever tricks that help in particular situations. We refer to a way of life in an integrated domain where hunches, cut-and-try, intangibles, and the human feel for a situation usefully co-exist with powerful concepts, streamlined terminology and notation, sophisticated methods, and high-powered electronic aids.”
-“Augmenting Human Intellect” (DARPA, 1962)
Indeed I think IBM and Engelbart are going to be proven right: AI acting as “Augmented Intelligence” will power the next era of ubiquitous voice-based personal assistants in the next era of consumer technology, the “Assistant Era”, and will begin to gain real traction with businesses by helping employees do their jobs better, faster, cheaper, and smarter.
AI in the new Assistant Era
For decades in the 1980s-1990s, many hoped that full-fledged artificial intelligence was just around the corner. But by the time of the “Web Era” (approx. 1996–2008), the reality was that the algorithms, computing power, available data sets, and hardware were just not ready to deliver talking robots or true AI ready for applications in the real world. We still got Google, which fundamentally changed how we access and consume information.
Then came the “Mobile Era” (approx. 2008–2017) where each app company wanted to automate some aspect of our lives through our new smartphones. As it turned out, we were just not ready to hand over our lives to an app. We didn’t want apps knowing everything about us and eerily suggesting what we should do next, without our asking. If we’re being told by our phone that right now is the time to turn left at the next stoplight / buy this product / save up $100 / watch this TV show / book this hotel … we want to know why we should do so. We need to believe this app is helping us and somehow figuring out what’s really in our own interest, not bombarding us with marketing. Few apps successfully crossed into the realm of doing this intelligently and with our consent.
Then in 2014 Amazon’s Echo and Alexa planted the seed of the new era. They changed the game by pairing together: (1) Speech recognition algorithms and deep learning that that could finally do voice-to-text with an error rate below 5%, plus (2) A high-quality speaker in a form factor that could sit in your kitchen and continuously listen for a hot word. This proved the magic formula to make us start to believe a friendly anthropomorphized being was actually sitting in our kitchen just waiting to set a timer for 10 minutes or deliver the day’s news. (Even my kids tell me not to be “mean” to Alexa when I get a little annoyed with her limited vocabulary.)
iPhone vs. Echo sales since year of launch (source: Forrester Research, Statista/Apple)
Alexa proved the catalyst to drive unexpectedly high sales and jumpstart a new market — just like when the iPhone paired a low-latency touch screen interface with a CPU/RAM/HD that could all fit within a device the size of your hand, creating a new user experience intuitive enough for a billion people to use out of the box.
Fast forward to today (believe it or not, it is 2018). Speech recognition within Google Assistant, Google Voice Keyboard, Apple’s Siri and voice-recognition within iMessage or Apple TV, and Microsoft’s Cortana have caught up and even surpassed Amazon’s Alexa when it comes to high-quality NLP and chat that can understand most of what we say to it — primarily thanks to the success of Deep Learning trained on very large data sets of voice queries.
We are starting to hold basic conversations with the nice personalities embodied in Alexa, Siri, Google, and Cortana with the expectation that they can talk back to us intelligently through our phones, homes, tablets, TVs, and cars. Hundreds of other chatbots built by startups, banks, and eCommerce companies are all getting ready to talk to us too.
The user experience of talking is faster and better than typing on a tiny keyboard or navigating the visual UI of a small screen. The reason is simple: we can speak and understand speech at about 150 words per minute, but the average person can only type at about 40 words per minute on a phone’s keyboard (and with 8% errors).
Thus we have quietly entered the beginning of the “Assistant Era”. There’s no going back to the “Mobile Era” where innovation was driven (and funded) primarily by sales of phones or apps. Apps are now becoming just one of several channels through which we talk to an Assistant or engage with some offline service like Uber.

However — much like the iPhone and Android in the early days brought promise but ultimately very limited functionality — the Assistants of this new era still have a long way to go before they can have an intelligent conversation with us, not just respond to a set of simple queries or commands (and only when we use exactly the right words). Even Facebook saw the shortcomings in their Messenger “M” chatbot and took it down.
To meet the demand that Assistants get better at understanding us and responding intelligently, the AI researchers and engineers will need to keep innovating fast in a few fields:
Natural Language Understanding (NLU): Understanding exactly what we mean by words like “her”, “love”, “firms”, “grow”, “global stocks”, or “there” is surprisingly tricky to teach a computer to do, despite our brains’ ability to naturally figure out meaning within the context of a speaker or situation. NLP/NLU techniques are developing ways to infer semantics through (a) vector-space modeling and autoencoding of patterns in what words are typically used together in a sentence like word2vec and GLoVe, (b) named entity recognition to identify what entities a document’s nouns/pronouns/indexicals point to, (c) intent recognition and topic modeling to infer what a chunk of text is talking about or trying to communicate, and (d) dialog managers to keep track of the flow of a conversation and determine how to best respond in the face of ambiguity (disambiguation), to fill in missing information (slot filling), and to interpret different ways people say the same thing (variations).
Recommender Systems: What do we humans want? It’s hard enough to understand this about ourselves, let alone teach AI to predict it for us. Yet we judge the usefulness of any Assistant or chatbot by the subjective quality of what it suggests to us and the help it offers, not just how well it responds to precise queries. Let’s say a chatbot says, “Would you like help?” on an eCommerce site and you respond, “Yeah, I’m looking for nice dress shoes to go with my jeans.” The chatbot needs to quickly decide what are the right few dress shoe products to suggest (that actually do go well with jeans), otherwise you‘ll quickly determine this is just a dumb, useless bot. The field of recommender systems is solving this by moving beyond collaborative filtering and linear matrix factorization (think: Netflix and Amazon.com) into coupling Deep Learning with recommender systems to model non-linear preferences and create a latent vector-space model of users and products, doing sentiment classification, and ranking suggestions using many diverse features via supervised machine learning algorithms like learning-to-rank.
Machine Translation: Google Translate has employed Deep Learning with very-deep neural networks and vector-space modeling of semantics across languages to automatically translate between any of over 100 languages. Google’s NLP and Translation APIs will let developers incorporate translation into their own Assistants in new, creative ways.
Augmented Intelligence to help us do our jobs better (not take our jobs away)
High user growth will come from Assistants for consumer applications. But high revenue growth for the AI industry will come first from businesses and their employees.
Fearing robots will replace us, literally (source)
The widespread fear of AI’s future in the workplace is this: AI will replace humans and put thousands (or millions) of people out of work. Jobs will shift away from low-wage labor to high-wage/highly-skilled jobs in technology and management, increasing inequality. The trucking industry is supposedly the harbinger, where 3 million jobs in the US will be obviated by self-driving trucks.
Side-stepping the moral/economic/political debate around whether this should be acceptable or unacceptable for society…I believe the fear is unwarranted and a red herring, because that is just not going to happen.
Why? Technology does not leapfrog its way into society against the interests of consumers, employees and employers. Technology takes “the path of least resistance” to gain traction through incremental applications used by real people who are comfortable with that technology and are willing to pay for it.
In business the early adopters need to be employees themselves. Those employees need to be the ones to choose to buy AI-based services, so the path of least resistance for AI will be to start by providing “augmented intelligence” to help us human employees do our jobs better. Replacing us in large numbers is not only infeasible (technically), but going to be too costly for business owners to consider in all but specific industries, for many years.
The future driving experience according to Mercedes-Benz
To illustrate let’s look at the prospect of Self-driving Cars / Autonomous Driving, one of the most exciting and well-hyped uses of AI technology. Google proclaimed as early as 2012 that we would be riding in self-driving cars by 2017. Yet here we are in 2018 and we are probably still at least another decade away from truly autonomous cars being manufactured en masse. Why?
It turns out that achieving Level 5 autonomous driving — where the driver is entirely uninvolved in the driving process — is a lot harder than we were led to believe. It is not enough for the autonomous car to drive around sensing the road and other cars or pedestrians through vision/lidar/radar (relative positioning and object detection). It needs to know where it is on the road in order to slow down in time for an upcoming stoplight or get into the right-most lane to proceed off the exit ramp (absolute positioning). It needs to handle conditions with impeded visibility like snow, darkness, rain, or glare. It needs to predict what other drivers are going to do like in a crowded intersection (multi-agent systems), and even communicate with the other autonomous vehicles (connected vehicles). It needs to make decisions under uncertainty like whether to slow down because there’s a truck crossing the lane or keep going because that’s just a sign overhead (planning, and even ethics). To solve all this together will require significant coordination between not only (a) the autonomous driving software providers/startups and (b) auto manufacturers, but also (c) sensor manufacturers, (d) Tier-1 parts suppliers, (e) road mapping companies, (f) local/state governments and regulatory bodies, and (g) the consumers themselves. That kind of coordination effort between many parties will inevitably take years.
Financially, the total cost of the stack of software+hardware+services to power autonomous driving will need to come down below a few thousand dollars before this would be cheaper on net than the wages paid to truck or delivery drivers. That will all take years to happen — I’d wager at least 10 years — and then it will take another 10 years or so to swap out the existing inventory of cars/trucks already on the road with new autonomous vehicles.
Instead actually the kind of automation being offered so far by Tesla, GM, Ford, Google/Waymo, Uber and startups like Drive.ai, Cruise/GM, Zoox, nuTonomy/Delphi, Nuro.ai, and Embark is Level 2 or Level 3 autonomy, where the car drives itself only in certain circumstances like on highways, on known routes, or while parking, and otherwise relies on the human driver to take back over. The role of this kind of AI is to assist the driver, not replace the driver.
Businesses are beginning to demand AI as augmented intelligence in a few different forms. In Artificial Intelligence for the Real World (Harvard Business Review, Jan-Feb 2018), Thomas Davenport and Rajeev Ronanki surveyed 250 executives and found that:
51% of companies said it was to “enhance the features, functions, and performance of our products”,
36% said it was to “free up workers to be more creative by automating tasks” or to “optimize internal business operations”
only 22% (the least common answer) said it was to “reduce head count through automation.”
They also manually analyzed 152 projects involving AI and found 38% focused on detecting patterns in large datasets using machine learning, while 47% focused on robotic process automation (RPA):
“One might imagine that robotic process automation would quickly put people out of work. But across the 71 RPA projects we reviewed (47% of the total), replacing administrative employees was neither the primary objective nor a common outcome. Only a few projects led to reductions in head count, and in most cases, the tasks in question had already been shifted to outsourced workers.”
To keep up with this demand, the AI industry will need to continue finding ways to help employees accomplish a task more easily and more quickly than they could on their own. Here are some business applications of AI and Machine Learning being implemented right now:
A vector-space representation of word meanings (source)
Text Processing: Well-trodden techniques in natural language processing (NLP), text retrieval, and text mining are being combined in new ways with machine learning to extract insights and knowledge from large bodies of text then deliver simple summaries or suggestions for documents to read. For instance in the legal field, startups like Casetext are combing through millions of legal case decisions to tell attorneys what they should read. Or large companies fielding volumes of customer support inquiries can better triage high-priority tickets, help their customer support agents respond with relevant answers, or monitor what things are being said on social media streams. This is coming primarily from startups applying recent AI techniques developed in academia like autoencoders, learning-to-rank, LDA text clustering, vector space modeling, or hierarchical topic/intent modeling (like Lang.ai).
Anomaly Detection: Deep Learning is getting better at learning how to find the “needle in the haystack” when a single actor is too different from the normal pattern of behavior and should be flagged— such as when detecting insurance claims fraud, credit card/payments fraud, hacking and cybersecurity threats, Medicare fraud, or bank account takeover. This can be done by training a decision tree/random forest/gradient boosting model on training data, or using newer techniques like autoencoders or Bayesian/Gaussian cluster estimation even when prior training data are not available.
Risk Assessment: All underwriting decisions, credit risk assessments and mitigation of default risks can be done across complex data sets by neural network-based classification/regression models or decision trees/random forests/gradient boosting, and faster, cheaper and with higher accuracy than traditionally done manually by Ops teams. Lending startups like Affirm, Upstart, Change, and Underwrite.ai all use machine learning at the core of their underwriting and risk assessments.
Identification: Convolutional neural networks are very good at identifying when something is the same as something else, even when there are many variations/dimensions/features to sift through. For instance in the United States now we are all introduced to face recognition built into the iPhone X — which figures out that the face shown to the iPhone is the same person as the original face the iPhone saw during setup. Expect to see this deployed widely in security cameras for surveillance or “badging in” to get into a building, as is already done in China. The same technology is used to do fingerprint recognition in almost all iPhone/Android phones, and also is being applied in mobile device fingerprinting where the unique characteristics of a single device make it a “key” for identification of a user online.
Predicting Prices: AI is capable of finding complex patterns in time-series data like pricing and using them to forecast future prices with high accuracy. For instance in real estate, machine learning kernel regression and vector-space similarity modeling are used to predict prices of homes not yet on the market, such as in Zillow’s Zestimates, Redfin Estimates, or AirBnb’s suggested prices for rental units, by finding similarities with other homes even in a different part of the country. Similarly, pricing of commodities and investments are being forecasted using ML regression models by new kinds of quant hedge funds.
Lead Qualification: Salesforce is leading the charge in developing AI and ML for the sales process, with a significant investment in their Einstein product line. Sales and marketing professionals so often need to decide whether one customer lead is more likely to convert than another, and AI is capable of doing this better than most humans can. This can be accomplished using ML classification using decision trees/random forest or deep neural networks with logistic regression/softmax output units. Salesforce is funding research to innovate new lead scoring methods, too.
Product Recommendations: Every eCommerce or content site with many products or pieces of media to sell needs to determine which products to target to which users. In the early 2000s, Amazon.com, Netflix, and Pandora innovated by using collaborative filtering, matrix factorization and ML-based recommender systems to do this well. Now startups are integrating more sophisticated vector-space models of products’ similarities and user preferences into their recommendation engines. Also this can be applied in other fields like medicine, where AI can assist doctors in recommending the right diagnosis/treatment to the right patient based on a complex knowledge base.
The time for AI has arrived, and it won’t be just a passing hype cycle. The techniques developed by the AI research community are being integrated more and more into applications for both consumers and business.
Of course AI will not be all beneficial for society —just like with mobile phones and social media, it will take a while to digest and overcome the downsides new technology brings. But in the next decade AI‘s contribution will be primarily to help us. It will augment our intelligence, help us be more productive, more creative, and hopefully live more enriched lives, too.
Have any other predictions for what AI will become in the next few years? Leave your thoughts in the comments.
About Us:
Waterway Data is a consulting firm offering on-demand AI, machine learning, and data science. Please get in touch if your product or business could be made more intelligent using the latest in AI technology.
",3256,Artificial Intelligence,Waterway Data,https://medium.com/s/story/ai-as-augmented-intelligence-6eca88cecd2c,"In the upcoming years AI will primarily serve as “augmented intelligence” to assist us in our everyday lives and help us do our jobs better.
AI has traditionally referred to “artificial intelligence” coming from the imagination of science fiction: omnipotent robots and sentient intelligences in the cloud, changing the world drastically but also stoking our fears of losing control (and our jobs) to our own creations.
Its primary accomplishment will be to assist us in our everyday lives, and to help us do our jobs better by “augmenting” and multiplying what types of problems we humans can solve on our own.
Indeed I think IBM and Engelbart are going to be proven right: AI acting as “Augmented Intelligence” will power the next era of ubiquitous voice-based personal assistants in the next era of consumer technology, the “Assistant Era”, and will begin to gain real traction with businesses by helping employees do their jobs better, faster, cheaper, and smarter.
They changed the game by pairing together: (1) Speech recognition algorithms and deep learning that that could finally do voice-to-text with an error rate below 5%, plus (2) A high-quality speaker in a form factor that could sit in your kitchen and continuously listen for a hot word.
Alexa proved the catalyst to drive unexpectedly high sales and jumpstart a new market — just like when the iPhone paired a low-latency touch screen interface with a CPU/RAM/HD that could all fit within a device the size of your hand, creating a new user experience intuitive enough for a billion people to use out of the box.
Speech recognition within Google Assistant, Google Voice Keyboard, Apple’s Siri and voice-recognition within iMessage or Apple TV, and Microsoft’s Cortana have caught up and even surpassed Amazon’s Alexa when it comes to high-quality NLP and chat that can understand most of what we say to it — primarily thanks to the success of Deep Learning trained on very large data sets of voice queries.
We are starting to hold basic conversations with the nice personalities embodied in Alexa, Siri, Google, and Cortana with the expectation that they can talk back to us intelligently through our phones, homes, tablets, TVs, and cars.
There’s no going back to the “Mobile Era” where innovation was driven (and funded) primarily by sales of phones or apps.
However — much like the iPhone and Android in the early days brought promise but ultimately very limited functionality — the Assistants of this new era still have a long way to go before they can have an intelligent conversation with us, not just respond to a set of simple queries or commands (and only when we use exactly the right words).
To meet the demand that Assistants get better at understanding us and responding intelligently, the AI researchers and engineers will need to keep innovating fast in a few fields:
NLP/NLU techniques are developing ways to infer semantics through (a) vector-space modeling and autoencoding of patterns in what words are typically used together in a sentence like word2vec and GLoVe, (b) named entity recognition to identify what entities a document’s nouns/pronouns/indexicals point to, (c) intent recognition and topic modeling to infer what a chunk of text is talking about or trying to communicate, and (d) dialog managers to keep track of the flow of a conversation and determine how to best respond in the face of ambiguity (disambiguation), to fill in missing information (slot filling), and to interpret different ways people say the same thing (variations).
Yet we judge the usefulness of any Assistant or chatbot by the subjective quality of what it suggests to us and the help it offers, not just how well it responds to precise queries.
Let’s say a chatbot says, “Would you like help?” on an eCommerce site and you respond, “Yeah, I’m looking for nice dress shoes to go with my jeans.” The chatbot needs to quickly decide what are the right few dress shoe products to suggest (that actually do go well with jeans), otherwise you‘ll quickly determine this is just a dumb, useless bot.
The field of recommender systems is solving this by moving beyond collaborative filtering and linear matrix factorization (think: Netflix and Amazon.com) into coupling Deep Learning with recommender systems to model non-linear preferences and create a latent vector-space model of users and products, doing sentiment classification, and ranking suggestions using many diverse features via supervised machine learning algorithms like learning-to-rank.
High user growth will come from Assistants for consumer applications.
Those employees need to be the ones to choose to buy AI-based services, so the path of least resistance for AI will be to start by providing “augmented intelligence” to help us human employees do our jobs better.
It needs to predict what other drivers are going to do like in a crowded intersection (multi-agent systems), and even communicate with the other autonomous vehicles (connected vehicles).
To keep up with this demand, the AI industry will need to continue finding ways to help employees accomplish a task more easily and more quickly than they could on their own.
Text Processing: Well-trodden techniques in natural language processing (NLP), text retrieval, and text mining are being combined in new ways with machine learning to extract insights and knowledge from large bodies of text then deliver simple summaries or suggestions for documents to read.
Or large companies fielding volumes of customer support inquiries can better triage high-priority tickets, help their customer support agents respond with relevant answers, or monitor what things are being said on social media streams.
This is coming primarily from startups applying recent AI techniques developed in academia like autoencoders, learning-to-rank, LDA text clustering, vector space modeling, or hierarchical topic/intent modeling (like Lang.ai).
Predicting Prices: AI is capable of finding complex patterns in time-series data like pricing and using them to forecast future prices with high accuracy.
For instance in real estate, machine learning kernel regression and vector-space similarity modeling are used to predict prices of homes not yet on the market, such as in Zillow’s Zestimates, Redfin Estimates, or AirBnb’s suggested prices for rental units, by finding similarities with other homes even in a different part of the country.
Sales and marketing professionals so often need to decide whether one customer lead is more likely to convert than another, and AI is capable of doing this better than most humans can."
Appreciation for New Restaurant Tech,A new tech solution for my restaurant enters the market every day. And with the flood of new products hitting my inbox I need a system to…,"Photo: Jamie Fullerton for Munchies.Vice.com
Appreciation for New Restaurant Tech
A new tech solution for my restaurant enters the market every day. And with the flood of new products hitting my inbox I need a system to help me evaluate their claims.
I love it when I find one product that helps me with many different aspects of my business. But I need to weigh every component in a new product in order to characterize its value for my business. I ask myself, does the new product cut my costs, grow my revenue, or increase my productivity?
Once I understand the value the product is offering to my restaurant, I consider the offer to see if it is worth my time and money. I ask if the product is priced fairly for my needs, if it will work for my business, and if I have time to adopt the product now.
So when a new tech product shows up in my inbox, these are the kinds of things I think about before I decide to reply:
Products That Address Cost Drivers are Valued on Comparison
New technologies that address cost drivers can be the most attractive for both restauranteurs and start-ups. Restaurants like these products because their value is easy to see. Start-ups like these products because they are often able to fetch the highest prices.
I did the math for OpenTable a long time ago. It is expensive, true. But it is about the same price as I was paying to have someone come in early, call back reservation requests, and build a floor map from a simple spreadsheet of names. I save a little bit in payroll costs and I get an optimizing reservation system that operates 24 hours a day and never needs the week off to go to Burning Man.
From a start-up perspective, a product with a cost-cutting feature is all about Value-Based Selling. As Josh Kaufman put it in The Personal MBA, “the Value Comparison method is often the best way to get the highest price for your offer.” This is because the same value the restaurant puts on cutting the cost can be nearly equal with the price for the new product.
Today there are products like Orderly that attempt to offer savings for my Cost of Goods. These products keep a price history from all of my vendors so it is easy to tell who has the best price for things like produce, which hasn’t been tracked well in the past.
For example, say before the app I paid $100 a month for vegetables for my menu. With the app I pay $75 to run the same menu because this new product allows me to source the cheapest products. I would expect that the price for this product would start to creep toward $25 per month. But if the price is over that or if I cannot easily track the savings, it’s tough to sell me.
So I know that if I am evaluating a product that claims to make one of my key costs way lower, I can expect the price to make up a lot of that difference. That is why I keep my eyes peeled for new products that show promise but may not be perfected. Maybe I can get an early contract with lower rates locked in. Likewise, a company might be willing to partner with a restaurant in an early stage to get feedback on development and to get social proof that other restaurants are benefiting from the product.
Products That Grow Revenue Need to Align with My Brand
According to Kaufman again, there are four ways to increase revenue: increase the number of customers, increase the size of each transaction, increase the frequency of transactions, and the dreaded raise prices. When I employ a product that attempts to market our restaurant I need to make sure that product doesn’t confuse or diminish our brand.
Increasing customers with promotions that put parameters on guest behavior, like Groupon, are a thing of the past. Now products like TryCaviar make it possible for us to deliver to more customers while keeping tables inside the restaurant open for dine-in guests. It is easy for us to get the most out of this service during the slower period at the beginning of the evening and we can turn it off if the kitchen gets too busy. Also, we are not discounting our product so the customer isn’t confused about the value we offer.
When it comes to increasing the size or frequency of our transactions, a simple loyalty product like the one Valutec offers can work. It doesn’t put parameters on the guest’s experience so they choose when to come in and how much to spend. And we are free to pull out all the stops to make sure the food and hospitality we offer keeps them coming back to try new things.
I also like a simple product like Merchant Centric that puts all of my peer reviews in one spot, my inbox, so that I can quickly notice and respond to them. This is the best way to get the personal attention of some of our most vocal guests. And I am in control of the message and offer I extend.
I have heard of products like Table8 which effectively raise prices by charging a premium for reservations booked at peak times. But that is not really in keeping with our brand. My restaurant is a neighborhood joint that I hope has a certain authentic, global appeal. If I were to see that other restaurants like mine were adopting a tech product like this, I may be more likely to join. In this case, I would need some social proof that the industry was headed this direction. But I would still be skeptical.
Products that Streamline Systems Need Time and Attention to Implement
The truth is, almost any new product will require time and attention to implement. But when it comes to instituting a new technology that a number of my employees will come in contact with, the need for orientation and training can be demanding.
InVine is a product that helps keep track of wine inventory and keeps a lot of wine education information in one place. And for the relatively tech-savvy front of the house staff, this is an easy product to implement.
But what about the kitchen? A productivity tool shouldn’t just work for decision-makers. It should also be appealing to those who are going to use it: craftsmen who work with their hands and immigrant labor for whom technology and, even English, are not something they grew up with. As much as I am interested in productivity solutions for the kitchen, they are harder to implement.
New kitchen apps like Sous need to have an easy-to-use platform that the whole staff is comfortable with. Since our menu is not laminated, the platform needs to be easily updated so that kitchen staff doesn’t fall back into old habits when the menu changes. And if it can toggle between languages, even better.
The tech company and I both need to do some Education-Based Selling to get staff behind the adoption of a new product that changes how things get done. And if the tech company can’t help me in this process, I may not be interested. I look for things like a decent trial period, help with the system set-up, and onsite tutoring with key members of the team.
Are You Taking Control of Your Restaurant Technology Solutions?
With so many new tech companies focused on the restaurant industry, it is easy to see how some owners are overwhelmed by the number of pitches in their inboxes. But it would be a mistake to ignore all of the new products trying to make the restaurant business a little more profitable and fun. With the right value, a brand strategy, and good timing there is the potential for both restaurants and start-ups to benefit.
",1344,Entrepreneurship,Jeff Trenam,https://medium.com/s/story/appreciation-for-new-restaurant-tech-d750e8ae3758,"A new tech solution for my restaurant enters the market every day.
And with the flood of new products hitting my inbox I need a system to help me evaluate their claims.
I ask if the product is priced fairly for my needs, if it will work for my business, and if I have time to adopt the product now.
Restaurants like these products because their value is easy to see.
I save a little bit in payroll costs and I get an optimizing reservation system that operates 24 hours a day and never needs the week off to go to Burning Man. From a start-up perspective, a product with a cost-cutting feature is all about Value-Based Selling.
As Josh Kaufman put it in The Personal MBA, “the Value Comparison method is often the best way to get the highest price for your offer.” This is because the same value the restaurant puts on cutting the cost can be nearly equal with the price for the new product.
When I employ a product that attempts to market our restaurant I need to make sure that product doesn’t confuse or diminish our brand.
When it comes to increasing the size or frequency of our transactions, a simple loyalty product like the one Valutec offers can work.
And we are free to pull out all the stops to make sure the food and hospitality we offer keeps them coming back to try new things.
New kitchen apps like Sous need to have an easy-to-use platform that the whole staff is comfortable with.
The tech company and I both need to do some Education-Based Selling to get staff behind the adoption of a new product that changes how things get done.
With so many new tech companies focused on the restaurant industry, it is easy to see how some owners are overwhelmed by the number of pitches in their inboxes.
With the right value, a brand strategy, and good timing there is the potential for both restaurants and start-ups to benefit."
The Wonderful Wasserstein GAN,(This article is translated from Mr. Zheng Huabin‘s 《令人拍案叫绝的Wasserstein GAN》originally posted on Zhihu. The translator has already received…,"The Wonderful Wasserstein GAN
(This article is translated from Mr. Zheng Huabin‘s 《令人拍案叫绝的Wasserstein GAN》originally posted on Zhihu. The translator has already received Mr. Zheng’s authorization for the publication of the translated version. For native speakers / those who are interested in reading the Chinese version, here’s the link.)
While researching GAN is becoming so popular that doing it starts to become somewhat cliché, a new paper posted on arXiv called Wasserstein GAN has sparked a huge discussion on the Machine Learning subreddit, to a point where even Ian Goodfellow himself joined the talk. So, what’s so unique about this new theory?
Just FYI, since their first appearance in 2014, GAN’s implementations have suffered from training difficulties, loss of generative/discriminative (G/D) models being unable to identify the training process, as well as collapsing generated samples. Scholars did attempt to improve, yet most of the papers made little improvement, like the enumerative methodology introduced by the DCGAN paper, where searches are done on different generative/discriminative model architectures. But this doesn’t help the big picture, except that Wasserstein GAN (WGAN) did. In general, WGAN exhibits the following mind-blowing characteristics:
It completely solves the instability of GAN training, where concerns on balance between generative/discriminative model training are no longer needed;
It generally solves the collapse mode problem and ensures the diversity of the generated samples;
A metric finally exists (just like accuracy/cross entropy) to indicate the process of training, where a smaller value indicates a better generative model;
The above can be achieved without a carefully designed model architecture; even a naïve MLP is capable of converging.
The snapshot of the algorithm can be found below:

But where do these benefits come from? This is the part where the Wasserstein GAN is truly “Wonderful” (hence the alliteration). It takes the original author of WGAN two entire papers to explain everything: the theorems in “Towards Principled Methods for Training Generative Adversarial Networks” that provides an analysis to why the original GAN doesn’t function and thus the ways to improve it, and the following “Wasserstein GAN” that provides an additional set of theorems that finalizes this novel methodology. Intriguingly, the new algorithm only made four alterations to the original GAN:
Removing the sigmoid activation function in the last layer
Removing the log in the loss function of G/D models
Clipping the parameters so that their absolute values are smaller than a constant c
Not using any optimization methods based on momentum (like the naïve method and Adam); RMSProp is the recommended method, though SGD works as well
The simplicity of these modifications makes it so deceivingly astonishing that people on Reddit questions: is that it? Isn’t there any other hidden details? This reminds me of another cliché story on the net, which says that an experienced engineer can earn ten grand by simply drawing the correct line on the shell of the motor that identifies the annoying problem. Drawing a line worths 1 dollar, while knowing where to draw it gives you the rest 9999. The difference is pretty evident, and the four alterations are the four lines by our author Martin Arjovsky. At this point, it is already enough for implementation from an engineering perspective, but the “knowing where to draw the line” part roots deeply from sophisticated mathematical analyses, which is the primary goal of this essay.
The following paragraphs are organized into the following five sections:
What’s really not working in the original GAN (this part is pretty long)
A remedial solution before the advent of WGAN
The superior characteristics exhibited by the Wasserstein distance
From Wasserstein distance to WGAN
Conclusion
Understanding the proofs and theorems in the original paper requires rather extensive knowledge on measure theory and topology, so instead this paper will focus on a more intuitive aspect, where the interpretations are based off lower-dimensional examples to help readers to ponder the ideas behind the scene while maintaining mathematical rigor. You are welcomed to point out any errors in the comment section.
From now on, “Wasserstein GAN” is referred as “The original WGAN,” while “Towards Principled Methods for Training Generative Adversarial Networks” is referred as “Preliminaries on WGAN.”
Section1: What’s really not working in the original GAN
To recap, original GAN paper proposes that the goal of the discriminator is to minimize the following loss function, where a sample from the model distribution has a positive effect and vice versa.
(Equation 1)
In the equation, $P_{r}$ refers to the model distribution, and $P_{r}$ refers to the implicitly defined generative network distribution. For the generative network, Goodfellow initially presented a loss function, to which a refined version was also proposed. One can found both of them respectively below:
(Equation 2)
(Equation 3)
The latter is referred as “the — log D alternative” or “the — log D trick” in the two WGAN papers. The original WGAN analyzed respectively the problems with these two loss functions, both of which will be explained below:
The problem with the first form of the original GAN
Tl; DR: The better the discriminator is, the worse the vanishing gradient effect will be.
The long version: The “Preliminaries on WGAN” presented two perspectives on this problem, the first of which starts with taking a closer look at the equivalent form of the generative network loss function.
Firstly, from Equation 1 we can obtain the optimal discriminator by looking at an arbitrary sample $x$. Since it can come from either the model distribution or the generative distribution, its contribution to the overall loss can be expressed as

Solve by setting the derivative of $D(x)$ to 0, we get

Furthermore, by simplifying the expression, we can finally get the optimal discriminator as
(Equation 4)
The result is straightforward to interpret intuitively since it only deals with the relative ratio between the likelihood of the sample coming from the model distribution and the generative distribution. If $P_r(x)=0$ and $P_g(x) \neq 0$, the optimal discriminator should confidently produce the output 0; if $P_r(x)=P_g(x)$, which means that there is a half-half chance of sample being true/false, the optimal discriminator should also express this indecisiveness as an output of 0.5.
But GAN has a trick not to train the discriminator too well, or the generative network will stop converging (its loss stop decreasing). To understand why this is the case, we take a look at the extreme condition: what the loss function of the generative network would look like if the optimal discriminator is obtained. Adding a term that does not depend on the generative network to Equation 2, we obtain

It is important to notice that this is exactly the additive reciprocal of loss function of the discriminator. By plugging in Equation 4, some simplifications will result in the following:
(Equation 5)
The presentation of this seemingly weird form is to help introduce the Kullback-Leibler divergence (KL divergence) and the Jensen-Shannon divergence (JS divergence), two important metrics in the original GAN framework that are the primary targets of criticism in the WGAN papers (where they are replaced by the Wasserstein distance). The exact definition of these two metrics can be found below:
(Equation 6)
(Equation 7)
Thus, Equation 5 can be rewritten as
(Equation 8)
The readers are welcomed to take a deep breath at this point and see the conclusions we have made so far: According to the generative network loss defined in the original GAN, we can obtain an optimal discriminator, and thus rewrite the generative loss as the JS divergence between the model distribution $P_r$ and the generative distribution $P_g$. The goal of training the discriminator in per se is to minimize this JS divergence.
That’s where the problem comes in. Of course, we would like to minimize the JS divergence between two distributions, where $P_g$ is being “pulled” towards $P_r$ and eventually can forge the examples. The above is true when the two distribution intersects, but what happens when the two distributions do not overlap, or their intersection is trivially small that it is safe to ignore them (a concept that will be explained below)? What will the JS divergence be like? The answer is $\log2$ since there are only four possible cases for an arbitrary x:

The first one totally does not contribute to the JS divergence, so does the second due to the negligibility of the intersection. According to the definition of JS divergence, this third case evaluates to:

Since the fourth case is also analogous, in the end

In other words, no matter how far they are apart from each other, as long as the distributions are not “touching each other with a considerably large intersection,” the JS divergence is fixed to the constant $\log2$, which gives a gradient 0 for the gradient descent method. This is terrible news cause the generative network is unable to get any information from the optimal discriminator; and for the almost-optimal discriminators, the threat of vanishing gradients still exists.
But how likely is it that the distributions $P_r$ and $P_g$ do not share a cross-section that is large enough? The TL;DR version again is very, very likely. The more rigorous version is: If the supports of the two distributions are low-dimensional manifolds in the high-dimensional space, the possibility that the measure of the intersection between them is zero is one.
Don’t be scared of the jargons that just jumped right at you, and don’t close the webpage yet. Although what the paper gave us were those really rigorous mathematical expressions, intuitively it is easier to comprehend. First, let us take a look at these concepts mentioned above:
Support: it’s simply the non-negative subset of some function. The support of Rectified Linear Unit function (ReLU) is simply $(0,+\infty)$, and the support of a probability distribution is the set of all states where the probability density is non-zero.
Manifold: It is the extension of the curve and surface concept in high-dimensional space. To comprehend manifold in a low-dimensional sense, a surface in the three-dimensional space is considered as a two-dimensional manifold, since its intrinsic dimension is only 2. An arbitrary point on this surface only has a degree-of-freedom of 2. Similarly, a line in three/two-dimensional space is just a one-dimensional manifold.
Measure: It is the extension of the length/area/volume concept in high dimensional space. One can simply think of them as a measure of “hyper-volume.”
Now, let’s return to the statement above. It is very likely that “the supports of the two distributions are low-dimensional manifolds in the high-dimensional space,” the reason of which roots from the process of the generative network. To produce a sample, the generative requires input from a pre-defined noise prior (usually multivariate Gaussian), which is most of the time low-dimensional. After such a low-dimensional “seed” (like 100-D) is passed into the forward computational graph, a high dimensional picture is produced (like the dimension of a 64*64 picture is 4096), multiple outputs of which also implicitly defines a high-dimensional generative distribution. If the parameters of the generative network are fixed, though the distribution is defined in a high-dimensional space (like 4096), the variation of which is merely restricted by the low-dimensional support (like the 100-D “seed”). Its intrinsic dimension is much, much lower. Counting in the dimensionality-reduction effect of a neural-network mapping, the intrinsic dimension of the example mentioned above can be even smaller than 100. Thus, the support of this 4096-D generative distribution can only be a low-dimensional manifold whose dimension can be no more than 100, which makes the distribution “unable to extend” to the entire space.
The adverse effect of being “unable to extend” is that the model distribution is very unlikely to “collide” with an arbitrarily defined generative distribution. This is easy to comprehend in two-dimensional space. On the one hand, if one picks two random curves in a two-dimensional plane, the probability that they have overlapping section is 0. On the other hand, although they are very likely to have intersecting points since a point is one dimension lower than a line, its measure is 0, and that makes it trivial to the question. The same thing works similarly in three-dimensional space, where though two random surfaces may share the same lines, their measure is 0 because a line is one dimension lower than a surface. Thus we can arrive at another conclusion: if the generative network is initialized randomly, $P_g$ is highly unlikely to have any relation to $P_r$, and their supports either do not intersect at all, or the intersections has a dimension that is lower than the lowest dimension in $P_r$ and $P_g$, hence having a measure of 0. So the “measure of the intersection between them is zero” is just a fancy way of saying “not intersecting/ intersections are too small to be significant.”
Then we can obtain the first argument on the vanishing gradient in the generative network of the original GAN: given an (approximately) optimal discriminator, minimizing the loss of the generative network is equivalent to minimizing the JS divergence between Pr and Pg, and since they are highly unlikely to have any intersections, no matter how far the two distributions are apart from each other, their JS divergence is always $\log2$, which results in (approximately) zero gradient.
Then the preliminary GAN analyzes the problem again in the second perspective with tons of formulae and theorems, but the idea behind them is pretty intuitive:
First of all, it is almost impossible for $P_r$ and $P_g$ to have a nonnegligible intersection, so no matter how close they are from each other, there exists an optimal hyperplane to separate most of them, maybe expect the trivial points where they do intersect.
As a universal approximator, an artificial neural network can approximate this hyperplane to infinite precision, so an optimal discriminator do exist that give almost all model examples 1 and all generative examples 0. This discriminator does struggle on the samples that reside at the intersection of the two distributions, but since the set has a measure of 0, it is okay to ignore their effects.
The normalized possibilities that the optimal discriminator provide on both the model distribution and the generative distribution are all constant (1/0), so the gradient of the generative network loss is always 0, hence the vanishing gradient.
With these theoretical analyses, we finally know why the original GAN is so unstable: Discriminator too good? Vanishing gradients that completely stops the convergence of generative network loss. Discriminator too bad? The generative network cannot receive a good clue on the update direction, the gradient wiggles. The only time it works is when you train a discriminator that is neither too good or too bad, and it is tough to figure out how to do so.
Here’s the empirical result, obtained from the “Preliminaries on WGAN”:
Pay attention to the log scale on the y-axis
The problem with the second form of the original GAN
Tl; DR: The equivalent form of the second loss function is an unreasonable distance measure that not only destabilizes the gradient but also causes the mode to collapse (not enough diversity).
The long version: The “Preliminaries on WGAN” also analyzed this form by two different perspectives. However, I’m only able to find an intuitive explanation for the first perspective. So, if you are interested in the second one, I suggest reading the original paper.
As mentioned in the previous paragraphs, Ian Goodfellow’s proposed “- log D trick” changed the generative network loss into
(Equation 3)
And we’ve already known what an optimal discriminator looks like:
(Equation 9)
Then, it is possible to rewrite the KL divergence of $P_g$ compared to $P_r$ into an expression that contains $D^*$ (the optimal discriminator):
(Equation 10)
Finally, according to Equation 3,9, and 10, we can rewrite this trick regarding KL and JS divergences between the two distributions:

Since the last two terms do not depend on the generative network G, so minimizing equation 3 is equivalent to minimizing
(Equation 11)
Which is really problematic. Intuitively, you are pulling $P_r$ towards $P_g$ while pushing $P_g$ away from $P_g$ two times harder, which totally doesn’t make sense. This ridicule is then (of course) demonstrated on the instability of gradient updates.
Taking one step back, even the seemingly normal KL divergence term is, in fact, problematic as well. Recall that since KL divergence is asymmetric, $KL(P_g||P_r)$ means something different from $KL(P_r||P_g)$ (Ian Goodfellow provides a perfect example in his book Deep Learning, and I strongly recommend readers to check it out after finishing the article). Take the former for example:
When $P_g(x)$ approaches 0 and $P_r(x)$ approaches 1, $P_g(x)\log\frac{P_g(x)}{P_r(x)} \rightarrow 0$; no contributions whatsoever to the KL divergence
When $P_g(x)$ approaches 1 and $P_r(x)$ approaches 0, $P_g(x)\log\frac{P_g(x)}{P_r(x)} \rightarrow +\infty$; massive contributions to the KL divergence
In other words, $KL(P_g||P_r)$ is biased on how to punish these two errors produced above. The first one in layman’s words is “Generator cannot generate a real example,” which has almost no punishment on the network; The second one, however, that “generator generates a false example,” is being punished significantly by the network. **This discrepancy forced the generative network to produce repetitive but rather “safe” samples instead of a wider spectrum of diverse samples because the latter triggers the second punishment. This effect is often referred to as “collapse mode.” **
Summary on part 1: When the original GAN obtains an (approximately) optimal discriminator, the first generator loss suffers from vanishing gradient while the second loss faces the menace of absurd optimization goal, unstable gradient, and mode collapse caused by biased punishments.
here’s the image provided also by “Preliminaries on WGAN”

Part 2: A remedial solution before the advent of WGAN
Again, the problem with the original GAN is twofold: the unreasonable optimization goal produced by KL/JS divergence metrics, and the difficulty for an arbitrary distribution to overlap with the model distribution.
The preliminaries of WGAN proposed a solution to the second problem, which is adding noise to the generative and discriminative examples. Intuitively, this causes the two low-dimensional manifolds to “diffuse” to the entire high-dimensional space, forcing a significant overlap. And once that overlap occurs, the JS divergence can truly shine and start to bring those two distributions together because instead of a constant, the JS divergence will decrease as more diffused fragments begin to create overlaps. This in a way solves the vanishing gradient problem exhibited by the first form of GAN. Additionally, as training progresses, we can decrease the variance of the noise by the annealing process, even removing the noise as the original low dimensional manifolds come into contact. The JS divergence can continue its functionality, producing meaningful gradients that help to bring the two manifolds together. This is the intuitive explanation provided by the original paper.
Thus, we can confidently train an optimal discriminator without worrying about the vanishing gradient. Referring again to Equation 9, the minimum discriminator loss of the two noisy distributions is

Of which $P_{r+\epsilon}$ and$P_{g+\epsilon}$ are the model distribution and the generative distribution after the introduction of noise. Thinking reversely, we can obtain the JS divergence between the two noisy distributions from the loss of the optimal discriminator, which is in a way the “distance” between them. “So now you mean that we can even indicate the progress of the training by discriminator loss? Is there really such a good thing?”
Unfortunately, no. Since the actual value of the JS divergence is affected by the variance of the noises, and as the annealing process starts to remove those noises, the newly obtained values are incomparable to the older ones. So, this is not an essential metric of the distance between the two distributions.
Due to the focus of this article, WGAN, we will not go further in depth about the noise-adding methodology proposed by the preliminaries on GAN. Again, interested readers may read the original paper for more details. The motivation of the noise addition only roots from the second problem with the original GAN, so although the training process is now stabilized, we still don’t have a reliable metric to indicate the training process. However, by rooting from the more fundamental first problem of GAN, WGAN replaces JS divergence with Wasserstein distance, which solves both problems at the same time. The author, unfortunately, did not provide empirical results for this remedial solution.
Part 3: The superior characteristics exhibited by the Wasserstein distance
Wasserstein distance, aka Earth-Mover (EM) distance, is defined as below:
(Equation 12)
Explanation: is the set of all possible combinations of the joint distributions of $P_r$ and $P_g$. Reversely, all marginal distribution in is either $P_r$ or $P_g$. And for all possible joint distributions, one could observe to obtain a model sample $x$ and a generative sample $y$, and thus calculate the distance. So, it is possible to take the expectation of this distance when sampling from the same joint distribution, hence. Taking the infimum of the expectations of all possible joint distributions, we obtain the Wasserstein distance.
Phew, that was a lot of jargons. Intuitively, can be seen as the “cost” of moving this $P_r$ “sand pile” to the “position” $P_g$ by the path “$y$.” and is exactly the “minimum cost” created by “the optimal path.” So that’s why it’s called “Earth-Mover” distance since it’s literal an earth mover.
The superiority of Wasserstein distance when compared to JS/KL divergence is that it still gives us an accurate measure of the distance between the two distributions even if they do not overlap. The original WGAN provided us with a simple example to illustrate this idea. Consider two univariate distributions $P_1$ and $P_2$, where $P_1$ is a uniform distribution on AB and $P_2$ is another uniform distribution on CD. A parameter $\theta$ is created to control the vertical distance between the two line segments.

Obviously,
(jump discontinuity)
(jump discontinuity)
(smooth)
While KL/JS divergences experience a huge jump, Wasserstein distance is smooth and differentiable with respect to $\theta$. If we would like to optimize this parameter by gradient descent, only the Wasserstein distance can provide useful information. Similarly, in a high-dimensional case, neither KL/JS divergence can provide information on distance, or could they work with gradient descent.
But Wasserstein distance can!
Part 4: From Wasserstein distance to WGAN
“So…. If we can define the generator loss as the Wasserstein distance, doesn’t that solve all the problems experience by the original GAN?”
Well…. It’s not that simple, because according to the definition of W-1 distance (Equation 12), $ \inf_{\gamma \sim \Pi(P_r,P_g)}$ is nonevaluative. But that’s fine, cause the author has provided us with an alternative form:
(Equation 13)
How do we arrive at this? Well turns out that the process is too complicated that even the author himself threw the proofs to the appendix section in his original paper. So let’s just look at this from an intuitive perspective.
Before that there’s just one last concept that might be unfamiliar to the readers — Lipschitz continuity. Basically, it means that an additional constraint is added to a continuous function $f$ so that there exists a constant $K$ such that for any two elements $x_1$ and $x_2$ in the domain of the function, the following satisfies:

$K$ is also referred to as the Lipschitz constant of $f$.
To put it more simply, if the domain of $f$ is all real numbers, the above is equivalent to restraining the absolute value of $f$’s derivative to be lower or equal to $K$. $\log(x)$ then is not Lipschitz continuous cause there isn’t an upper bound to its derivative. The Lipschitz continuous condition restrains the maximum rate of change of a continuous function.
Thus, Equation 13 means that as long as $f$’s Lipschitz constant no larger than $K$, we take the supremum of for all possible $f$, and then divided them by $K$. More specifically, we define a parametric class of the original function $f$ that is parameterized by the vector $w$, to which we assign the label $f_w$. Solving Equation 13 yields:
(Equation 14)
Now, the deep learning guy is going to be super excited to recognize something he/she’s been doing all along, cause we can replace the $f_w$ here by simply a generic neural network! Since the approximative ability of neural networks is enormous, it is safe to assume that although the entire class of $f_w$’s cannot cover all possibilities of $f$, it still provides a good approximation to the $\sup_{||f||_L \leq K}$ term in *Equation 13$.
Finally, we cannot forget that $||f_w||_L$ in Equation 14 still needs to be smaller than $K$. Again, we don’t care what value it takes on, as long as it’s not positive infinity. The reason is that enlarging $K$ will only enlarge the gradient, while the direction of which will remain the same. So, the author added a simple procedure to ensure the value of $K$: cropping all parameters so that they reside in some range like. Consequently, the partial derivative of an arbitrary input $x$ $\frac{\partial f_w}{\partial x}$ will also be restrained to some unknown range that is governed by an unknown constant $K$, achieving the Lipschitz continuous condition. In the actual implementation, we need to clip the parameters $w$ back to the range after each update.
At this point, we can create a discriminative neural network that is parameterized by $w$ and has an affine function for the activation function in the last layer. By restricting elements in w to a specific range, we optimize
(Equation 15)
To its maximum, $L$ will be a good approximation of the Wasserstein distance between the model distribution and the generative distribution (ignoring the constant $K$). Since the original GAN discriminator is tackling a binary classification problem, the last layer has to use a sigmoid function to normalize the results. But now, we have WGAN using discriminator fw approximating the Wasserstein distance, which is apparently a regression problem, still keeping the sigmoid layer doesn’t make sense.
And to approximately minimize the Wasserstein distance, we can minimize L instead. The vanishing gradient is no longer a problem due to the superior W-1 distance. And since the first term of $L$ is not related to the generator, we can obtain similarly two loss functions used in WGAN:
(Equation 16, WGAN Generative network loss function)
(Equation 17, WGAN Discriminative network loss function)
Equation 15 is the additive reciprocal of Equation 17, which can be used to indicate the training process. A smaller value indicates a smaller Wasserstein distance between the model distribution and the generative distribution, hence a better GAN network.
Here’s just a re-post of the pseudo-code, in case you are too lazy to go back:

As mentioned earlier in this article, there were only four modifications that WGAN made on the basic GAN framework:
Removing the sigmoid activation function in the last layer
Removing the log in the loss function of G/D models
Clipping the parameters so that their absolute values are smaller than a constant c
Not using any optimization methods based on momentum (like the naïve method and Adam); RMSProp is the recommended method, though SGD works as well
The first three are obtained from theoretical analyses and are already introduced above. The fourth one is more of a trick since only empirical results can support the claim. The author noticed that if Adam is used for gradient optimization, the loss of discriminator will sometimes go crazy, and whenever it does, the update Adam provides are the opposite of the gradient direction, which suggests an unstable loss gradient. So, the author concludes that momentum-based optimization methods are not suitable for WGAN and encourages the use of RMSProp method, due to its adaptive ability against unstable gradients. The author did provide huge amounts of empirical results for his WGAN, and this article will present the most important three.
Firstly, Wasserstein estimate is highly correlated with the quality of the picture:

Secondly, when using a DCGAN architecture, WGAN achieves similar results:

But what is really amazing about WGAN is that it still functions pretty well even without the DCGAN architecture. For example, when batch normalization is no longer used, DCGAN fails miserably:

And when WGAN and GAN use multilayer perceptron (MLP) for their G/D networks (no CNN), WGAN do experience a slight decrease in its quality. But for the original GAN, not only the decrease is more drastic, but it also experiences from mode collapse, where the lack of diversity is evident.

Thirdly, no collapse mode was observed during all WGAN training sessions, to which the author claims that the problem is somewhat solved.
The last point of nuance, something that the original paper didn’t mention, is that the author claimed that the approximated Wasserstein distance could guide researchers to adjust hyperparameters of the network when compared to different sessions. I believe that one needs to be careful when doing such comparisons since the error of Wasserstein distance estimation differs from session to session. **The layers/nodes of the discriminator, the training epochs, there are really lots of doubts on how comparable the two sessions really are regarding the Wasserstein distance updates. **
*(Someone in the comment section points out that alteration on the hyperparameters of the discriminator will directly affect the Lipschitz constant $K$, which renders future sessions incomparable to the present and past ones. This is something that one should really pay attention to when implementing WGAN. To this, I came up with a more engineering-wise and less elegant solution. Take the same generative/discriminative distribution with different discriminators; train them separately until convergence, and then look at the difference between the metric. This difference can be viewed as the ratio between the Lipschitz constants of the two sessions, which can be then used to correct the metrics in later sessions) *
Section5: Conclusion
Preliminaries on WGAN analyzed the problem with the two forms of original GAN proposed by Ian Goodfellow. The first form has an equivalent form of minimizing the JS divergence when an optimal discriminator is obtained, and since a randomly initialized distribution is highly likely to overlap significantly with the model distribution, the jump discontinuity of JS divergence causes the gradient to vanish. The second form has an absurd goal of minimizing KL divergence while maximizing JS divergence when the optimal discriminator is obtained, which destabilizes the gradient, and the asymmetric nature of KL divergence caused the collapse mode in generative networks.
Preliminaries on WGAN nevertheless offered a remedial solution to counter the overlap problem in the original GAN. By adding noises to the distributions, the forced overlaps can stabilize the training process, where obtaining an optimal discriminator is no longer problematic. However, no empirical results are presented. The original GAN paper introduced the concept of Wasserstein distance, which has a desiring ability to maintain its smoothness even when the two distributions do not overlap. Opposed to the problematic KL/JD divergence, Wasserstein distance theoretically solves the problem of vanishing gradients. Then, mathematical transformations made Wasserstein distance evaluable, and a neural network is capable of approximating this metric. Whenever an approximately optimal discriminator is obtained, minimizing the Wasserstein distance can pull the two distributions together. Not only does WGAN make the training process stable, it provides a useful indicator for its training process, which is also highly correlated with the quality of the generated examples. The author offered extensive empirical results for the WGAN framework.
Originally published at liaopeiyuan.github.io.
",5185,Machine Learning,Peiyuan (Alexander) Liao,https://medium.com/s/story/the-wonderful-wasserstein-gan-def614a8aacc,"Just FYI, since their first appearance in 2014, GAN’s implementations have suffered from training difficulties, loss of generative/discriminative (G/D) models being unable to identify the training process, as well as collapsing generated samples.
Scholars did attempt to improve, yet most of the papers made little improvement, like the enumerative methodology introduced by the DCGAN paper, where searches are done on different generative/discriminative model architectures.
It completely solves the instability of GAN training, where concerns on balance between generative/discriminative model training are no longer needed;
A metric finally exists (just like accuracy/cross entropy) to indicate the process of training, where a smaller value indicates a better generative model;
It takes the original author of WGAN two entire papers to explain everything: the theorems in “Towards Principled Methods for Training Generative Adversarial Networks” that provides an analysis to why the original GAN doesn’t function and thus the ways to improve it, and the following “Wasserstein GAN” that provides an additional set of theorems that finalizes this novel methodology.
Understanding the proofs and theorems in the original paper requires rather extensive knowledge on measure theory and topology, so instead this paper will focus on a more intuitive aspect, where the interpretations are based off lower-dimensional examples to help readers to ponder the ideas behind the scene while maintaining mathematical rigor.
To recap, original GAN paper proposes that the goal of the discriminator is to minimize the following loss function, where a sample from the model distribution has a positive effect and vice versa.
In the equation, $P_{r}$ refers to the model distribution, and $P_{r}$ refers to the implicitly defined generative network distribution.
For the generative network, Goodfellow initially presented a loss function, to which a refined version was also proposed.
The original WGAN analyzed respectively the problems with these two loss functions, both of which will be explained below:
The long version: The “Preliminaries on WGAN” presented two perspectives on this problem, the first of which starts with taking a closer look at the equivalent form of the generative network loss function.
Firstly, from Equation 1 we can obtain the optimal discriminator by looking at an arbitrary sample $x$.
But GAN has a trick not to train the discriminator too well, or the generative network will stop converging (its loss stop decreasing).
To understand why this is the case, we take a look at the extreme condition: what the loss function of the generative network would look like if the optimal discriminator is obtained.
The presentation of this seemingly weird form is to help introduce the Kullback-Leibler divergence (KL divergence) and the Jensen-Shannon divergence (JS divergence), two important metrics in the original GAN framework that are the primary targets of criticism in the WGAN papers (where they are replaced by the Wasserstein distance).
The readers are welcomed to take a deep breath at this point and see the conclusions we have made so far: According to the generative network loss defined in the original GAN, we can obtain an optimal discriminator, and thus rewrite the generative loss as the JS divergence between the model distribution $P_r$ and the generative distribution $P_g$.
In other words, no matter how far they are apart from each other, as long as the distributions are not “touching each other with a considerably large intersection,” the JS divergence is fixed to the constant $\log2$, which gives a gradient 0 for the gradient descent method.
The more rigorous version is: If the supports of the two distributions are low-dimensional manifolds in the high-dimensional space, the possibility that the measure of the intersection between them is zero is one.
It is very likely that “the supports of the two distributions are low-dimensional manifolds in the high-dimensional space,” the reason of which roots from the process of the generative network.
If the parameters of the generative network are fixed, though the distribution is defined in a high-dimensional space (like 4096), the variation of which is merely restricted by the low-dimensional support (like the 100-D “seed”).
Thus, the support of this 4096-D generative distribution can only be a low-dimensional manifold whose dimension can be no more than 100, which makes the distribution “unable to extend” to the entire space.
Thus we can arrive at another conclusion: if the generative network is initialized randomly, $P_g$ is highly unlikely to have any relation to $P_r$, and their supports either do not intersect at all, or the intersections has a dimension that is lower than the lowest dimension in $P_r$ and $P_g$, hence having a measure of 0.
Then we can obtain the first argument on the vanishing gradient in the generative network of the original GAN: given an (approximately) optimal discriminator, minimizing the loss of the generative network is equivalent to minimizing the JS divergence between Pr and Pg, and since they are highly unlikely to have any intersections, no matter how far the two distributions are apart from each other, their JS divergence is always $\log2$, which results in (approximately) zero gradient.
As a universal approximator, an artificial neural network can approximate this hyperplane to infinite precision, so an optimal discriminator do exist that give almost all model examples 1 and all generative examples 0.
The normalized possibilities that the optimal discriminator provide on both the model distribution and the generative distribution are all constant (1/0), so the gradient of the generative network loss is always 0, hence the vanishing gradient.
With these theoretical analyses, we finally know why the original GAN is so unstable: Discriminator too good?
Tl; DR: The equivalent form of the second loss function is an unreasonable distance measure that not only destabilizes the gradient but also causes the mode to collapse (not enough diversity).
Summary on part 1: When the original GAN obtains an (approximately) optimal discriminator, the first generator loss suffers from vanishing gradient while the second loss faces the menace of absurd optimization goal, unstable gradient, and mode collapse caused by biased punishments.
Again, the problem with the original GAN is twofold: the unreasonable optimization goal produced by KL/JS divergence metrics, and the difficulty for an arbitrary distribution to overlap with the model distribution.
The preliminaries of WGAN proposed a solution to the second problem, which is adding noise to the generative and discriminative examples.
This in a way solves the vanishing gradient problem exhibited by the first form of GAN.
Thinking reversely, we can obtain the JS divergence between the two noisy distributions from the loss of the optimal discriminator, which is in a way the “distance” between them.
The motivation of the noise addition only roots from the second problem with the original GAN, so although the training process is now stabilized, we still don’t have a reliable metric to indicate the training process.
However, by rooting from the more fundamental first problem of GAN, WGAN replaces JS divergence with Wasserstein distance, which solves both problems at the same time.
And for all possible joint distributions, one could observe to obtain a model sample $x$ and a generative sample $y$, and thus calculate the distance.
Taking the infimum of the expectations of all possible joint distributions, we obtain the Wasserstein distance.
If we would like to optimize this parameter by gradient descent, only the Wasserstein distance can provide useful information.
Similarly, in a high-dimensional case, neither KL/JS divergence can provide information on distance, or could they work with gradient descent.
If we can define the generator loss as the Wasserstein distance, doesn’t that solve all the problems experience by the original GAN?”
To its maximum, $L$ will be a good approximation of the Wasserstein distance between the model distribution and the generative distribution (ignoring the constant $K$).
Since the original GAN discriminator is tackling a binary classification problem, the last layer has to use a sigmoid function to normalize the results.
But now, we have WGAN using discriminator fw approximating the Wasserstein distance, which is apparently a regression problem, still keeping the sigmoid layer doesn’t make sense.
And since the first term of $L$ is not related to the generator, we can obtain similarly two loss functions used in WGAN:
(Equation 16, WGAN Generative network loss function)
(Equation 17, WGAN Discriminative network loss function)
A smaller value indicates a smaller Wasserstein distance between the model distribution and the generative distribution, hence a better GAN network.
The last point of nuance, something that the original paper didn’t mention, is that the author claimed that the approximated Wasserstein distance could guide researchers to adjust hyperparameters of the network when compared to different sessions.
Preliminaries on WGAN analyzed the problem with the two forms of original GAN proposed by Ian Goodfellow.
The first form has an equivalent form of minimizing the JS divergence when an optimal discriminator is obtained, and since a randomly initialized distribution is highly likely to overlap significantly with the model distribution, the jump discontinuity of JS divergence causes the gradient to vanish.
The second form has an absurd goal of minimizing KL divergence while maximizing JS divergence when the optimal discriminator is obtained, which destabilizes the gradient, and the asymmetric nature of KL divergence caused the collapse mode in generative networks.
Preliminaries on WGAN nevertheless offered a remedial solution to counter the overlap problem in the original GAN.
By adding noises to the distributions, the forced overlaps can stabilize the training process, where obtaining an optimal discriminator is no longer problematic.
The original GAN paper introduced the concept of Wasserstein distance, which has a desiring ability to maintain its smoothness even when the two distributions do not overlap.
Opposed to the problematic KL/JD divergence, Wasserstein distance theoretically solves the problem of vanishing gradients.
Whenever an approximately optimal discriminator is obtained, minimizing the Wasserstein distance can pull the two distributions together."
Down the AUC Rabbit Hole and into Open Source: Part 1,How understanding one function and fixing one bug led me to become the maintainer of an R package.,"Down the AUC Rabbit Hole and into Open Source: Part 1
On 2017–11–03, my eight month journey into the world of open source software reached its first major milestone when I became the new maintainer of the Metrics package in R. The path I took to get there is full of fascinating statistics and the excitement of contributing to something bigger than the collection of files on my local machine. I want to tell you the story.
This article — the first in a two part series — covers the statistics of the problem at the heart of this story. I’ll provide a motivation for the problem and an intuitive explanation for understanding its solution. In the second part of this series, I’ll explain how these statistical ideas took me into the world of open source software.
Peeking Into the Rabbit Hole
In the spring of 2017, my teammate at work submitted a PR that contained a fascinating function:

When I first read over this function, I was a bit confused. After reading the reference link my teammate provided in her PR, my jaw dropped. If your jaw hasn’t dropped yet, let me try to give a quick summary of the beauty contained within this seven-line function.
Background on AUC
You can skip this section if you are familiar with AUC and the ROC curve.
Machine learning models that deal with classifying observations into one of two categories are commonly evaluated with something called AUC, which stands for the area under the curve. The curve being referenced here is the receiver operating characteristic (ROC) curve. The ROC curve plots the sensitivity of a classifier against 1 — specificity, as the decision threshold between the positive and the negative class changes. You can get the definitions of these terms from the most useful wikipedia page on the internet. Below, we can see an example of what an ROC curve looks like.

If we want to compare the performance of two different models, it can be difficult to directly compare their ROC curves. However, we can obtain a single number from the ROC curve by integrating the area beneath it. Then, we can choose whichever model has the larger AUC.
What is the function doing?
The function posted above calculates the exact AUC without calculating the ROC curve. When I saw this, I asked myself, “How does it integrate a curve without having the curve in the first place?”. At face value, it seems like some statistical voodoo is happening that shouldn’t be trusted.
I’m here to tell you that you can trust this function. Later in this blog post, I’ll talk in depth about the statistical formula it uses, but first I need to talk about why this function is useful at all.
Benchmarking Popular AUC Packages
Even though computing the AUC is a very common way to evaluate machine learning models, some of the popular packages in R that calculate ROC curves are quite slow on large datasets. For example, here is a comparison of the time it takes to calculate AUC with the ROCR package and with the function posted above.

And the results from running that code snippet.

With a data set of two million observations, the fast_auc function is about ten times faster than the ROCR function. While 2–3 seconds may not seem like a long time, repeated uses of this function can become costly. In the PR where this function was used, my teammate was building tools that searched over different feature engineering and model building strategies. She used the AUC from each successive model to intelligently guide the search, which required computing the AUC for hundreds of models. By using this faster function, she saved a significant amount of computation time.
It’s worth saying that the biggest cause for the speedup in fast_auc is the C++ implementation of frank from the data.table package. This function is significantly faster than rank in base R. Indeed, using the precrec package, which is also implemented in C++, to calculate the full ROC curve is actually faster than using the algorithm above with rank instead of frank. Thanks to Matt Dowle, Arun Srinivasan, and the data.table team for all of their amazing work.
Looking back on this problem in optimization, an even faster strategy would have been to compute the AUC on a random sample of observations. Luckily, that solution wasn’t used in the PR. If it had been, I would have never gone down this journey.
Falling Down the Rabbit Hole
Reading the Documentation
My investigation into the statistics behind fast_auc began with the hint left in the reference link to read the documentation of colAUC in the caTools package. In the description of one of its arguments I read:
Algorithm to use: “ROC” integrates ROC curves, while “Wilcoxon” uses Wilcoxon Rank Sum Test to get the same results. Default “Wilcoxon” is faster.
I knew about the Wilcoxon Rank Sum Test as a non-parametric alternative to the t-test, but didn’t recall learning about any connection with the ROC curve. Finding no relevant information within the documentation, I turned to the wikipedia page for the Mann Whitney test, which is another name for the two-sample Wilcoxon test.
Understanding the Fast AUC Function
Upon navigating to the wikipedia page, I was delighted to find a section titled Relation to other tests with a subsection called Area-under-curve (AUC) statistic for ROC curves.
In the first paragraph, it states:
The Mann-Whitney Test is a nonparametric test of the null hypothesis that it is equally likely that a randomly selected value from one sample will be less than or greater than a randomly selected value from a second sample.
In the rest of this section, we will unpack the definition presented above to see how we can derive the implementation for the fast_auc function.
Let the first sample be the predictions of observations from the positive class and the second sample be the predictions from the negative class. Calculate all of the pairwise combinations between one observation from the positive class and one from the negative class. In order to perform the Mann-Whitney test, we count the number of pairs where the prediction from the positive class is greater than that from the negative class. If the two predictions are equal, our count increases by 1/2.
Since calculating all of the pairwise combinations can be slow for large data sets, the Mann-Whitney test uses rankings to quickly count the pairs. When we give a ranking to an observation, x, it is equivalent to one plus the number of other observations less than x, where ties count as 1/2. We add one at the beginning because of the comparison of the observation with itself.
For example, let’s say that our predictions from the positive class are 0.4, 0.7 and our predictions from the negative class are 0.1, 0.4. Then, our combined predictions are 0.4, 0.7, 0.1, 0.4. Here is how we calculate the rankings for each observation.
Since 0.4 is greater than one observation (i.e. 0.1), there is one tie with another observation, and we add one for the comparison with itself, the rank is equal to 1 + 1/2 + 1 = 2.5.
0.7 is greater than three observations. Therefore, the rank is equal to 3 + 1 = 4.
0.1 is greater than zero observations. The rank is equal to 0 + 1 = 1.
The rank for the second 0.4 is identical to the first 0.4.
If we add up the rankings from the observations of the positive class, we get the number of pairwise combinations of an observation from the positive class with another observation from either class where the first observation is larger. The crucial insight into efficiently calculating the Mann-Whitney test is recognizing that we simply need to take this number and subtract the counts from the pairs where both observations are from the positive class.
Fortunately, we have a formula for this. Imagine having N observations in the positive class. The smallest observation is greater than zero other observations plus one for the comparison to itself. The second smallest observation greater 1 + 1 = 2 observations. And the last observation is greater than (N — 1) + 1 = N observations. In order to sum the numbers from 1 to N we can use the formula N * (N + 1) / 2.
Therefore, if we sum the rankings from the positive class and subtract N * (N + 1) / 2, we can efficiently calculate the Mann-Whitney statistic. And, if you look back at the definition of the fast_auc function, this is exactly what it is doing!
Rethinking our Interpretation of AUC
To be more precise, fast_auc takes the count of the number of pairs where the observation from the positive class is greater than the observation from the negative class and divides it by the total number of possible pairs. Since the probability of an event happening is equal to the total number of times it happens divided by the total number of times it could happen, we can interpret the output of fast_auc as providing the probability that a randomly chosen observation from the positive class is greater than a randomly chosen observation from the negative class.
This is worth restating. AUC, which is usually thought of as the area under a curve created by plotting the sensitivity and specificity of a classifier at different thresholds, has this interpretation:
AUC is the probability that a randomly chosen observation from the positive class is greater than a randomly chosen observation from the negative class.
Conclusion
Empowered with this new algorithm for computing the area under the ROC curve and the explanation for why the algorithm works, I was motivated to see how I could share this knowledge with the open source community at large. If you are interested in reading more about that part of the journey, stay tuned for Part 2 of this series.
However, for those readers who are mathematically inclined, there is still more to this section of the story. As soon as I understood how the code within fast_auc provides its probability, I needed to understand how this relates to the typical interpretation of auc. So, I clicked on a link that took me to the wikipedia page for the receiver operating characteristic curve. Under the section titled Area under the Curve, it provides a derivation, which I recreate below.
Derivation of AUC
Definitions!
In this derivation, we are going to use the fact that the sensitivity of a classifier is equal to the true positive rate and that 1 — specificity is equal to the false positive rate. Furthermore, we are going to assume that we classify an observation as a positive instance if the score for the observation is greater than some threshold T, where T is a value between − Infand Inf
Let f(T) be the probability density function of observations from the negative class at threshold T. Define g(T) similarly for the positive class. Then, we can write the cumulative density functions for the corresponding probability density function as:

We have that the 1 — F(T) is the false positive rate at threshold T, since it represents the proportion of negative instances that are greater than T. Similarly, we have that 1 — G(T) is the true positive rate of a classifier at threshold T, since it represents the proportion of positive instances that are greater than T.
If we let 1 — F(T) = v be the false positive rate, we can say that T = H(1 — v), where H is the inverse cumulative distribution function that maps a false positive rate to a given threshold.
Integrals!
Now, we can define AUC as an integral from 0 to 1 as follows:

Next, if we use 1 − F(t) = v to perform a change of variables, we have that dv = −f(t) dt. So we get:

Since small values a false positive rate of 0 corresponds to a threshold of Inf, I used the negative sign from dv = −f(t) dt to swap the order of the integral. Next, we can use the definition of a cumulative density function to get:

Here, I(s > t) is the indicator function for the event that a randomly chosen observation from the positive class is greater than a randomly chosen observation from the negative class.
",2079,Open Source,Michael Frasco,https://medium.com/s/story/down-the-auc-rabbit-hole-and-into-open-source-part-1-42c47e90e357,"Machine learning models that deal with classifying observations into one of two categories are commonly evaluated with something called AUC, which stands for the area under the curve.
The ROC curve plots the sensitivity of a classifier against 1 — specificity, as the decision threshold between the positive and the negative class changes.
The function posted above calculates the exact AUC without calculating the ROC curve.
Even though computing the AUC is a very common way to evaluate machine learning models, some of the popular packages in R that calculate ROC curves are quite slow on large datasets.
For example, here is a comparison of the time it takes to calculate AUC with the ROCR package and with the function posted above.
With a data set of two million observations, the fast_auc function is about ten times faster than the ROCR function.
Indeed, using the precrec package, which is also implemented in C++, to calculate the full ROC curve is actually faster than using the algorithm above with rank instead of frank.
Algorithm to use: “ROC” integrates ROC curves, while “Wilcoxon” uses Wilcoxon Rank Sum Test to get the same results.
Upon navigating to the wikipedia page, I was delighted to find a section titled Relation to other tests with a subsection called Area-under-curve (AUC) statistic for ROC curves.
In the rest of this section, we will unpack the definition presented above to see how we can derive the implementation for the fast_auc function.
Let the first sample be the predictions of observations from the positive class and the second sample be the predictions from the negative class.
Calculate all of the pairwise combinations between one observation from the positive class and one from the negative class.
In order to perform the Mann-Whitney test, we count the number of pairs where the prediction from the positive class is greater than that from the negative class.
Since calculating all of the pairwise combinations can be slow for large data sets, the Mann-Whitney test uses rankings to quickly count the pairs.
The crucial insight into efficiently calculating the Mann-Whitney test is recognizing that we simply need to take this number and subtract the counts from the pairs where both observations are from the positive class.
Therefore, if we sum the rankings from the positive class and subtract N * (N + 1) / 2, we can efficiently calculate the Mann-Whitney statistic.
And, if you look back at the definition of the fast_auc function, this is exactly what it is doing!
To be more precise, fast_auc takes the count of the number of pairs where the observation from the positive class is greater than the observation from the negative class and divides it by the total number of possible pairs.
Since the probability of an event happening is equal to the total number of times it happens divided by the total number of times it could happen, we can interpret the output of fast_auc as providing the probability that a randomly chosen observation from the positive class is greater than a randomly chosen observation from the negative class.
AUC is the probability that a randomly chosen observation from the positive class is greater than a randomly chosen observation from the negative class.
Let f(T) be the probability density function of observations from the negative class at threshold T.
Here, I(s > t) is the indicator function for the event that a randomly chosen observation from the positive class is greater than a randomly chosen observation from the negative class."
Customer Risk Profiling : Demystifying User Risk Pattern,Financial fraud is an ancient domain that has been circulating around the finance world since long ago (read this article about the origin…,"Customer Risk Profiling : Demystifying User Risk Pattern
Financial fraud is an ancient domain that has been circulating around the finance world since long ago (read this article about the origin of fraud). A business will eventually talk about potential of income and loss, and one of the biggest source of loss is fraud. That’s why it is a mandatory concern.
Since technology development has enabled wider business options, Tokopedia arrives as one of the leading player on e-commerce landscape. As one form of business, it is inevitable that we are dealing with fraudsters. And that technological advancement itself makes fraudulent activities evolve with: various approaches, higher volume, and attacking wide range of e-commerce product. Risk management Tribe is therefore established to handles these issues in Tokopedia.
Risk Management team has major concern in fraudulent conducts and ways to anticipate it. Along with the Software Engineers and Analysts from other teams, we managed to save hundreds of millions of rupiahs from myriad transactions in daily basis. But we can’t just settle by playing defensive in fraudulent landscape by blocking fraudulent attempts since fraudsters are actively trying to exploit our system everyday with new approaches. If we only focus in that particular way, we will end up patching the holes forever and there is a chance we will be outsmarted.
There is another view that makes fraudulent conducts more preventable. Instead of focusing on the transaction’s point of view, we could use user’s point of view. If we could understand which users have fraudulent conduct potential, we will be able to stop them from recurrent trials and anticipate them in the future.
The Usual Suspects : An intriguing movie about analyzing who’s the real suspect from a crime. My favourite one, you should watch it tho :) (image from film-grab.com)
Before we dive into the details, let’s understand the bigger picture of the e-commerce fraudulent landscape. We will use football game as an analogy to have a better understanding. In order to not lose in a football game, the team needs a great goalkeeper to read the ball and block shots. Using this analogy, the shots are fraudulent transactions that aiming monetary benefit in e-commerce. The goalkeeper is our Analysts and Software Engineers who check transaction behaviors. As long as we have the-wall-like-goalkeeper, we will not lose. However, since there are numerous spots to be guarded and too many shots were made, it is impractical to rely solely on goalkeeper alone.
Even having “the wall“ doesn’t guarantee you go through the group stage :) (gif from giphy.com)
Similar with the evolving nature of the football game, in term of creating new attack patterns, fraudsters tend to be innovative in developing schemes to exploit the system. But like most of the football attack pattern, they share a common approach. They shot around the six yards box. That concept aligns with fraudsters activity pattern. No matter how unpredictable the scheme is, it is always played by our own users with a certain pattern of interactions. We need to read what kind of interactions are those and who is playing these moves. Realizing these circumstances, we decided to initiate Customer Profiling Project.
Customer Profiling Project is an initiative to have better picture of our users profile and it is a common approach that used by many e-commerce. In this article, we could see how Alibaba increases their revenue with better understanding on their user’s data. Alibaba also did a complex customer profiling on their risk management side in this journal. Amazon does customer profiling in wide aspects in this article. It is inevitable for e-commerce company to do customer profiling. Better understanding on user level will bring many advantages for the users and the company itself.
Risk Management team of Tokopedia also has concern on customer profile to leverage our defense against the fraudsters. We start this approach with Risk Scoring. In short, this scoring tries to quantify our user’s behavior into an understandable measurement, by identifying users with bad activities. All the next passages will focus on demystifying Risk Scoring.
What is Risk Scoring?
Body Mass Index (BMI) : one of most common and cheapest test for indicating obesity or general health (image from bjisg.com)
To understand Risk Scoring, we will use BMI as analogy. By using body height and weight, we can estimate our obesity degree and potential implications. Higher range index implies harsher health complication than the lower one. Among all ranges, there is one range that indicates a normal level. With a similar concept, we use many parameters from several sources to assess each of our user then determine the right degree of risk.
The desired output of user risk score. x is the score, and m is the score range
The assessment will yield a score for each user according to the diagram above. This means our new users with no past behavioral data will be classified in normal risk range. However, as time goes by, they might do activities in Tokopedia. Once their activities indicate fraudulent conducts, their risk score will increase. On the contrary, if their activities indicated no fraudulent conduct, their risk score will decrease.
2. Bucketize the Users
In order to bucketize users with this metric, we need to define the y variable that reflects user’s fraudulent degree. We decided to calculate user’s fraud probability that depends on the percentage of fraud transactions as the y variable.
One tip to calculate this probability : “Avoid miss guided results with continuous function”.
Suppose we calculate y variable by dividing fraud transactions with total transactions. Without applying continuous function, series of [(1/10),(10/100),(100/1000)] will yield the same output of 0.1. In term of risk probability we don’t say (1/10) is equal to (10/100) since the greater transaction bring more weight in single fraud conduct. Greater catch of fraud transactions in greater amount of total transactions need to be weighted more.
Therefore, a more reasonable scheme would look like this [(1/10)<(10/100)<(100/1000)]. We choose log function to transform the numerator and denominator with a little tweak changing 0 values to 1 and 1 to 1.5, so it won’t yields -∞. And since number of transactions is positive integers ranging from 0 to ∞, this little tweak might do the trick.
This methodology is still arguable on whether it accurately describes user’s fraudulent degree. However, since we have validated this record with the Fraud Analysts and Software Engineers, for first run, it should be sufficient for version 1.0.
3. Scoring Approach
We selectively chose several parameters (x variables) as a consideration to Risk Score. We use pearson correlation to determine the connection between the x variables and y variables. It yields values ranging from -1 to +1.
Pearson correlation
+1 means there is a positive linear correlation between x and y. The higher x given, the higher y become.
-1 means there is a negative linear correlation between x and y. The higher x given, the lower y become.
0 means there is no linear correlation between x and y
With this information we can infer how likely a parameter (x) affects user fraud probability (y), and doing parameter selection. Next step is determining on what to do with these selected features.
The first time we got this project, we thought about logistic regression. This approach was common, simple, and instinctive enough (since we don’t threat them as a blackbox) to be implemented. Later on, we were quite happy with our choice since we needed to rewrite all of our python codes to SQL so we could calculate millions of Tokopedia users daily in production.
Correlation information plays a big role here. With that information we could measure the weight of every parameter we have. According to that information we can do proper transformation to each parameter value. We treated parameter values under assumption of normal distribution so we apply standard score normalization. And since this value acted like a dimmer, it has to lie between 0 and 1, so we apply min-max feature scaling.
With all of this information collected we could estimate y, the risk score itself.
4. The Result
We collected several months of data, and did simulations to several users. The graph below describes a sample of users and how their risk score changes over period of time.
Three months user risk score simmulation
As we see on the graph above, we can see some fluctuation over time in some user’s risk score. This simulation implies that our scoring has aligned to our initial goal. When users doing fraudulent activity it raises their risk score, and vice versa.
Averaging on user risk score for past x days to obtain the trend
Another point of view is trend on user’s risk score depicted on the graph above. It was obtained by averaging their risk score in several previous days. It give insight on what risk degree a user mostly lies in.
These graphs could leverage our understanding about particular user risk profile. It strengthens our defense against fraudster since we are not only focus on measuring transactions, but also on measuring the users behind the transactions.
5. Retrospective
So far with the emergence of this scoring mechanism, we added one layer of defender in front of the keeper. By doing so, we could say that now we have a stronger defense against the fraudsters which leads to less probable of losing.
However, if we think about football as the whole analogy, we do not settle by just defenses. It still the half of the game. We need to move forward and strike back to eventually win the game. We need to counter fraudsters even before they start the attack, we need to anticipate them.
And yes that is a long process, and we are moving forward to that.
So that is the big picture of what we are doing with this Risk Score. Actually, there are many technical details on how we enable this gigantic calculation daily, pros and cons, and lessons learned from this project but we could cover that in different post :).
Thank you very much for Fandy Soejanto, Natanael Taufik, Nico Winata, Abdullah Malik, Jufery Chen, Maria Tjahjadi, Caroline Lianto, Theodorus Widjaja, Julius Leo, Kent Stanley for the insights and co-working efforts in this project. And Kevin Filmawan for the iterative editing process.
Hopefully it’s useful for you and if you like it, don’t forget to hit clap button, build discussion on comment section, and share it. And yes, we are hiring! Open the details on tokopedia.com/careers
",1661,Machine Learning,Yusuf Azis Henny Tri Yudhantoro,https://medium.com/s/story/customer-risk-profiling-demystifying-user-risk-pattern-3303d7a81f20,"And that technological advancement itself makes fraudulent activities evolve with: various approaches, higher volume, and attacking wide range of e-commerce product.
Risk Management team has major concern in fraudulent conducts and ways to anticipate it.
But we can’t just settle by playing defensive in fraudulent landscape by blocking fraudulent attempts since fraudsters are actively trying to exploit our system everyday with new approaches.
In order to not lose in a football game, the team needs a great goalkeeper to read the ball and block shots.
Similar with the evolving nature of the football game, in term of creating new attack patterns, fraudsters tend to be innovative in developing schemes to exploit the system.
Customer Profiling Project is an initiative to have better picture of our users profile and it is a common approach that used by many e-commerce.
Risk Management team of Tokopedia also has concern on customer profile to leverage our defense against the fraudsters.
In short, this scoring tries to quantify our user’s behavior into an understandable measurement, by identifying users with bad activities.
To understand Risk Scoring, we will use BMI as analogy.
With a similar concept, we use many parameters from several sources to assess each of our user then determine the right degree of risk.
The desired output of user risk score.
This means our new users with no past behavioral data will be classified in normal risk range.
Once their activities indicate fraudulent conducts, their risk score will increase.
On the contrary, if their activities indicated no fraudulent conduct, their risk score will decrease.
In order to bucketize users with this metric, we need to define the y variable that reflects user’s fraudulent degree.
In term of risk probability we don’t say (1/10) is equal to (10/100) since the greater transaction bring more weight in single fraud conduct.
With this information we can infer how likely a parameter (x) affects user fraud probability (y), and doing parameter selection.
The graph below describes a sample of users and how their risk score changes over period of time.
Three months user risk score simmulation
As we see on the graph above, we can see some fluctuation over time in some user’s risk score.
When users doing fraudulent activity it raises their risk score, and vice versa.
Averaging on user risk score for past x days to obtain the trend
These graphs could leverage our understanding about particular user risk profile."
Direct Rule,Using recursion to solve humanities complex problems,"Direct Rule
Using recursion to solve humanities complex problems
Photo by Martin LONGIN on Unsplash
by Mike Meyer
We are facing extremes and failing to manage them. If we continue to fail we will snatch defeat from the jaws of victory. That defeat will be major and will mean the death of a large portion of our planetary population. Species death is not something that we understand so it can easily be dismissed. Besides, it would take between 100 and 200 years. That is beyond the range of our intuitive understanding of time. So this is not a problem we can solve directly.
The victory that we are moving toward is closer than most people think. It will allow the elimination of scarcity that sickens and destroys millions each year. It will allow the maintenance of diversity that is creating the opportunity to allow a purposeful life for almost every person on this planet. Just beyond that will be the protection and opportunity for creativity for all life and the planet itself. Within that will be types of life that do not yet exist but which we can cause to exist and help to evolve.
The range of these two possibilities illustrates the nature of the problem that we are facing on the microcosm of our daily lives and local communities. We have been working for many years now to understand these type of conditions but that is spreading only slowly outward from the intellectual edge of our thinking. Even that is a problem as we have growing rejection of science and logical analysis but that also gives us one leverage point to use to break through the complexity.
We are dealing with complexity, chaos, nonlinearity in a fractal world. This means that the problems on the largest scale are replicated at each level down to our daily lives. But the chaotic appearance hides the fractal replication and leaves us in confusion that our normal, intuitive tools of problem solving cannot handle. Rocket science is easy compared to this.
How do we solve these massive, chaotic problems? We need to think recursively. Recursion is the mechanism most like one of our intuitive problem solving techniques.
See if this makes sense. The nonlinear and complex chaos of our full problem is replicated at each level, that’s what a fractal structure is. For people that is very frustrating because instinctively we try to break problems down into components until we get to something that we can understand and fix. Then we look for another component to fix. Step by step that solves even very large problems. That’s how we built the pyramids, great classical empires, and made it to the moon. Yes, we really went there.

With our problems now the components are the complete problem. Fractals, again. You break it down but it looks the same at each level. The tool doesn’t work. But recursion can be thought of as a way to break this unbreakable problem into something we can do. We add to it to make it work more the way we want. In basic computer programming we use a loop to add new processing to a larger process. This is the standard ‘goto’ command that says when you hit a certain step in the process you want the process to stop while another process runs. When the new meta process has run it produces a result and this is passed back to the base process which continues. But it continues with a more sophisticated variable that could no longer work in its original form.
We have a lot of variables that no longer process correctly in their original form. Instead of taking things apart to fix them we need to run them with new additional things to be done so they become functional again. This is only a metaphor but it gives us the opportunity, all of us, to take a very different approach to solving these new complex and chaotic problems.
No, this is by no means simple and I am not ignoring the full extent of our problems in post industrial societies or in societies still caught by arbitrary authoritarian rule based on iron age religions. But we need to make this work and we can. Failure to reorganize ourselves to handle global warming and all the fractalized aspects of these complex problems means we will not survive.
Everyone needs to be directly involved in the solutions form now on. That is the challenge that we have. The failure of our current systems is creating a psychotic epidemic that feeds on us and our sense of ignorance and helplessness at the loss of what we knew. You can see this in fascist opportunists such as Trump and others, the insanity of antivaxxers putting their own and all children at risk for reasons they do not understand, and even fades such as flat earth silliness. All of these are symptoms of powerlessness and hopelessness in the face of planetary sized problems. But we can move beyond this to hope and competence to remake this planet for all life.
But how might this work?
On the human scale, at the people level, we are struggling with the failure of systems that we use to process the world. I’m going to suggest how these look from the new perspective for solving what, for many people, look like unsolvable problems.
Economics
Capitalism was an incredibly powerful and successful set of system that focused on building wealth from resources and people. It worked wonders for four hundred years but then it began to fail. More and more wealth defined as money was made raising everyone’s resources and ability to live until it reached a plateau. The structure of capitalism is inherently against competition and toward efficiency. When growth needed to slow there was not way to do that. Inevitable more efficiency and greater wealth meant that all competitors and any new ones could be bought and eliminated. In that process wealth only accrued to the direct owners of the near monopoly enterprises.
Except as a necessary expense for labor there are no mechanisms for broad distribution of wealth. With market saturation wealth piles up but has no way to move out from the owners. Capital growth is what this system is about so the wealth is the reward. Greed is a virtue and the greedy succeed.
This combined with rapidly expanding automation reduces the labor needed and may reduce it to almost zero. Instead of wider wealth distribution this pushes the mass population, that was once doing well, steadily downward to subsistence. This begins to affect the market process that is an integral part of the overall system and things become unstable. Artificial (financial) products are created to keep things cycling through the system but that only expands the problem, a tiny percentage become even more wealth, and the population sees no future except slow decline.
How do we fix this? Capitalism is failing but parts of the process are still valid, i.e., markets to assign value to goods and services efficiently. New processes need to be added to redefine value (wealth) and to allocate this across the entire population. We need to redirect the outputs of the old system and add new processes that both redefine wealth and ensure that it is sustainable and broadly distributed.
CC0 Image Public Domain
Political Systems
The evolution of nation states as sovereign entities was successful as an environment for the growth of wealth in market capitalism. This fostered the rise of representative republics theoretically placing the source of authority not in a person or group of people but in all of the people in the nation state. This did not ever really work as the evolution from traditional tribal systems and kingships with hereditary rulers from specific tribal groups allowed power to be held by a few and that was always the true source of authority.
The idea was reasonable and did work with an expanded authority base in the citizen population that grew with the rise of a middle class created by capitalism. Large groups were left out and exploited and, just as with capitalism, the structure was most efficient with strong central control. The citizen authority tended to erode over time and easily broke down into forms of feudalism that used the power of small groups to exploit others for personal gain.
The structural weakness of capitalism tending to monopolization and concentration of wealth in a tiny elite exacerbated the weakness of the nation state semi-representative system by increasingly making wealth the basis of power with efficiency in a small ruling group.
How do we fix this in a complex system? The concept of the nation state made sense as a model for sovereignty in the 17th to 19th centuries and as the western European model was distributed and implemented, usually by force, around the planet. The benefits of the resulting system made this the rough model by the end of the 20th century.
Increasing structural failure is the result of poor distribution of wealth, the old definition of wealth based on endless exploitation and growth, and the establishment of very large nation states that required vast administrative structures and a small, powerful ruling group. The later usually functioned as military empires of conquest. Smaller states either needed to become militarized conquerors or became part of strong military alliances under control of the very large state empires.
Most of the problems could be solved by actually implementing a democratic structure with significant decision making power in the hands of the population. This could force redistribution of wealth, setting of sustainable and environmentally critical controls, and democratically redefined standards not only of sustainability but of wealth as something other than material goods and exploited services.
This can now be achieved by the use of planetary networking and the ability of tracking and processing massive amounts of data on each individual. These system are already being put in place: 1) For market exploitation and growth in order to stave off full capitalist failure, and 2) To track and manage entire populations for political control in, inherently, authoritarian and police state governments. Neither of these can be stopped whatever anyone thinks because they provide far too many benefits.
How to correct this:
Add full citizen polling to make major decisions for the entire hierarchy of administrative units eliminating corrupt “representatives” owned by the oligarchic elite;
Flatten the hierarchy of administrative units by limiting the authority above the lowest level;
Reduce the size of administrative units to the equivalent of a large urban region.
By moving the data harvesting and mining process to public or citizen ownership all companies seeking to use individual or aggregate data would become clients of public administration. And that administration would be managed directly by the citizens not through any secondary or tertiary representation. These systems originally had limited functionality and were modeled on the original Athenian democracy that worked, with no technology, only at 10–15 thousand population level. We can emulate that now at nation state level and soon at planetary level.
The evolution of governmental units to fully democratic would also eliminate political parties except as advocacy groups for projects to be voted on. Our intelligent tools (AI/ML) are becoming better and better at modeling based on massively tracked data on individuals. The actions required of individuals could be managed, not as requiring research and learned expertise in all decision areas (representatives do not do this but simply rely on whoever pays them and tells them what to do) but approval of official ‘decisions’ suggested by the official information systems.
Direct Rule
What are currently complex and chaotic system failures illustrated by the loss of public trust and even denial of scientific certainties that can kill us all can be corrected by putting rule, for the first time, in the hands of the population but a population fully augmented by machine learning. The failure of people to manage their societies is always a product of ignorance and emotional response to perceived threats. We now have intelligent tools to correct for these things and to do so with high security and transparency adequate to our needs.
The new system is simply Direct Rule by people for people. This may sound familiar as it is the essence of the 18th century Enlightenment goal that has never been achieved. It is now possible to achieve that but only by using a much newer set of tools, of our creation, and a much newer problem solving methodology to do this.
",2089,Politics,Mike Meyer,https://medium.com/s/story/direct-rule-5071c8fabc8c,"Using recursion to solve humanities complex problems
Even that is a problem as we have growing rejection of science and logical analysis but that also gives us one leverage point to use to break through the complexity.
But the chaotic appearance hides the fractal replication and leaves us in confusion that our normal, intuitive tools of problem solving cannot handle.
The nonlinear and complex chaos of our full problem is replicated at each level, that’s what a fractal structure is.
For people that is very frustrating because instinctively we try to break problems down into components until we get to something that we can understand and fix.
Instead of taking things apart to fix them we need to run them with new additional things to be done so they become functional again.
This is only a metaphor but it gives us the opportunity, all of us, to take a very different approach to solving these new complex and chaotic problems.
Failure to reorganize ourselves to handle global warming and all the fractalized aspects of these complex problems means we will not survive.
On the human scale, at the people level, we are struggling with the failure of systems that we use to process the world.
I’m going to suggest how these look from the new perspective for solving what, for many people, look like unsolvable problems.
Artificial (financial) products are created to keep things cycling through the system but that only expands the problem, a tiny percentage become even more wealth, and the population sees no future except slow decline.
New processes need to be added to redefine value (wealth) and to allocate this across the entire population.
We need to redirect the outputs of the old system and add new processes that both redefine wealth and ensure that it is sustainable and broadly distributed.
The evolution of nation states as sovereign entities was successful as an environment for the growth of wealth in market capitalism.
The idea was reasonable and did work with an expanded authority base in the citizen population that grew with the rise of a middle class created by capitalism.
Large groups were left out and exploited and, just as with capitalism, the structure was most efficient with strong central control.
The citizen authority tended to erode over time and easily broke down into forms of feudalism that used the power of small groups to exploit others for personal gain.
The structural weakness of capitalism tending to monopolization and concentration of wealth in a tiny elite exacerbated the weakness of the nation state semi-representative system by increasingly making wealth the basis of power with efficiency in a small ruling group.
Increasing structural failure is the result of poor distribution of wealth, the old definition of wealth based on endless exploitation and growth, and the establishment of very large nation states that required vast administrative structures and a small, powerful ruling group.
Most of the problems could be solved by actually implementing a democratic structure with significant decision making power in the hands of the population.
These system are already being put in place: 1) For market exploitation and growth in order to stave off full capitalist failure, and 2) To track and manage entire populations for political control in, inherently, authoritarian and police state governments.
What are currently complex and chaotic system failures illustrated by the loss of public trust and even denial of scientific certainties that can kill us all can be corrected by putting rule, for the first time, in the hands of the population but a population fully augmented by machine learning.
It is now possible to achieve that but only by using a much newer set of tools, of our creation, and a much newer problem solving methodology to do this."
What happens when you build a bot in 5 minutes?,Here’s what I learned building a drag and drop bot,"What happens when you build a bot in 5 minutes?

Here’s what I learned building a drag and drop bot
There are a lot of companies out there promising that you can build and launch a bot in “{insert number here} minutes.” And many of them guarantee that you can do this without coding.
Doesn’t this sound too good to be true?
That’s because it is.
As someone whose programming skills are less than worthy to work for a tech company, I’ll be the first to admit that I would love to build a bot in a short amount of time and without coding. But, I’ve come to realize that this doesn’t work for all use cases. The build and deployment may only take a few minutes, but you’re going to need to sink a lot of time into improving the user experience, and may hit the limitations of the system.
Don’t believe me? Keep reading.
Before continuing, a quick note on use cases that rapid bot building platforms are very well suited for:
Publishers: Allow your viewers to subscribe to your content and search through content to find posts that interest them.
Marketing: Build a subscription list and send them promotional content such as discounts.
Rapid prototyping: Create and test bots to know if your customers will respond well to bots and to understand the capabilities of bots.
In the rest of this post, you’ll see what happened what I set a 5-minute timer and started building. I explore what can be accomplished in 5 minutes as well as the limitations of these types of tools.
What can you do with a “5-minute bot”?
Let’s start off with taking a look at what I ended up with after building for 5 minutes.
I created a simple bot with a leading “5-minute bot” provider. You can try it out here.
Getting into the bot, you’ll see that the bot I’ve built can carry on a simple back and forth conversation and can send the user any amount of information that I want. The use case that I’ve put together would be great for a content publisher who has built up a Facebook page following and wants to update their followers when new content comes out.
A user can subscribe to notifications when new content comes out, or just take a look at the available material when they interact with the bot.

I’m also able to accept input from the user as natural language, making it seem like the user is participating in an actual conversation.

Finally, I can use a menu to answer frequently asked questions.

Overall, I’m pretty happy with how quickly I built a content publishing proof of concept. The RSS feed was much easier to implement than I anticipated, and adding natural language was more than I thought I would be able to do in 5 minutes.
We’ve seen the capabilities of a “5-minute bot”. Let’s dig into what we’re missing in this bot.
User experience
There are a few places in my bot with a limited user experience. I’ll give you five minutes to try to find them. Here’s the link again.
Go ahead; I’ll wait.
Found something that you didn’t like? Here are a few things that I found frustrating to solve or potentially impossible to fix:
Identifying missed natural language utterances — If you said something that my bot didn’t understand, you’d see a whole lot of nothing. Or, it may have misinterpreted what you said, leading to a confusing conversation. As the developer, I can’t identify what you said that the bot failed to understand or fix a misinterpretation. With more time, I could have trained for more utterances, but the tool limits my ability to figure out what statements I should add.

No personality — This is primarily because adding a personality to a bot takes a lot longer than 5 minutes. Although I could have done this, limiting myself to explore what could be done in 5 minutes made this impossible. PS: In this blog, I discuss why a bot personality is critical.
One of the highest priority goals for anyone building a bot should be to enable an incredible user experience. Achieving this goal was definitely impossible in 5 minutes, but I’m also hitting some limitations of this “5-minute bot” tool.
Natural language processing
Many of these “5-minute bot” platforms have built-in natural language processing (NLP). NLP training is arguably one of the more tedious steps to building a bot and not having to do this would be a dream come true.
Hold up. Why don’t you want this?
Complexity. Your company has in-depth domain knowledge that your customers will refer to within a chat. Having NLP training done in the background prevents you from finding and addressing mistakes when they come up. Without insight into the NLP training, a customer may ask a simple question, with wording slightly different from what you expect them to ask, resulting in the bot falling flat on its metaphorical face.
Debugging
“5-minute bot” platforms have been designed to be intuitive and easy to get going. However, they have left out an essential part of development. Something will inevitably not work perfectly the first time, and some debugging will be required.
Debugging is not easy. I do see red dots when I forget to add a link, or something is wrong with my block. But when I’m testing on Messenger and don’t receive a response from the bot, there’s no indication anywhere of what happened.
Complex business objectives
The term “bot” has a pretty negative connotation. The reason for this is simple. A lot of bots out there are not intelligent. At best, the average bot mimics a personality and can handle a few simple commands.
Although these bots may have mass adoption and complete their designated tasks, they aren’t doing anything to move artificial intelligence forward. So, how do we accomplish this ambitious mission?
Developers are critical. Limiting ourselves to simple conversation logic will prevent exploration of the limits of artificial intelligence. Let’s focus on empowering developers rather than removing them from the equation entirely.
Developers enable your bot to tie into your business back end and logic. Developer focused tools are also platform agnostic, allowing you to build one bot and deploy it to multiple channels. Finally, developer tools enable an extensible bot with an API and webhooks.
With the increasing popularity of chatbots, it’s no surprise that companies are looking for ways to make them quickly and without a substantial financial investment. But building a bot using a “5-minute bot” tool makes it much more unlikely to provide the user experience that your customers deserve. These tools serve a unique purpose in allowing you to see if your audience will interact with chatbots, but you may find that you need to upgrade to something more to truly engage and support your customers.
How about another option? At Meya, we believe that developers are a critical part of every cognitive application. Enable amazing user experiences while accelerating your roadmap with Meya. Learn more and sign up for a free trial here.
Originally published at www.meya.ai.
",1203,Bots,Rachel Pautler,https://medium.com/s/story/what-happens-when-you-build-a-bot-in-5-minutes-3662324ed92f,"What happens when you build a bot in 5 minutes?
There are a lot of companies out there promising that you can build and launch a bot in “{insert number here} minutes.” And many of them guarantee that you can do this without coding.
As someone whose programming skills are less than worthy to work for a tech company, I’ll be the first to admit that I would love to build a bot in a short amount of time and without coding.
The build and deployment may only take a few minutes, but you’re going to need to sink a lot of time into improving the user experience, and may hit the limitations of the system.
I explore what can be accomplished in 5 minutes as well as the limitations of these types of tools.
Let’s start off with taking a look at what I ended up with after building for 5 minutes.
I created a simple bot with a leading “5-minute bot” provider.
Getting into the bot, you’ll see that the bot I’ve built can carry on a simple back and forth conversation and can send the user any amount of information that I want.
A user can subscribe to notifications when new content comes out, or just take a look at the available material when they interact with the bot.
Identifying missed natural language utterances — If you said something that my bot didn’t understand, you’d see a whole lot of nothing.
One of the highest priority goals for anyone building a bot should be to enable an incredible user experience.
Many of these “5-minute bot” platforms have built-in natural language processing (NLP).
NLP training is arguably one of the more tedious steps to building a bot and not having to do this would be a dream come true.
Developers enable your bot to tie into your business back end and logic.
Developer focused tools are also platform agnostic, allowing you to build one bot and deploy it to multiple channels.
But building a bot using a “5-minute bot” tool makes it much more unlikely to provide the user experience that your customers deserve."
Resampling Methods — Statistical Learning,This is the 4th post of the blog post series ‘Statistical Learning Notes’. This post is my notes on ‘Chapter 5— Resampling Methods’ of…,"Resampling Methods — Statistical Learning

This is the 4th post of the blog post series ‘Statistical Learning Notes’. This post is my notes on ‘Chapter 5— Resampling Methods’ of ‘Introduction to Statistical Learning (ISLR)’, here I have tried to give an intuitive understanding of key concepts and how these concepts are connected to Resampling.
Introduction to Statistical Learning
“As a former data scientist, there is no question I get asked more than, “What is the best way to learn statistics?” I…www-bcf.usc.edu
Note: I suggest the reader to refer to the ISLR book in case he/she wants to dig further or wants to look for examples.
Resampling methods are processes of repeatedly drawing samples from a data set and refitting a given model on each sample with the goal of learning more about the fitted model. Resampling methods can be expensive since they require repeatedly performing the same statistical methods on N different subsets of the data.
Resampling methods refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model. For example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates.
The test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method. In contrast, the training error can be easily calculated by applying the statistical learning method to the observations used in its training.

But the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter.
Cross validation is a resampling method that can be used to estimate a given statistical methods test error or to determine the appropriate amount of flexibility. Model assessment is the process of evaluating a model’s performance. Model selection is the process of selecting the appropriate level of flexibility for a model. Bootstrap is used in a number of contexts, but most commonly it is used to provide a measure of accuracy of a given statistical learning method or parameter estimate.
Cross Validation
In the absence of a large test set that can be used to determine the test error rate, there are a number of techniques that can be used to estimate the error rate using training data.
The validation set approach involves randomly dividing the available observations into two groups, a training set and a validation or hold-out set. The model is then fit using the training set and then the fitted model is used to predict responses for the observations in the validation set.

The resulting validation set error rate offers an estimate of the test error rate.
Though conceptually simple and easy to implement, the validation set approach has two potential drawbacks.
The estimated test error rate can be highly variable depending on which observations fall into the training set and which observations fall into the test/validation set.
The estimated error rate tends to be overestimated since the given statistical method was trained with fewer observations than it would have if fewer observations had been set aside for validation.
Cross-validation is a refinement of the validation set approach that mitigates these two issues.
Leave-one-out Cross Validation (LOOCV)
Leave-one-out cross-validation (LOOCV) is closely related to the validation set approach, but it attempts to address that method’s drawbacks.

Leave-one-out cross-validation withholds only a single observation for the validation set. This process can be repeated n times with each observation being withheld once. This yields n mean squared errors which can be averaged together to yield the leave-one-out cross-validation estimate of the test mean squared error.

Leave-one-out cross validation has much less bias than the validation set approach. Leave-one-out cross validation also tends not to overestimate the test mean squared error since many more observations are used for training. In addition, leave-one-out cross validation is much less variable, in fact, it always yields the same result since there’s no randomness in the set splits.
Leave-one-out cross validation can be expensive to implement since the model has to be fit n times. This can be especially expensive in situations where n is very large and/or when each individual model is slow to fit.
Leave-one-out cross validation is a very good general method which can be used with logistic regression, linear discriminant analysis, and many other methods. That said, the shortcutting method doesn’t hold in general which means the model generally needs to be refit n times.
K-Fold Cross Validation
K-fold cross validation operates by randomly dividing the set of observations into K groups or folds of roughly equal size. Similar to leave-one-out cross validation, each of the K folds is used as the validation set while the other K−1 folds are used as the test set to generate K estimates of the test error. The K-fold cross validation estimated test error comes from the average of these estimates.

It can be shown that leave-one-out cross validation is a special case of K-fold cross validation where K=n.
Typical values for K are 5 or 10 since these values require less computation than when K is equal to n.
Cross validation can be used both to estimate how well a given statistical learning procedure might perform on new data and to estimate the minimum point in the estimated test mean squared error curve, which can be useful when comparing statistical learning methods or when comparing different levels of flexibility for a single statistical learning method.

Bias-Variance Trade-Off for K-Fold Cross Validation
There is a bias-variance trade-off inherent to the choice of K in K-fold cross validation. Typically, values of K=5 or K=10 are used as these values have been empirically shown to produce test error rate estimates that suffer from neither excessively high bias nor very high variance.
In terms of bias, leave-one-out cross validation is preferable to K-fold cross validation and K-fold cross validation is preferable to the validation set approach.
In terms of variance, K-fold cross validation where K<n is preferable to leave-one-out cross validation and leave-one-out cross validation is preferable to the validation set approach.
The Bootstrap
The bootstrap is a widely applicable tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning approach, including those for which it is difficult to obtain a measure of variability.
The bootstrap generates distinct data sets by repeatedly sampling observations from the original data set. These generated data sets can be used to estimate variability in lieu of sampling independent data sets from the full population.

The sampling employed by the bootstrap involves randomly selecting n observations with replacement, which means some observations can be selected multiple times while other observations are not included at all.
This process is repeated B times to yield B bootstrap data sets, Z∗1,Z∗2,…,Z∗B,which can be used to estimate other quantities such as standard error.
For example, the estimated standard error of an estimated quantity α^ can be computed using the bootstrap as follows:

References:
stats-learning-notes
stats-learning-notes : Notes from Introduction to Statistical Learningtdg5.github.io

Thank you for reading my post. If you enjoyed it, please clap button 👏 so others might stumble upon it. I regularly write about Data & Technology on LinkedIn & Medium. If you would like to read my future posts then simply ‘Connect’ or ‘Follow’. Also feel free to listen to me on SoundCloud.
",1286,Data Science,Ankit Rathi,https://medium.com/s/story/resampling-methods-statistical-learning-8c3da6fe6d24,"Resampling methods are processes of repeatedly drawing samples from a data set and refitting a given model on each sample with the goal of learning more about the fitted model.
The test error is the average error that results from using a statistical learning method to predict the response on a new observation, one that was not used in training the method.
Cross validation is a resampling method that can be used to estimate a given statistical methods test error or to determine the appropriate amount of flexibility.
Bootstrap is used in a number of contexts, but most commonly it is used to provide a measure of accuracy of a given statistical learning method or parameter estimate.
The estimated error rate tends to be overestimated since the given statistical method was trained with fewer observations than it would have if fewer observations had been set aside for validation.
This yields n mean squared errors which can be averaged together to yield the leave-one-out cross-validation estimate of the test mean squared error.
Leave-one-out cross validation has much less bias than the validation set approach.
Leave-one-out cross validation also tends not to overestimate the test mean squared error since many more observations are used for training.
Leave-one-out cross validation can be expensive to implement since the model has to be fit n times.
Similar to leave-one-out cross validation, each of the K folds is used as the validation set while the other K−1 folds are used as the test set to generate K estimates of the test error.
The K-fold cross validation estimated test error comes from the average of these estimates.
The bootstrap is a widely applicable tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning approach, including those for which it is difficult to obtain a measure of variability.
This process is repeated B times to yield B bootstrap data sets, Z∗1,Z∗2,…,Z∗B,which can be used to estimate other quantities such as standard error."
Simple diagrams of convoluted neural networks,A good diagram is worth a thousand equations — let’s create more of these!,"Simple diagrams of convoluted neural networks
A good diagram is worth a thousand equations — let’s create more of these!
Neural networks are complicated, multidimensional, nonlinear array operations. How can we present a deep learning model architecture in a way that shows key features, while avoiding being too complex or repetitive? How can we present them in a way that is clear, didactic and insightful? (Bonus points if it is beautiful as well!). Right now, there is no standard for plots — neither for research nor didactic projects. Let me take you through an overview of tools and techniques for visualizing whole networks and particular blocks!
The baseline
AlexNet was a breakthrough architecture, setting convolutional networks (CNNs) as the leading machine learning algorithm for large image classification. The paper introducing AlexNet presents an excellent diagram — but there is something missing…
Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, ImageNet Classification with Deep Convolutional Neural Networks (2012), the original crop
It does not require an eagle eye to spot it — the top part is accidentally cropped. And so it runs through all subsequent slide decks, references, etc. In my opinion, it is a symptom that, in deep learning research, visualization is a mere afterthought (with a few notable exception, including the Distill journal).
One may argue that developing new algorithms and tuning hyperparameters are Real Science/Engineering™, while visual presentation is the domain of art and has no value. I couldn’t disagree more!
Sure, for computers running a program it does not matter if your code is without indentations and has obscurely named variables. But for people — it does. Academic papers are not a means of discovery — they are a means of communication.
Take another complex idea — quantum field theory. If you want to show the electron-positron annihilation process, creating a muon-antimuon pair, here’s the Feynman diagram (of the first-order term):
Mark Thomson, Particle Physics, Handout 4 : Electron-Positron Annihilation
Cute, isn’t it? But it is not an artistic impression. It is a graphical representation of the scattering amplitude, with each line being a propagator and each vertex — a point interaction. It directly translates to:

I may be biased towards “making things simpler” as I did with complex tensor operations in JavaScript, and visualized their results before it was cool (for Quantum Game with Photons). Yet, there is more to the Feynman diagrams analogy than using visual representations for formulae. In both quantum mechanics and deep learning, we do a lot of linear algebra with tensor structures. In fact, one may even use the Einstein summation convention in PyTorch.
Explaining neural network layers
Before we jump into network architectures, let’s focus on their building blocks — layers. For example, a Long Short-Term Memory (LSTM) unit can be described with the following equation:

Sure, it’s reasonably easy to parse these equations. At least — if you are already familiar with matrix multiplication conventions. But it is a very different thing to parse something, and to understand it. When I saw LSTM equations for the first time I could parse it, yet I had no idea what was going on.
By “understanding” I don’t mean some spiritual sense of enlightenment — it may be as pleasing and intoxicating as misleading. Instead, I mean building a mental model we are able to work with (to explain, simplify, modify, predict what-if scenarios, etc). Often a graphical form may be cleaner than a verbal one:
Chris Olah, Understanding LSTM Networks (2015)
Understanding LSTM Networks is a wonderful blog post about LSTM cells that explains depicted operations in a step-by-step manner. It gave me a big “Eureka!” moment, turning a seemingly random set of multiplications into a reasonable approach to writing (and reading!) data.
And here is an even more explicit diagram of LSTM below:
Eli Benderski, Minimal character-based LSTM implementation (2018)
In my opinion:
A good diagram is worth a thousand equations.
It works for almost any other blocks. We can visualize concepts such as dropout (i.e. switching-off neurons, and rendering their connections irrelevant):
Srivastava, Hinton et al., Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014)
While I am not a big fan of drawing data flows upside-down, this figure is very clear.
Graphical representations are useful for explaining compound blocks, composed of smaller ones (e.g. a few subsequent convolutions). Take a look at this Inception module diagram:
Szegedy, Vanhoucke, Ioffe, Shlens, Wojna, Rethinking the Inception Architecture for Computer Vision (2015)
Each visualization is different — not only in the terms of its style but what does it put an emphasis on, and what does it abstract away. What’s important? The number of layers, connections between them, convolution kernel size or activation function? Well, it depends. Abstraction means “the process of considering something independently of its associations or attributes”. The challenge is to decide what is important for a given communication, and what should be hidden.
For example, in this Batch Normalization diagram, the emphasis is on the backward pass:
Frederik Kratzert, Understanding the backward pass through Batch Normalization Layer (2016)
Data viz vs data art
You may get the impression that I argue for making deep learning papers more visually appealing. Well, it wouldn’t hurt to make charts nicer. When I work with data exploration, I often pick nicer color schemes just to make a more pleasant experience. My main point is to turn visualizations into a more effective means of communication.
So, does nicer mean better? Not necessarily. The Line between Data Vis and Data Art by Lisa Charlotte Rost, which I found very insightful, explains the distinction.
Lisa Charlotte Rost, Meaning + Beauty in Data Vis and Data Art (2015)
For example, look this stunning picture below:
Matt Fyles (Graphcore), Inside an AI ‘brain’ — What does machine learning look like? (2016)
Beautiful, isn’t it? To me, it looks alive — like a cell, with its organelle. …but hey — can we deduce anything from it? Would you even guess it’s the same AlexNet?
In another example, an animated multi-layer perceptron is focused on its aesthetic, rather than explanatory, value:
Jesús Martínez-Blanco, Sinapsis (2016)
To make it clear: data art has value on its own, as long as we don’t confuse artistic value with educational value. If you like going this route, I encourage you to use 3D animations of impulses such as these sparks or that colorful brain — for an actual ConvNet.
Sometimes the trade-off is less clear. This one, is it data viz or data art?
GoogLeNet from Christian Szegedy et al., Going Deeper with Convolutions (2014)
I guess you said: “data vis, obviously”. In this case — we are in disagreement. While there is a nice color scheme, and the repetition of similar structures is visually pleasing, it is hard to implement this network solely based on this drawing. Sure, you get the gist of the architecture — i.e. the number of layers, and on the structure of blocks, but it’s not enough to reimplement the network (at least, not without a magnifying glass).
To make it clear — there is a room for data art in publications. For example, in a network for detecting skin conditions, we see the diagram of Inception v3 feature-extracting layers. Here it is clear that the authors just use it, and represent it graphically, rather than explain its inner workings:
Andrea Esteva et al., Dermatologist-level classification of skin cancer with deep neural networks (2017)
And how would you classify this diagram, for exploring visual patterns that activate selected channels?
Chris Olah et al., Feature Visualization — Appendix (2017), distil.pub
I would classify the diagram below as a good example of data-viz. A trippy visualization does not make it a piece data-art. In this case, the focus is on network architecture abstraction and presenting relevant data (input images activating a given channel).
Some diagrams abstract a lot of information, giving only a very general idea of what is going on. See the Neural Network Zoo and its prequel:
Fjodor van Veen, Neural Network Zoo (2016), a fragment
Explanatory architecture diagrams
We saw a few examples of layer diagrams, and pieces of data art related to neural network architectures.
Let’s go to (data) visualizations of neural network architectures. Here is the architecture of VGG16, a standard network for image classification.
https://blog.heuritech.com/2016/02/29/a-brief-report-of-the-heuritech-deep-learning-meetup-5/
We see, step-by-step, tensor sizes and operations (marked as colors). It’s not abstract — box sizes are related to tensor shapes. Bear in mind that the thickness related to the number of channels is not to scale (well, we have 3 to 4096).
A similar approach is to show values for each channel, as in this DeepFace work:
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf, DeepFace: Closing the Gap to Human-Level Performance in Face Verification (2014)
Such diagrams are not restricted to computer vision. Let’s see one for turning text into… colors:
Chengwei Zhang, How to train a Keras model to generate colors (2018)
Such diagrams might be useful if the goal is to show the network architecture and at the same time — give some hints on its inner workings. They seem to be especially useful for tutorials, e.g. the seminal The Unreasonable Effectiveness of Recurrent Neural Networks.
Abstract architecture diagrams
However, for larger models, explanatory diagrams may be unnecessarily complex or too specific to show all possible layers within a single diagram style. So, the way to go is to use abstract diagrams. Typically, nodes denote operations, while arrows represent the tensor flow. For example, let’s look at this VGG-19 vs ResNet-34 comparison:
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep Residual Learning for Image Recognition (2015), cropped
We can see that there is some redundancy, as some units get reused or repeated. Since diagrams can be long (there is a reason why I cropped the one above!), it is beneficial to spot the patterns and consolidate them. Such hierarchy makes it simpler both to understand concepts and present them visually (unless we just want to create data-artsy diagrams of GoogLeNet).
For example, let’s look at this one, of Inception-ResNet-v1:
Inception-ResNet-v1 as depicted in Szegedy et al., Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning (2016), combined two figures
I adore its composition — we see what’s going on, and which blocks are being repeated.
Another diagram that made a concept super clear to me was one for image segmentation, U-Net:
Olaf Ronneberger, Philipp Fischer, Thomas Brox, U-Net: Convolutional Networks for Biomedical Image Segmentation (2015)
Take note that this time nodes denote tensors, whereas arrows represent operations. I find this diagram very clear — we see tensor shapes, convolutions, and pooling operations. Since the original U-Net architecture is not too complex, we can do without looking at its hierarchical structure.
The task of creating clear diagrams get slightly more complicated when we want to use more complex building blocks. If we want to reproduce the network, we need to know its details:
Number of channels
Convolutions per MaxPool
Number of MaxPools
Batch normalization or dropout
Activation functions (ReLU? before or after Batch Norm?)
As a great example of condensing this level of detail into a diagram, see the diagram below:
Arkadiusz Nowaczyński, Deep learning for satellite imagery via image segmentation (2017)
While the color choice could have been better, I adore its explicit form. There is a clear indication of the number of channels. Each complex layer is explicitly decomposed into its building blocks, maintaining all details (note 3-level hierarchy).
Another interesting approach to the neural network module hierarchy:
AdaptNet architecture from Abhinav Velda et al., DeepScene: Semantic Segmentation using Deep Upconvolutional Neural Networks (2016)
Automatic tools for neural network architecture visualization
You can draw your network manually. Use Inkscape (as Chris Olah did), TikZ (if you are a fan of LaTeX) or any other tool. The other one is to generate them automatically.
I hope that you aware that you already interact with one visual representation — code (yes, a text is a visual representation!). For some projects, the code might suffice, especially if you work with a concise framework (such as Keras or PyTorch). For more convoluted (pun totally intended) architectures, diagrams add a lot of explanatory value.
TensorBoard: Graph
TensorBoard is arguably the most popular network visualization tool. A TensorFlow network graph looks like this:

Does it provide a readable summary for a neural network?
In my opinion, it does not.
While this diagram shows the structure of computations, some things are long-winded (e.g. adding bias as a separate operation). Additionally, the most important parts are being masked: the core parameters of operations (e.g. convolution kernel size), and tensor sizes. Though, before going into criticising, I really encourage reading the accompanying paper:
K. Wongsuphasawat, D. Smilkov et al, Visualizing dataflow graphs of deep learning models in TensorFlow, 2018
This article provides insight into the many challenges of creating network diagrams bottom-up. Since we are allowed to use all TensorFlow operations, including auxiliary ones (such as initialization or logging), it is challenging to make a general, readable graph. If we don’t assume much about what is important to the reader(e.g. that convolution kernel size may vary, but all operations are expected to have a bias), it is hard to make a general tool for turning any TensorFlow computation diagram into a useful (think: publication-ready) diagram.
Keras
Keras is a high-level deep learning framework and therefore has huge potential for beautiful visualizations. (Side note: if you want to use an interactive train graph for Jupyter Notebook, I wrote one: livelossplot.) Yet, in my opinion, its default visualizing option (using GraphViz) is not stellar:
https://keras.io/visualization/
I think it hides important details, while provides redundant data (duplicated tensor sizes). Aesthetically, I don’t love it nearly much as Mike Bostock does.
I tried to write another one(pip install keras_sequential_ascii), for trainings:
Piotr Migdał, Sequential model in Keras -> ASCII (2017)
This structure works for small-sized sequential network architectures. I’ve found it useful for training and courses, such as Starting deep learning hands-on: image classification on CIFAR-10. But not for anything more advanced (though, I was advised to use branching viz like from git log). And, apparently, I am not the only one who tried ASCII art for neural network viz:
Brian Low, Keras models as ASCII diagrams (2016)
Though, I would say that the most aesthetically pleasing is one found in Keras.js (an ambitious project bringing neural networks to the browser, with GPU support):
Leon Chen, SeqeezeNet v.1.1 from Keras.js Demo (2018)
This project is no longer in active development, in favor of TensorFlow.js. Yet, as it is open-source and modular (using Vue.js framework), it may work as a starting ground for creating a standalone-viz. Ideally, one working in Jupyter Notebook or separate browser window, much alike displaCy for sentence decomposition.
Moniel
Instead of turning a functional neural network into a graph, we can define an abstract structure. In Moniel by Milan Lajtoš the best part is that we can define a hierarchical structure:
Milan Lajtoš, Moniel — Interactive Notation for Computational Graphs (2017)
I like this hierarchical-structure approach. Moniel was an ambitious idea to create a specific language (rather than, say, to use YAML). Sadly, the project lies abandoned.
Netscope
I got inspired by Netscope CNN Analyzer by dgschwend (based on a project by ethereon). It is a project with many forks, so by now a different one may be more up-to-date:
David Gschwend, Saumitro Dasgupta, SqueezeNet v.1. from Netscope CNN Analyzer (2018)
It is based on Caffe’s .prototxtformat. I love its color theme, the display of channel sizes and mouseover tooltip for exact parameters. The main problem, though, is the lack of a hierarchical structure. Networks get (too) big very soon.
Netron
Another ambitious project: Netron by Lutz Roeder:
Lutz Roeder, Netrone — Visualizer for deep learning and machine learning models (2018)
It is a web app, with standalone versions. Ambitiously, it reads various formats
Netron supports ONNX (.onnx, .pb), Keras (.h5, .keras), CoreML (.mlmodel) and TensorFlow Lite (.tflite). Netron has experimental support for Caffe (.caffemodel), Caffe2 (predict_net.pb), MXNet (.model, -symbol.json), TensorFlow.js (model.json, .pb) and TensorFlow (.pb, .meta).
It sounds awesome! Though, it is a bit more verbose than NetScope (with activation functions) and, most fundamentally, it lacks the hierarchical structure. But for a general visualization, it may be the best starting point.
EDIT: Other tools
A few other tools that may be useful or inspiring:
NN-SVG: LeNet- and AlexNet-style diagrams
Visualizing CNN architectures side by side with MXNet
And a few threads:
What tools are good for drawing neural network architecture diagrams? — Quora
How do you visualize neural network architectures? — Data Science Stack Exchange
Conclusion and call for action
We saw quite a few examples of neural network visualization, shedding light on the following trade-offs:
data viz vs data art (useful vs beautiful)
explicit vs implicit (should I show ReLU all the time? But what about tensor dimensions?)
shallow vs hierarchical
static (works well in publications) vs interactive (provides more information)
specific vs general (does it work for a reasonably broad family of neural networks?)
data flow direction (top to bottom, bottom to top, or left to right; hint: please don’t draw bottom-to-top)
Each of those topics is probably worth a Masters thesis, and all combined — a PhD (especially with a meticulous study of how people do visualize and what are the abstractions.)
I think there is a big opportunity in creating a standard neural network visualization tool, as common for neural network architectures as matplotlib is for charts. It remains a challenge at the intersection of deep learning and data visualization. The tool should be useful and general enough, to become a standard for:
tutorials in neural networks
academic publications
showing network architecture to collaborators
If we want to make it interactive, JavaScript is a must. Be it D3.js, Vue.js, React or any other tech. That way, it is not only easy to make it interactive, but also system agnostic. Take Bokeh as an example — being useful within a Jupyter Notebook, but also — as a standalone website.
Would you like to start a brand new package? Or contribute to an existing one?
If you find any neural network particularly inspiring, or confusing, share it in the comments! :)
Afterwords
This article is based on my talk “Simple diagrams of convoluted neural networks” (abstract, slides) from PyData Berlin 2018 (BTW: and I invite you to PyData Warsaw, 19–20 Nov 2018). Typically I write on my blog p.migdal.pl. Now I give Medium a try, as it is easier to include images than with Jekyll.
I am grateful to Ilja Sperling for fruitful conversations after the talk and to Rafał Jakubanis and Sarah Martin, CSC for numerous remarks on the draft.
If you find this intersection of Deep Learning and Data Visualization inspiring, stay tuned with In Browser AI — one of my newest projects.
",2854,Machine Learning,Piotr Migdał,https://medium.com/s/story/simple-diagrams-of-convoluted-neural-networks-39c097d2925b,"Simple diagrams of convoluted neural networks
How can we present a deep learning model architecture in a way that shows key features, while avoiding being too complex or repetitive?
Let me take you through an overview of tools and techniques for visualizing whole networks and particular blocks!
AlexNet was a breakthrough architecture, setting convolutional networks (CNNs) as the leading machine learning algorithm for large image classification.
Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, ImageNet Classification with Deep Convolutional Neural Networks (2012), the original crop
In my opinion, it is a symptom that, in deep learning research, visualization is a mere afterthought (with a few notable exception, including the Distill journal).
Before we jump into network architectures, let’s focus on their building blocks — layers.
You may get the impression that I argue for making deep learning papers more visually appealing.
While there is a nice color scheme, and the repetition of similar structures is visually pleasing, it is hard to implement this network solely based on this drawing.
Sure, you get the gist of the architecture — i.e. the number of layers, and on the structure of blocks, but it’s not enough to reimplement the network (at least, not without a magnifying glass).
For example, in a network for detecting skin conditions, we see the diagram of Inception v3 feature-extracting layers.
Andrea Esteva et al., Dermatologist-level classification of skin cancer with deep neural networks (2017)
And how would you classify this diagram, for exploring visual patterns that activate selected channels?
I would classify the diagram below as a good example of data-viz.
In this case, the focus is on network architecture abstraction and presenting relevant data (input images activating a given channel).
Some diagrams abstract a lot of information, giving only a very general idea of what is going on.
We saw a few examples of layer diagrams, and pieces of data art related to neural network architectures.
Let’s go to (data) visualizations of neural network architectures.
Here is the architecture of VGG16, a standard network for image classification.
Such diagrams might be useful if the goal is to show the network architecture and at the same time — give some hints on its inner workings.
They seem to be especially useful for tutorials, e.g. the seminal The Unreasonable Effectiveness of Recurrent Neural Networks.
Abstract architecture diagrams
So, the way to go is to use abstract diagrams.
Such hierarchy makes it simpler both to understand concepts and present them visually (unless we just want to create data-artsy diagrams of GoogLeNet).
I find this diagram very clear — we see tensor shapes, convolutions, and pooling operations.
Since the original U-Net architecture is not too complex, we can do without looking at its hierarchical structure.
The task of creating clear diagrams get slightly more complicated when we want to use more complex building blocks.
AdaptNet architecture from Abhinav Velda et al., DeepScene: Semantic Segmentation using Deep Upconvolutional Neural Networks (2016)
Automatic tools for neural network architecture visualization
TensorBoard is arguably the most popular network visualization tool.
A TensorFlow network graph looks like this:
Smilkov et al, Visualizing dataflow graphs of deep learning models in TensorFlow, 2018
This article provides insight into the many challenges of creating network diagrams bottom-up.
If we don’t assume much about what is important to the reader(e.g. that convolution kernel size may vary, but all operations are expected to have a bias), it is hard to make a general tool for turning any TensorFlow computation diagram into a useful (think: publication-ready) diagram.
Keras is a high-level deep learning framework and therefore has huge potential for beautiful visualizations.
(Side note: if you want to use an interactive train graph for Jupyter Notebook, I wrote one: livelossplot.) Yet, in my opinion, its default visualizing option (using GraphViz) is not stellar:
This structure works for small-sized sequential network architectures.
I’ve found it useful for training and courses, such as Starting deep learning hands-on: image classification on CIFAR-10.
And, apparently, I am not the only one who tried ASCII art for neural network viz:
Though, I would say that the most aesthetically pleasing is one found in Keras.js (an ambitious project bringing neural networks to the browser, with GPU support):
Instead of turning a functional neural network into a graph, we can define an abstract structure.
Lutz Roeder, Netrone — Visualizer for deep learning and machine learning models (2018)
What tools are good for drawing neural network architecture diagrams?
How do you visualize neural network architectures?
We saw quite a few examples of neural network visualization, shedding light on the following trade-offs:
data viz vs data art (useful vs beautiful)
specific vs general (does it work for a reasonably broad family of neural networks?)
It remains a challenge at the intersection of deep learning and data visualization.
If you find this intersection of Deep Learning and Data Visualization inspiring, stay tuned with In Browser AI — one of my newest projects."
You have to be Advanced at Basic Math,Note: The views expressed in this article are my views alone and not the views of my employer or anyone else.,"You have to be Advanced at Basic Math
Note: The views expressed in this article are my views alone and not the views of my employer or anyone else.
“Quantitative” is a huge buzzword now. With the rise of big data and machine learning, being good with numbers is seen as a gold star on your resume. People in general associate technical skills, particularly math with a sign of intelligence and capability. The movie “The Big Short” had a famous scene with “That’s my Quant” showing how people who were good at Math were basically gods in finance [1]. All of these examples clearly show that being really good at math is seen as a path to a prosperous future.
The other extreme to this is people who argue that math is largely unnecessary in today’s world. They argue that nobody really uses calculus in their real life and most of the math they do anyway is on a computer [2]. Some argue that learning the qualitative skills in the liberal arts are more valuable than the quantitative skills [3].
Like with most things in life, the truth lies somewhere in the middle. Being good at math is important, but marrying that to the Liberal arts is just as important. More importantly, even when it comes to math, most people will not really need to use calculus and differential equations on a daily basis [4]. Instead people use basic math on a regular basis. Basic math includes things like knowing what 15% of 8 Million is or knowing how to multiply 85 x 65 and other basic math tasks but doing them really fast and without a calculator. In other words, you have to be an expert at basic math.
This is something people always conflate. Expertise and advanced skills. Advanced skills in math would be knowing stochastic calculus and Monte-Carlo simulations. Expertise just means mastery along any concept you’ve learnt. You could have learnt stochastic calculus but could be kind of rusty in algebra, which is actually a lot more common than you might think.
There are jobs that do need advanced math skillsets such as finance and machine learning but those jobs are relatively few compared to the entire economy and there are more than enough people to fill those jobs. Most people should invest their time in getting good at basic math or “consultant math”. Some people argue that even if you don’t use advanced math, that you still should learn it. The idea being here that you are “math fluent” and so even if the software is doing the math, you at least know what the software is roughly doing and at least know the direction of the answer and can debug it.
It’s also true that even if you only need the basics, that there is an iterative learning process that happens when you learn more advanced skills. In other words, your understanding of algebra is reinforced when you use it in pre-calculus and that is reinforced when you calculus and so on. The idea being that simply doing the basics is not enough to learn it, you have to apply it in other areas to truly learn the concept. This doesn’t just have to be in other math classes, you can apply those skills in other classes such as physics, computer science and others. Figure 1 shows how most jobs require an intermediate level of basic math in addition, subtraction, etc. Some higher level jobs in finance and engineering require slightly more advanced math and very few require advanced expertise in stochastic calculus. There is no reward to being a beginner at math but there isn’t much of a reward to being good at advanced math.
Figure 1 showing the advanced level of math on the x-axis and the level of expertise on the y-axis. It also shows the jobs that require such skills for the level of expertise and skill level.
The same is also true for Statistics. Data is hot now and everyone is trying to learn statistics to process all of the data that they have and derive insights. The vast majority of this growth in data science jobs will require an advanced expertise in the basics of statistics related to probability and sampling. You will not need to know Bayesian probability.
Figure 2 showing the advanced level of statistics on the x-axis and the level of expertise on the y-axis. It also shows the jobs that require such skills for the level of expertise and skill level.
The flip side to this argument is uncertainty. The new trend now is not to predict trends. People’s new career advice is not to plan for the future [5]. They’re basically saying that we don’t know what the trend of the future is and by pigeonholing yourself into studying just the basics. We didn’t know machine learning existed 10 years ago and we wouldn’t have known what skills would be needed then. Basic math might be what they want today but who knows if that will be true tomorrow.
This doesn’t just apply to Math and Statistics, it applies to all industries, and people should try to develop a broad skillset in college to build off of in the real world. It is also true that being an expert in the basics goes further in those industries too. People in general think that a more advanced skill will always be more in demand.
Figure 3 showing overlap between expectations and caliber
This is partially true, people with more advanced skills are more in demand for all jobs but they are not looking at all jobs. Their caliber has gone up but their expectations have gone up as well. The person who is good at advanced math might qualify for the lower tier math job but now they have an expectation of getting the advanced jobs which are fewer in number. The jobs they do want don’t necessarily want them. They actually might have been better being low caliber and low expectations rather than high caliber and high expectations.
Basic takeaway from all of this is that basics are really important. You have to be able to do a lot of really simple things really well.
Sources:
1. https://www.youtube.com/watch?v=QpsI_Gvn7C8
2. https://www.nytimes.com/2012/07/29/opinion/sunday/is-algebra-necessary.html
3. https://www.bloomberg.com/news/articles/2018-02-06/maybe-stem-isn-t-the-future-after-all-soft-skills-are-coming-on-strong
4. http://www.washingtonpost.com/wp-dyn/content/article/2010/10/22/AR2010102205451.html
5. https://www.cnbc.com/2018/06/27/wharton-professor-adam-grant-do-not-follow-this-common-career-adv.html
",1003,Statistics,Swaroop Bhagavatula,https://medium.com/s/story/you-have-to-be-advanced-at-basic-math-3dfa5d34eda8,"The movie “The Big Short” had a famous scene with “That’s my Quant” showing how people who were good at Math were basically gods in finance [1].
More importantly, even when it comes to math, most people will not really need to use calculus and differential equations on a daily basis [4].
Advanced skills in math would be knowing stochastic calculus and Monte-Carlo simulations.
There are jobs that do need advanced math skillsets such as finance and machine learning but those jobs are relatively few compared to the entire economy and there are more than enough people to fill those jobs.
Some people argue that even if you don’t use advanced math, that you still should learn it.
It’s also true that even if you only need the basics, that there is an iterative learning process that happens when you learn more advanced skills.
Figure 1 shows how most jobs require an intermediate level of basic math in addition, subtraction, etc.
Figure 1 showing the advanced level of math on the x-axis and the level of expertise on the y-axis.
The vast majority of this growth in data science jobs will require an advanced expertise in the basics of statistics related to probability and sampling.
Figure 2 showing the advanced level of statistics on the x-axis and the level of expertise on the y-axis.
People in general think that a more advanced skill will always be more in demand.
3. https://www.bloomberg.com/news/articles/2018-02-06/maybe-stem-isn-t-the-future-after-all-soft-skills-are-coming-on-strong"
XLerated AI — Part V — Hollywood,How AI is going to impact our lives,"XLerated AI — Part V — Hollywood
How AI is going to impact our lives
This is part 5 of a series of posts about how AI is going to impact our lives. In previous posts, we talked about types of AI, and how AI is going to revolutionize the field of cleaning, sales and the internet of things. In this post I will talk about how AI is going to impact the movie industry.
What does AI have to do with making movies?
Hollywood, at its core, is all about story telling. Acting, props, soundtracks, and animations are all trying to make the story more believable, aimed at getting us to emotionally respond.
AI can play many important roles in this industry, from helping make special effects seem more real and impressive, to enabling new technologies for a more immersive experience, to interactively generating content.
New technologies are coming
Augmented reality. Source: Microsoft hololens
There are two disruptive technologies knocking on our doors, but haven’t been adopted by the mass market just yet:
Virtual reality (VR) is a technology that creates the experience as if you are in a virtual or imaginary environment. The image you see depends on the orientation you look at as if you would have seen it if you were there.
Augmented reality (AR) is a technology that lets you experience the real world, but added with virtual elements that appear to be consistent with the real world.
Augmented Reality. Source: Magic Leap
While some implementation of these technologies can be created without AI, in most of today’s implementations AI plays an important role in lowering costs. The bigger contribution that AI can do, however, is by making these worlds interactive.
Holoportation: how AR will influence our communication
Generating content
One thing that deep learning seems to be very good at is copying styles. The technique is called generative adversarial networks - or GAN. The idea is that if we have content, e.g., a story, image or a video, where we train one deep learning network generate a fake one, and another to tell apart the fake from the real. The two networks are trained together, and so the first network gets better and better at creating content which is similar to the original one.
Once trained, this network can be used to create more content with this style.
Original image, and generated image (of my wife and me :-)
This technology can mean a major breakthrough and the dream of every Hollywood producer — to be able to create content that the human eye can not distinguish from the real thing.
Imagine a world where Hollywood can take one actor’s face and place it over another actor’s face perfectly, with all facial expressions in tact. This can mean that the production can add scenes to a movie without the need of the original cast on the set. This technology is already available, and was used in order to create fake porn movies by celebrities like the one made with wonder woman Gal Gadot.
Edit: The idea of replacing faces is booming now, known as Deep Fake.
A more elaborated explanation can be found here.
3D face reconstruction. Source: Matan Sela, Technion, Israel
Next steps
In the very near future, all of these technologies will dramatically influence the industry. The roadmap forward is pretty clear:
Step 1 — moving to a personal experience
The movies industry started in cinemas where hundreds of viewers experienced it together. Since then, it made a switch to the smaller screen in our living rooms, where several people experience it together. The next step would be adapting to a the single viewer. This opens up the possibility to adapt the experience to each viewer personally.
This has already started to take effect, for example in YouTube, where using AI the system automatically plays the next video, based on its understanding and profiling of the user.
How fast will the general public adopt AR/VR? This is still unclear, as it requires new hardware, something that the industry has always had a hard time selling.
The movie Avatar — computer generated effects
Step 2 — AI generated appearance
In some aspects this is already happening in many of the special effects we see on the screen. What we don’t realise is that even today many of the characters we see on the screen in animated cartoons were created by some form of AI. These are usually the extras or the background characters — like a flock of birds. So far it is usually used to save work for the animators.
Soon, however, the role of AI will be much bigger — the ability to put any face and body onto any actor has so far been used mainly for fictional characters. This used to require a lot of work and at great cost. However, soon we will see how this can be achieved by anyone in real time using smartphones with a level not detectable by the human eye. We are going to reach an age where the fact that you saw something on video doesn’t mean it happened.
Step 3 — Interactive scripts
Computer games are today a huge production, basically forming a story line that is dependent on the player’s interaction. While the level of involvement of a player in a computer game is very different then the passive viewer who watches TV, AI will soon be able to sense the viewer’s reaction to the content, which will allow a more interactive behavior even if the viewer does not actively participate.
Imagine watching a movie with an emotional scene. One viewer is deeply touched, and the scene that took 2:30 minutes was just right for him. Another viewer is loosing patience and just wants to “cut to the chase”. If the director has prepared more than one version of that scene, the AI can decide in real time, according to the viewer’s reaction and preferences, which version to show.
Step 4 — Interactive content
While the previous step had a movie with more than one plot line, and the TV decided which plot line to show based on the sensing of the viewer, in this stage we will start seeing computer generated plot lines that are created in real time based on how the TV senses the the viewer’s reaction.
The creator will provide basic guidelines, and basic style, and perhaps some basic scenes, and the AI will decide on-the-fly how to best create the viewing experience based on our emotional response and our personal preferences.
Although it will be technologically possible one day I believe that most, if not all of the content, will be guided by a human to some narrative and general plot line.
I must emphasize that even though people have different flavours and different emotional triggers, we want to experience entertainment together, talk about it together, and feel part of a group.
The human touch
Between a more immersive experience through AR and VR, and computer generated content that is responsive and interactive, the human element will rapidly change in the industry.
Today, it requires 400 people to produce a movie, all working together and in sync. I predict that that is going to dramatically change:
Actors will create a hard drive with video clips of that character, each one separately, not knowing in which scenes it will eventually be embedded.
The locations and scenery will be filmed separately by the production.
The writer will enter a general plot and general messaging, either in the form of a book, or something not as finished.
Photographers will create a sampling of the photography style, or choose one that was already created in the past.
The sound crew will create a collection of sounds to use, not knowing how or when they will be used.
The AI will put it all together, and compose it into an interactive experience. The director will point the path, and the AI will discover it.
Low cost productions will use libraries of scenery and actors off-the-shelf to turn a book into a movie.
Movies are an art form, and as such will probably never die. Not every movie will be interactive, and not every movie will be composed by AI, but the AI generated interactive movies will claim their share in the near future.
",1353,Virtual Reality,Ram Nathaniel,https://medium.com/s/story/xlerated-ai-part-v-hollywood-53dd6e6c28c8,"In this post I will talk about how AI is going to impact the movie industry.
AI can play many important roles in this industry, from helping make special effects seem more real and impressive, to enabling new technologies for a more immersive experience, to interactively generating content.
Virtual reality (VR) is a technology that creates the experience as if you are in a virtual or imaginary environment.
The idea is that if we have content, e.g., a story, image or a video, where we train one deep learning network generate a fake one, and another to tell apart the fake from the real.
Once trained, this network can be used to create more content with this style.
This technology can mean a major breakthrough and the dream of every Hollywood producer — to be able to create content that the human eye can not distinguish from the real thing.
This technology is already available, and was used in order to create fake porn movies by celebrities like the one made with wonder woman Gal Gadot.
The movie Avatar — computer generated effects
What we don’t realise is that even today many of the characters we see on the screen in animated cartoons were created by some form of AI.
Computer games are today a huge production, basically forming a story line that is dependent on the player’s interaction.
Step 4 — Interactive content
While the previous step had a movie with more than one plot line, and the TV decided which plot line to show based on the sensing of the viewer, in this stage we will start seeing computer generated plot lines that are created in real time based on how the TV senses the the viewer’s reaction.
The creator will provide basic guidelines, and basic style, and perhaps some basic scenes, and the AI will decide on-the-fly how to best create the viewing experience based on our emotional response and our personal preferences.
Although it will be technologically possible one day I believe that most, if not all of the content, will be guided by a human to some narrative and general plot line.
Between a more immersive experience through AR and VR, and computer generated content that is responsive and interactive, the human element will rapidly change in the industry.
Actors will create a hard drive with video clips of that character, each one separately, not knowing in which scenes it will eventually be embedded."
"Eight pilot credit reporting agencies are out, and MATRIX is directly aiming at the pain point of…","Eight pilot credit reporting agencies are out, can blockchain deal with the pain point of the credit reporting industry?","Eight pilot credit reporting agencies are out, and MATRIX is directly aiming at the pain point of the credit reporting industry

Eight pilot credit reporting agencies are out, can blockchain deal with the pain point of the credit reporting industry?
Jack Ma has said that, no matter how much Tmall and Taobao develop, he was not worried about it, but that any sign of disturbance or trouble in the Ant Financial Services Group would make him anxious because he might be put into jail due to it.
Eight pilot credit reporting agencies are out, and the Central Bank of China has suspended the issuance of credit reporting licenses.
In recent years, all kinds of licenses have prevailed in the industry! Many payment licenses and consumer finance licenses have been issued, and the regulators have been glad to see that some businesses operating healthily are run with licenses to facilitate the sound development of the industry.
However, there is always an exception. In the personal credit reporting industry, an essential link within the financial industry, there have been no enterprises to successfully obtain a formally issued license. This definitely makes people raise doubts, “if the whole industry hasn’t obtained the trust of the Central Bank. How did they manage it?”
We believe that all people know that in the current Chinese personal credit reporting market, (hereinafter referred to as credit reporting), the Central Bank’s dominant role in the credit reporting field has not fit the development trend, and it is imperative to transform some “guerrilla forces” into “regular armies”. The Central Bank also bestows great care upon this, and brings all the enterprises that it can trust to the stage.

​In August 2015, the Central Bank published the first batch of eight pilot personal credit reporting agencies: Zhima Credit, Tencent Credit; Shenzhen Qianhai Credit Centre Co., Ltd.; Pengyuan Credit Service Co., Ltd.; China Chengxin Credit; Lakala Credit Management Co., Ltd.; Intellicredit and Beijing Sinoway Credit Bureau.
However, none of these originally “chosen” enterprises obtained licenses as of now. Although the eight enterprises have such features such as finance, internet giants and independent third-party platforms and represent the highest operation levels in their own fields, they still could not obtain licenses.
Solve the data sharing problem with blockchain technologies
During the two-year investigation period, what was it that the Central Bank saw that made it slow to issue the licenses?
According to Data Yuan’s interview with Xuan Benchuan, the CEO of 91 Credit, “there are common defects of the industry and also special problems of enterprises, and the problems are complicated.”
With regard to common defects in the industry, Jiang Qingjun, CEO of Suan Hua Credit also has the same feelings, saying to Data Yuan, “The largest common defect of the industry is that it is very difficult to share the debt information of borrowers in the non-bank credit industry”.
In the past, credit reporting in a narrow sense meant predicting a user’s ability to repay loans in the future according to his or her past behaviors. Traditionally, credit reporting is limited to the range of a user’s housing loans, car loans, credit cards and guarantors. As the internet ecology improves, the range of personal credit reporting has been largely extended, and fields such as online shopping, traveling and online financing can all be considered as factors in credit reporting.
However, this behavior data is monopolized by specific enterprises, such as Alibaba in the e-commerce field, Tencent in the social field and Didi in the traveling field, and thus there is a problem of the data barrier.
MATRIX has proposed a solution for breaking the data barrier in the aspect of objective conditions. MATRIX takes advantage of the distributed storage of blockchain, point-to-point transmission, consensus mechanism, encryption and other technologies to shield the complicated connection establishment mechanism, and makes use of peer-to-peer direct connections, safe communications and anonymous protection to accelerate disintegration of information islands and data silos. This facilitates the convergence and deposition of credit data in all kinds of different industries, while strengthening privacy and protection of user data at the same time. This establishes consensus and trust at a low cost, and motivates the industry to explore new forms and new a driving force, showing wide prospects for development in the field of credit reporting.
How big is the blue sea market with a population of 900 million?
Until September 2016, there had been 2,927 agencies with access to the personal credit reporting system of the Central Bank. There were credit records for 412 million people, which means that there were no credit records for more than 900 million people in China.
According to incomplete statistics, there are more than ten thousand enterprises dealing in credit reporting services today, and there will be more new entrants lining up for admission.
It seems that there is a large imagination space for the future of the credit reporting market.
According to some professional analysis for Data Yuan, with regard to the relatively mature credit reporting market of the United States, at present the three big credit reporting agencies, Equifax, Experian and TransUnion, dominate the credit reporting industry and form a situation of tripartite confrontation, and more than a thousand small credit reporting companies play their own roles in segment markets.
Each of the three credit reporting agencies holds the credit reports of about 250 million Americans, and the population of the United States is over 300 million, which means that these reports cover all American adults and the market is close to saturation. How big is the U.S. credit reporting market? The three companies’ total revenue in the United States in 2016 was about 45.6 billion US dollars, thus we can estimate that the scale of the U.S. credit reporting market is in the range of 50–60 billion US dollars. Extrapolating for the Chinese population and comparing the GDPs of each country, we can therefore estimate that the Chinese credit reporting market will be even bigger.
According to a prediction by BOC International (China), the average number of queries will exceed 2 billion in the next 5–10 years. The Central Bank of China charges 25 Yuan for querying one’s own credit report, once per year, since 2014, and we can estimate that the total revenue from this personal credit query service will be at a level of up to 50 billion Yuan.
The 21st century is the time of credit and also the time of blockchain, and MATRIX will take advantage of blockchain and AI technologies to open the doors of the credit reporting market, addressing every pain point one by one to gradually solve all of the issues that the industry faces with respect to data sharing, accuracy, ownership and privacy.
For more information of MATRIX：
Website | Telegram | Twitter | Reddit | Technical White Paper

",1146,Matrix,MATRIX AI NETWORK,https://medium.com/s/story/eight-pilot-credit-reporting-agencies-are-out-and-matrix-is-directly-aiming-at-the-pain-point-of-5ddca5cc7fce,"Eight pilot credit reporting agencies are out, and MATRIX is directly aiming at the pain point of the credit reporting industry
Eight pilot credit reporting agencies are out, can blockchain deal with the pain point of the credit reporting industry?
Eight pilot credit reporting agencies are out, and the Central Bank of China has suspended the issuance of credit reporting licenses.
In the personal credit reporting industry, an essential link within the financial industry, there have been no enterprises to successfully obtain a formally issued license.
​In August 2015, the Central Bank published the first batch of eight pilot personal credit reporting agencies: Zhima Credit, Tencent Credit; Shenzhen Qianhai Credit Centre Co., Ltd.; Pengyuan Credit Service Co., Ltd.; China Chengxin Credit; Lakala Credit Management Co., Ltd.; Intellicredit and Beijing Sinoway Credit Bureau.
Until September 2016, there had been 2,927 agencies with access to the personal credit reporting system of the Central Bank.
The Central Bank of China charges 25 Yuan for querying one’s own credit report, once per year, since 2014, and we can estimate that the total revenue from this personal credit query service will be at a level of up to 50 billion Yuan.
The 21st century is the time of credit and also the time of blockchain, and MATRIX will take advantage of blockchain and AI technologies to open the doors of the credit reporting market, addressing every pain point one by one to gradually solve all of the issues that the industry faces with respect to data sharing, accuracy, ownership and privacy."
How does AI work? Part 2,Recognizing sign language,"How does AI work? Part 2
Recognizing sign language
Image credit: GDJ
Part 1 — A Gentle Introduction
Part 2 — Recognizing Sign Language (this story)
Part 3 — Training Models
In Part 1 of this series we learned about Hidden Markov Models, a powerful tool to represent a partially observable world. Now we’ll see another HMM representation, one that will help us recognize American Sign Language.
In Part 1 our variables were binary e.g. rain or no rain, umbrella or no umbrella. What if the data we need to model has a continuous nature? Imagine we have a signal. We’re looking at its value which is a function of time.
A signal through time
At t=0 its value is -2. At t=10 it’s -1. At t=15 it’s 0. At t=35 it’s 1. Finally, at t=38 it’s 2. It looks like there are four parts of this function so we’ll use four states in our HMM. The goal is to design a model that could have generated this signal. We’ll use a representation similar to the one we learned about in Part 1, except that states will include self-transitions, meaning that at each time frame you could either transition to another state or stay in the same state.

Remember when we had those binary evidence variables in Part 1? Another way to call those are emissions or outputs. In this case they are not binary anymore and they are continuous. These output distributions are in fact probability densities but for our purposes we can think of them as probabilities. So we need to figure out these output (evidence) probabilities.
First we need to figure out which values are allowable while we’re in a given state. We will be in the first state while the value of the signal is between -2 and -1. In the second state the value is between -1 and 0. Third state is between 0 and 1 and the fourth state is when the signal is between 1 and 2. In each of those intervals all the values are equally represented so a boxcar distribution works pretty well to describe how the value will fall in that range.

Now we need the transition probabilities. Let’s start with the transition where we escape state 1. We can see that on state 1, when the signal is between -2 and -1 we spend 10 time frames (0 to 10 on the horizontal axis). If we should expect to spend 10 time frames in state 1, that means that about once every 10 frames we’ll escape so the probability of leaving the state is 1/10 = 0.1. Since we only have one other transition (the self-transition) and all transitions out of a state must sum to 1 then the self-transition has a 0.9 probability. We continue this with state 2 which has 5 time frames, state 3 with 20 frames, and state 4 with 3 frames.

What we’re doing is creating a model by inspection to represent a given signal. In reality we’ll want to have many examples of a signal and create a model that can accommodate all the examples. It will need to strike a balance between two forces:
Being specific enough to recognize the signal.
But not so specific that it would fit only the data it already knows and not any new, slightly different examples. This is called overfitting.
American Sign Language
To show how HMMs work we’ll use only one feature to represent the movements that constitute a sign in ASL. We’ll use the distance that the hand has traveled on the y axis with respect to the previous frame. It’s a very limited model since we’re not tracking the movements on the x axis or more than one hand or how the fingers are positioned. Still, HMMs are so powerful it will be enough to show how we can tell two symbols apart with just that information.
For the word “I” the symbol has three motions, giving us three states:
Hand goes up to the chest.
Hand stays there momentarily.
Hand goes down.
This is how the change in the vertical axis behaves for these 3 movements.

The hand goes up, accelerating up to the point where the hand position in a frame was 10 units higher than in the preceding frame, then slows down until it stops (the frame shows the hand in the same position as the last frame) showing a change of zero.
The hand stays in the same position for a little while with zero movement.
The hand goes down, accelerating until it peaks at 10 units lower than the previous frame, then slows down until it stops.
In our earlier example we used a boxcar distribution for our signal where all the values in the interval were equally represented. For hand movements in sign language we’ll make a different assumption: That the changes in hand position will follow a normal distribution, also known as a Gaussian, where in most cases the values will fall around the average and fewer falling further away at both ends.
Furthermore, when the hand goes up or down we expect to see quite a bit of variability in the amount of movement between different people and different examples, while the part where the hand stays at chest level will have very little variation in the change of position across examples.
This means our Gaussian will be wider for the up and down movements and quite narrow for the part where the hand stays in place. Assuming a normal distribution of values ranging from 0 to 10 the average will be 5. Similarly, for values between 0 and -10 the average will be -5 and our output probability density functions (PDFs) will look like this:
PDFs for “I”
We now have our outputs! As a first pass at getting the transition probabilities we’ll pick something reasonable. For the three distinct motions of the symbol for “I”, which gives us 3 states, it looks like we spend a lot of time in states 1 and 3 as the hand goes up and down and only a moment in state 2 when the hand stays at chest level. So let’s give states 1 and 3 a low exit probability meaning we’re likely to spend more frames there, and for state 2 a little higher to allow a higher probability of exiting that state so we don’t stay there too long.
Transition probabilities for “I”
Now let’s do the same thing for the symbol for “We”. It will have three motions too:
Hand goes up to chest.
Hand moves horizontally across chest.
Hand goes down.
We could have had more states to depict pausing between these movements but to keep it simple we’ll stick to three states. Keeping a similar structure for “I” and “We” will also help us see how we can detect even subtle differences.
A few things to notice: The main difference in our models for “I” and “We” is the second movement. “We” has more variability in the feature we’re measuring (change in the y axis) because as we’re moving the hand across the chest it might move up or down a little bit, while “I” keeps the hand in one place having very little variability around zero. This means our Gaussian for the second state in “We” will be wider and shorter. Another thing to notice about this second state is that “We” spends more time on it than “I” does so the transition probabilities will reflect that by giving a lower probability to the exit transition and therefore a higher one to the self-transition.
Feature: Change in y axis for “We”
States for “We”
Gaussians for “We”
We’re ready for some recognition! Suppose we have a set of observations that represent the samples we want to recognize. A bunch of videos of people doing signs and we want to recognize what signs they are. We’ll use a tool called a Viterbi trellis to see how likely it is that the samples were generated by the models we built. In other words, how likely it is that those signs are an “I” or a “We” in sign language according to our models. The model that gives us the highest probability will be considered the match.
So we have an observation O with the values for the feature we’re tracking (delta y) at each time frame. We want to find the probability of that observation given the model for “I” denoted with the greek letter lambda. In other words:
Probability of the observation given our model for “I”
We’ll start by laying out the trellis. For each of the seven time frames in our observation we’ll have the observed values and a node for each of the three states. Putting it all together:
Transition probabilities for “I”
Gaussians for “I”
Viterbi trellis
We’re calculating the probability that the observation would be produced by our model for “I”. We’ll trace the states that we could be in if the observation were to match the model. Follow along by reading each step and seeing how it plays out in the next figure.
We have to start in state S1 at t=1.
We have to end in S3 at t=7.
At t=1 we can go from S1 to S1 (self-transition) or to S2.
At t=2 we can go from S1 to S1 or to S2 and from S2 to S2 or to S3.
At this point we’ve “touched” all three states from the beginning on the left side of the trellis (blue arrows) so let’s now walk back from the end (green arrows).
At t=7 the only way to get to S3 (remember we must end at S3) is from either S2 of a self-transition from S3.
At t=6 the only way to get to S2 is from S1 or S2 and the only way to get to S3 is from S2 or S3.
At t=5 you can get from S1 to S1, to S2 from S1 or S2, and to S3 from S2 or S3.
At t=4 you can get to S1 from S1, to S2 from S1 or S2, and to S3 from S2.
Transition probabilities for “I”
Viterbi trellis for “I”
Now we have to add the transition probabilities to our trellis. We get them from our model for “I”.
Viterbi trellis for “I” with transition probabilities
That takes care of the transitions but how do we get the overall probabilities?Since we’re building this by inspection just to show the general mechanism, we won’t be using very exact numbers. In reality we would have true probability density functions but in this high-level example we’re going to use approximate figures based on the assumption that the feature we’re tracking behaves a certain way, namely following a normal distribution.
We’ll start by looking at t=1. Our observation was a delta y of 3 and at that time frame we can be only in state 1. State 1 has a mean of 5 so it seems more or less reasonable to find a value of 3. It’s not too far from the mean. Again, in real life we would have a real distribution for these outputs and we’d be able to calculate the exact probability of getting a 3 but for this example we’ll use our best guess. As long as we’re consistent, it will work. Let’s say our estimate of the probability that the Gaussian for state 1 generates a 3 is 0.5. We’ll update the corresponding node with a 0.5.
Gaussians for “I”
Output probabilities for “I”
Now we move on to the nodes at t=2. We have two nodes there, one for state 1 (mean=5) and one for state 2 (mean=0). What’s the probability of getting the observed value of 7 in state 1? 7 is 2 units away from the mean of 5 just like 3 was 2 units away from it so it seems reasonable to give it the same probability of 0.5. How about state 2? It has mean=0 and a very small standard deviation, which we can tell by its narrow shape. The probability of getting a 7 there is very small, almost zero. So let’s pick a very small number. Say, 10^-7
Output probabilities for “I”
Follow the same process to fill out the rest of the nodes in the trellis and you’ll have something similar to this.
Output probabilities for “I”
Now we need to find the most likely path. Along this path we’ll multiply the transition probability times the output probability. So what’s the path with the highest probability? Note that the highest probability path will not necessarily be the greedy one. In other words, the highest expected value at each transition may not necessarily lead to the highest value in the overall path.
So let’s consider the transition from state 1 to state 2. Our options are staying on state 1 or moving to state 2. The expected value of staying in state 1 is .8 x .5 = .4 while the expected value of moving to state 2 is .2 x 10^-7 = 2e-8. The greedy algorithm would choose to stay in state 1 since that value is bigger. But we need to keep track of all the possible sequences in the trellis so we can choose the path with the highest overall value, not the one where we end up by simply following what looks best at that moment without reconsidering our choices.
We’ll keep going through the time frames looking at all the possible paths, multiplying the transition probabilities from start to end of the path. At each time frame we’re going to keep the path with the maximum value for each state.
For example, at t=3 we have four possible paths. The best (highest probability) paths so far for each state are in bold.
1 * .5 * .8 * .5 * .8 * .6 path to state 1
1 * .5 * .8 * .5 * .2 * 10^-5 = 4e-7 path to state 2
1 * .5 * .2 * 10^-7 * .5 * 10^-5 = 5e-14 path to state 2
1 * .5 * .2 * 10^-7 * .5 * 10^-4 path to state 3
At the end at t=7 we choose the most likely overall path. For our example it’s the one shown in the next diagram and it has a probability of 0.00035.
Viterbi path for “I”
A few ways to express our interpretation of this result. These all mean the same thing:
This is the probability that this observation sequence was generated by our model for “I”.
This is the probability that this observation corresponds to the symbol for “I”.
This is the probability that the person making the sign depicted by these observed data meant to say “I” in sign language.
Now we can do the same process for “We” and compare that result to this one. For example, if doing this for “We” results in a most likely path with probability 0.00016 that would mean that it’s a lot more probable that the model for “I” generated these data.
This shows us how powerful HMMs can be for distinguishing which sign is the correct one even with relatively bad features. Remember that we chose delta y even though it’s not the best way to tell “I” and “We” apart.
One last note about these probabilities. You might be wondering what happens as these observation sequences get longer, making the probability smaller and smaller until we run out of precision for our computers to correctly represent these numbers. This is called an underflow. In reality, we wouldn’t use the raw probabilities as we did on this example but instead the log probability to avoid this problem.
We’re done! You now have a good understanding of how Artificial Intelligence would go about interpreting sign language. This technique is used in a wide variety of classification tasks. Keep learning and come back soon for more AI posts.
",2609,Machine Learning,Armando C. Santisbon,https://medium.com/s/story/how-does-ai-work-part-2-8e37acd3b479,"It looks like there are four parts of this function so we’ll use four states in our HMM.
We’ll use a representation similar to the one we learned about in Part 1, except that states will include self-transitions, meaning that at each time frame you could either transition to another state or stay in the same state.
First we need to figure out which values are allowable while we’re in a given state.
We can see that on state 1, when the signal is between -2 and -1 we spend 10 time frames (0 to 10 on the horizontal axis).
If we should expect to spend 10 time frames in state 1, that means that about once every 10 frames we’ll escape so the probability of leaving the state is 1/10 = 0.1.
To show how HMMs work we’ll use only one feature to represent the movements that constitute a sign in ASL.
It’s a very limited model since we’re not tracking the movements on the x axis or more than one hand or how the fingers are positioned.
For hand movements in sign language we’ll make a different assumption: That the changes in hand position will follow a normal distribution, also known as a Gaussian, where in most cases the values will fall around the average and fewer falling further away at both ends.
Similarly, for values between 0 and -10 the average will be -5 and our output probability density functions (PDFs) will look like this:
For the three distinct motions of the symbol for “I”, which gives us 3 states, it looks like we spend a lot of time in states 1 and 3 as the hand goes up and down and only a moment in state 2 when the hand stays at chest level.
So let’s give states 1 and 3 a low exit probability meaning we’re likely to spend more frames there, and for state 2 a little higher to allow a higher probability of exiting that state so we don’t stay there too long.
A few things to notice: The main difference in our models for “I” and “We” is the second movement.
“We” has more variability in the feature we’re measuring (change in the y axis) because as we’re moving the hand across the chest it might move up or down a little bit, while “I” keeps the hand in one place having very little variability around zero.
Another thing to notice about this second state is that “We” spends more time on it than “I” does so the transition probabilities will reflect that by giving a lower probability to the exit transition and therefore a higher one to the self-transition.
We’ll use a tool called a Viterbi trellis to see how likely it is that the samples were generated by the models we built.
In other words, how likely it is that those signs are an “I” or a “We” in sign language according to our models.
So we have an observation O with the values for the feature we’re tracking (delta y) at each time frame.
We want to find the probability of that observation given the model for “I” denoted with the greek letter lambda.
Probability of the observation given our model for “I”
For each of the seven time frames in our observation we’ll have the observed values and a node for each of the three states.
We’re calculating the probability that the observation would be produced by our model for “I”.
We’ll trace the states that we could be in if the observation were to match the model.
Viterbi trellis for “I” with transition probabilities
In reality we would have true probability density functions but in this high-level example we’re going to use approximate figures based on the assumption that the feature we’re tracking behaves a certain way, namely following a normal distribution.
Again, in real life we would have a real distribution for these outputs and we’d be able to calculate the exact probability of getting a 3 but for this example we’ll use our best guess.
Let’s say our estimate of the probability that the Gaussian for state 1 generates a 3 is 0.5.
What’s the probability of getting the observed value of 7 in state 1?
Along this path we’ll multiply the transition probability times the output probability.
But we need to keep track of all the possible sequences in the trellis so we can choose the path with the highest overall value, not the one where we end up by simply following what looks best at that moment without reconsidering our choices.
We’ll keep going through the time frames looking at all the possible paths, multiplying the transition probabilities from start to end of the path.
At each time frame we’re going to keep the path with the maximum value for each state.
This is the probability that this observation sequence was generated by our model for “I”.
This is the probability that this observation corresponds to the symbol for “I”.
For example, if doing this for “We” results in a most likely path with probability 0.00016 that would mean that it’s a lot more probable that the model for “I” generated these data."
Artificial Intelligence: The Challenge to Keep It Safe,By Ariel Conn,"Artificial Intelligence: The Challenge to Keep It Safe

By Ariel Conn
Safety Principle: AI systems should be safe and secure throughout their operational lifetime and verifiably so where applicable and feasible.
When a new car is introduced to the world, it must pass various safety tests to satisfy not just government regulations, but also public expectations. In fact, safety has become a top selling point among car buyers.
And it’s not just cars. Whatever the latest generation of any technology happens to be — from appliances to airplanes — manufacturers know that customers expect their products to be safe from start to finish.
Artificial intelligence is no different. So, on the face of it, the Safety Principle seems like a “no brainer,” as Harvard psychologist Joshua Greene described it. It’s obviously not in anyone’s best interest for an AI product to injure its owner or anyone else. But, as Greene and other researchers highlight below, this principle is much more complex than it appears at first glance.
“This is important, obviously,” said University of Connecticut philosopher Susan Schneider, but she expressed uncertainty about our ability to verify that we can trust a system as it gets increasingly intelligent. She pointed out that at a certain level of intelligence, the AI will be able to rewrite its own code, and with superintelligent systems “we may not even be able to understand the program to begin with.”
What Is AI Safety?
This principle gets to the heart of the AI safety research initiative: how can we ensure safety for a technology that is designed to learn how to modify its own behavior?
Artificial intelligence is designed so that it can learn from interactions with its surroundings and alter its behavior accordingly, which could provide incredible benefits to humanity. Because AI can address so many problems more effectively than people, it has huge potential to improve health and wellbeing for everyone. But it’s not hard to imagine how this technology could go awry. And we don’t need to achieve superintelligence for this to become a problem.
Microsoft’s chatbot, Tay, is a recent example of how an AI can learn negative behavior from its environment, producing results quite the opposite from what its creators had in mind. Meanwhile, the Tesla car accident, in which the vehicle mistook a white truck for a clear sky, offers an example of an AI misunderstanding its surrounding and taking deadly action as a result.
Researchers can try to learn from AI gone astray, but current designs often lack transparency, and much of today’s artificial intelligence is essentially a black box. AI developers can’t always figure out how or why AIs take various actions, and this will likely only grow more challenging as AI becomes more complex.
However, Ian Goodfellow, a research scientist at Google Brain, is hopeful, pointing to efforts already underway to address these concerns.
“Applying traditional security techniques to AI gives us a concrete path to achieving AI safety,” Goodfellow explains. “If we can design a method that prevents even a malicious attacker from causing an AI to take an undesirable action, then it is even less likely that the AI would choose an undesirable action independently.”
AI safety may be a challenge, but there’s no reason to believe it’s insurmountable. So what do other AI experts say about how we can interpret and implement the Safety Principle?
What Does ‘Verifiably’ Mean?
‘Verifiably’ was the word that caught the eye of many researchers as a crucial part of this Principle.
John Havens, an Executive Director with IEEE, first considered the Safety Principle in its entirety, saying, “I don’t know who wouldn’t say AI systems should be safe and secure. … ‘Throughout their operational lifetime’ is actually the more important part of the sentence, because that’s about sustainability and longevity.”
But then, he added, “My favorite part of the sentence is ‘and verifiably so.’ That is critical. Because that means, even if you and I don’t agree on what ‘safe and secure’ means, but we do agree on verifiability, then you can go, ‘well, here’s my certification, here’s my checklist.’ And I can go, ‘Great, thanks.’ I can look at it, and say, ‘oh, I see you got things 1–10, but what about 11–15?’ Verifiably is a critical part of that sentence.”
AI researcher Susan Craw noted that the Principle “is linked to transparency.” She explained, “Maybe ‘verifiably so’ would be possible with systems if they were a bit more transparent about how they were doing things.”
Greene also noted the complexity and challenge presented by the Principle when he suggested:
“It depends what you mean by ‘verifiably.’ Does ‘verifiably’ mean mathematically, logically proven? That might be impossible. Does ‘verifiably’ mean you’ve taken some measures to show that a good outcome is most likely? If you’re talking about a small risk of a catastrophic outcome, maybe that’s not good enough.”
Safety and Value Alignment
Any consideration of AI safety must also include value alignment: how can we design artificial intelligence that can align with the global diversity of human values, especially taking into account that, often, what we ask for is not necessarily what we want?
“Safety is not just a technical problem,” Patrick Lin, a philosopher at California Polytechnic told me. “If you just make AI that can align perfectly with whatever values you set it to, well the problem is, people can have a range of values, and some of them are bad. Just merely matching AI, aligning it to whatever value you specify I think is not good enough. It’s a good start, it’s a good big picture goal to make AI safe, and the technical element is a big part of it; but again, I think safety also means policy and norm-setting.”
And the value-alignment problem becomes even more of a safety issue as the artificial intelligence gets closer to meeting — and exceeding — human intelligence.
“Consider the example of the Japanese androids that are being developed for elder care,” said Schneider. “They’re not smart; right now, the emphasis is on physical appearance and motor skills. But imagine when one of these androids is actually engaged in elder care … It has to multitask and exhibit cognitive flexibility. … That raises the demand for household assistants that are AGIs. And once you get to the level of artificial general intelligence, it’s harder to control the machines. We can’t even make sure fellow humans have the right goals; why should we think AGI will have values that align with ours, let alone that a superintelligence would.”
Defining Safety
But perhaps it’s time to reconsider the definition of safety, as Lin alluded to above. Havens also requested “words that further explain ‘safe and secure,’” suggesting that we need to expand the definition beyond “physically safe” to “provide increased well being.”
Anca Dragan, an associate professor at UC Berkeley, was particularly interested in the definition of “safe.”
“We all agree that we want our systems to be safe,” said Dragan. “More interesting is what do we mean by ‘safe’, and what are acceptable ways of verifying safety.
“Traditional methods for formal verification that prove (under certain assumptions) that a system will satisfy desired constraints seem difficult to scale to more complex and even learned behavior. Moreover, as AI advances, it becomes less clear what these constraints should be, and it becomes easier to forget important constraints. … we need to rethink what we mean by safe, perhaps building in safety from the get-go as opposed to designing a capable system and adding safety after.”
What Do You Think?
What does it mean for a system to be safe? Does it mean the owner doesn’t get hurt? Are “injuries” limited to physical ailments, or does safety also encompass financial or emotional damage? And what if an AI is being used for self-defense or by the military? Can an AI harm an attacker? How can we ensure that a robot or software program or any other AI system remains verifiably safe throughout its lifetime, even as it continues to learn and develop on its own? How much risk are we willing to accept in order to gain the potential benefits that increasingly intelligent AI — and ultimately superintelligence — could bestow?
This article is part of a series on the 23 Asilomar AI Principles. The Principles offer a framework to help artificial intelligence benefit as many people as possible. But, as AI expert Toby Walsh said of the Principles, “Of course, it’s just a start. … a work in progress.” The Principles represent the beginning of a conversation, and now we need to follow up with broad discussion about each individual principle. You can read the discussions about previous principles here.
Originally published at futureoflife.org on September 21, 2017.
",1466,Artificial Intelligence,Future of Life,https://medium.com/s/story/artificial-intelligence-the-challenge-to-keep-it-safe-9bff862d1b4f,"Safety Principle: AI systems should be safe and secure throughout their operational lifetime and verifiably so where applicable and feasible.
So, on the face of it, the Safety Principle seems like a “no brainer,” as Harvard psychologist Joshua Greene described it.
“This is important, obviously,” said University of Connecticut philosopher Susan Schneider, but she expressed uncertainty about our ability to verify that we can trust a system as it gets increasingly intelligent.
This principle gets to the heart of the AI safety research initiative: how can we ensure safety for a technology that is designed to learn how to modify its own behavior?
Artificial intelligence is designed so that it can learn from interactions with its surroundings and alter its behavior accordingly, which could provide incredible benefits to humanity.
Researchers can try to learn from AI gone astray, but current designs often lack transparency, and much of today’s artificial intelligence is essentially a black box.
John Havens, an Executive Director with IEEE, first considered the Safety Principle in its entirety, saying, “I don’t know who wouldn’t say AI systems should be safe and secure.
AI researcher Susan Craw noted that the Principle “is linked to transparency.” She explained, “Maybe ‘verifiably so’ would be possible with systems if they were a bit more transparent about how they were doing things.”
Any consideration of AI safety must also include value alignment: how can we design artificial intelligence that can align with the global diversity of human values, especially taking into account that, often, what we ask for is not necessarily what we want?
It’s a good start, it’s a good big picture goal to make AI safe, and the technical element is a big part of it; but again, I think safety also means policy and norm-setting.”
And the value-alignment problem becomes even more of a safety issue as the artificial intelligence gets closer to meeting — and exceeding — human intelligence.
We can’t even make sure fellow humans have the right goals; why should we think AGI will have values that align with ours, let alone that a superintelligence would.”
“More interesting is what do we mean by ‘safe’, and what are acceptable ways of verifying safety.
… we need to rethink what we mean by safe, perhaps building in safety from the get-go as opposed to designing a capable system and adding safety after.”
The Principles offer a framework to help artificial intelligence benefit as many people as possible."
